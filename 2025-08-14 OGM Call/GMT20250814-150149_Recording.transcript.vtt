WEBVTT

1
00:00:02.640 --> 00:00:08.270
Jerry Michalski: This is the Open Global Mind Community Call for Thursday, August 14th, 2025.

2
00:00:10.520 --> 00:00:23.570
Jerry Michalski: We are gathered here in part to talk about what our news sources are, or what… who we trust, or how the world is melting, or any one of those directions that you would like to take this in. It cuts across things like

3
00:00:23.690 --> 00:00:31.779
Jerry Michalski: How, you know, what norms should we use to share information with one another in our list and other places? It also goes to

4
00:00:32.400 --> 00:00:48.790
Jerry Michalski: How do you not spin into conspiracy theories, but find interesting and trustworthy information? It goes to the death of journalism, or the ongoing, protracted, agonizing death of journalism, which would make a great movie, or animation at some point.

5
00:00:48.880 --> 00:00:55.929
Jerry Michalski: And what replaces it, if anything? Because people always need information. Even in terrible situations, people always get information.

6
00:00:56.070 --> 00:00:58.350
Jerry Michalski: Small side note.

7
00:00:59.360 --> 00:01:06.800
Jerry Michalski: Trust is this lovely, deep, complicated issue, and one of the things I point out is that in the former East Germany, in the DDR,

8
00:01:07.840 --> 00:01:20.689
Jerry Michalski: on the surface, it was an extremely low-trust environment, because 1 in 10 citizens were basically informants to the STADI, the state police, and there was this huge observation apparatus. There's a really good,

9
00:01:20.690 --> 00:01:28.619
Jerry Michalski: The Lives of Others is a really good movie dramatizing, sort of, the spice, the total surveillance state, and I'm actually wearing

10
00:01:28.880 --> 00:01:33.999
Jerry Michalski: who will watch the Watcher's t-shirt. So, a total surveillance date going on in East Germany.

11
00:01:34.000 --> 00:01:50.960
Jerry Michalski: But underground, the way you got healthcare, education, Western goods, whatever else, was an incredibly high-trust network, because people had… their lives were on the line. And the more the pressure is socially, the more the private networks are deep and important.

12
00:01:51.100 --> 00:01:53.260
Jerry Michalski: And in those settings.

13
00:01:53.490 --> 00:02:06.949
Jerry Michalski: When those nations come into some new form, those people will probably never experience a trust as deep as the one they had with their underground connections, and the people that they were, like, really making a life with.

14
00:02:07.110 --> 00:02:25.650
Jerry Michalski: Because… because so much was on the line. And, so I… I like this about trust. I think trust is weird in that it's fragile, but it's really solid. It happens in quirky places, and sometimes in the places that look the least trustworthy is where some of the deepest trust is being brewed. And that was a kind of a long digression off of.

15
00:02:25.990 --> 00:02:28.980
Jerry Michalski: how do we know what information to trust? How do we know…

16
00:02:29.410 --> 00:02:48.319
Jerry Michalski: the things that are floating past us in the information torrent, which we are mostly drowning in, how do we know which ones are worth remembering and forwarding? How do we sort of put our reputations behind them when we send them into some community space where,

17
00:02:48.320 --> 00:03:00.519
Jerry Michalski: the information is normally shared, because that's what we're doing. We're basically saying, hey, I think this is interesting for you all. That is the gesture of forwarding something into a group. And it matters a lot, because

18
00:03:00.520 --> 00:03:14.749
Jerry Michalski: It's in trusted communities that we're going to get, hopefully, some of the most reliable info, but I don't know. Are trusted communities open to being spun hard and losing their track? Judy, a whole new environment for you. I love this.

19
00:03:14.800 --> 00:03:16.209
Jerry Michalski: Your move is done?

20
00:03:17.500 --> 00:03:21.889
Jerry Michalski: You're muted, but that's… I'm thrilled. I'm thrilled to know that your move is over.

21
00:03:24.480 --> 00:03:27.629
Judith Benham: Sorry, I just unmuted. Yes, I'm done.

22
00:03:27.900 --> 00:03:29.150
Jerry Michalski: Congratulations!

23
00:03:29.150 --> 00:03:32.239
Judith Benham: Thank you. It was interesting.

24
00:03:32.520 --> 00:03:38.279
Jerry Michalski: Interesting is a good word. It's a nice placeholder when things are interesting.

25
00:03:39.090 --> 00:03:45.549
Jerry Michalski: Thanks. Well, I'm just sort of introing the topic, and I'm about to head to Gil and Kevin and Mike, who've raised their hands.

26
00:03:45.710 --> 00:03:47.140
Jerry Michalski: So let's go Gil.

27
00:03:47.920 --> 00:03:50.029
Gil Friend • Sustainability OG • CxO Coach: Yeah, interesting is an interesting word.

28
00:03:50.180 --> 00:03:51.049
Jerry Michalski: Isn't it interesting?

29
00:03:51.050 --> 00:03:54.220
Gil Friend • Sustainability OG • CxO Coach: It says so much and so little at the same time. Oh, God, yeah.

30
00:03:54.220 --> 00:03:55.010
Judith Benham: I know.

31
00:03:55.010 --> 00:04:10.800
Gil Friend • Sustainability OG • CxO Coach: Yeah, I'm trying to weed it out of my conversations with, only partial success. Love the opening provocation, Jerry. You touched on a bunch of themes that were all really the same theme. Yeah. And it's really fascinating. And the notion of,

32
00:04:11.580 --> 00:04:15.049
Gil Friend • Sustainability OG • CxO Coach: in the East Germany example of no trust and deep trust.

33
00:04:15.320 --> 00:04:25.429
Gil Friend • Sustainability OG • CxO Coach: living together, and somehow maybe interacting, is a really fascinating and rich provocation. I've never heard it put quite that way before. Excuse me.

34
00:04:26.280 --> 00:04:27.280
Gil Friend • Sustainability OG • CxO Coach: I've been…

35
00:04:27.530 --> 00:04:32.419
Gil Friend • Sustainability OG • CxO Coach: Very aware at this question of what do we trust, how do we trust, what are trusted sources?

36
00:04:33.190 --> 00:04:41.759
Gil Friend • Sustainability OG • CxO Coach: In, in, lately following multiple sources for the news out of Gaza, Israel, Palestine, that particular mess.

37
00:04:41.980 --> 00:04:52.329
Gil Friend • Sustainability OG • CxO Coach: And I'm finding really contradictory credible sources, about matters of what's going on, and realizing that,

38
00:04:52.890 --> 00:04:55.020
Gil Friend • Sustainability OG • CxO Coach: We have no idea what's so.

39
00:04:56.590 --> 00:05:10.729
Gil Friend • Sustainability OG • CxO Coach: I mean, if you're not there, even if you're there, it's hard, because you're there with a limited view of what's going on, but, you know, to… to get… I'm scanning multiple sources from multiple different perspectives, politically alliances and so forth.

40
00:05:10.870 --> 00:05:12.749
Gil Friend • Sustainability OG • CxO Coach: And I can't figure out what's soap.

41
00:05:13.900 --> 00:05:23.719
Gil Friend • Sustainability OG • CxO Coach: I think of myself as usually pretty discerning and able to sift through different kinds of information and figure stuff out, and I'm stumped, and I'm finding that fascinating and weird.

42
00:05:26.710 --> 00:05:31.990
Jerry Michalski: what's the degree of the lockdown, information lockdown, in Gaza? Is there no, like…

43
00:05:32.020 --> 00:05:49.000
Jerry Michalski: I monitor the Ukraine war in part by watching some Telegram channels, because Telegram is outside of most of the news channels. I know it's owned by Vikontakt, which is a Russian company, but still, Telegram seems to be pretty reliable for these kinds of communications. Is there something like that happening in Gaza, where there's an….

44
00:05:49.000 --> 00:05:54.470
Gil Friend • Sustainability OG • CxO Coach: It's really, it's really tricky as far as I can tell. I mean, Israel officially is not letting foreign journalists in.

45
00:05:54.560 --> 00:05:56.969
Jerry Michalski: Right, we know that there's a journalism blockade.

46
00:05:56.970 --> 00:06:10.829
Gil Friend • Sustainability OG • CxO Coach: Yeah, and even worse, the news feeds into the Israeli public are very limited, so the Israeli population doesn't… apparently doesn't have a deep sense of what's going on, although I'm hearing from people I know there who do.

47
00:06:11.540 --> 00:06:29.140
Gil Friend • Sustainability OG • CxO Coach: And there's, you know, there's another rising wave of anti-war and the war sentiment in Israel. Obviously, deep disdain for Netanyahu, who's got kind of a lock on things. I just got a post… just got a message this morning from a colleague of mine there who's agitating for general strike.

48
00:06:29.980 --> 00:06:47.740
Gil Friend • Sustainability OG • CxO Coach: So we'll see what happens with that. But overall, the Israeli public doesn't seem to have a view of what's going on. There are some independent journalists in and out of Gaza. There are some… there are some outlier, anti-Hamas Palestinian voices starting to be heard.

49
00:06:48.200 --> 00:06:51.539
Gil Friend • Sustainability OG • CxO Coach: … But it's really hard to know.

50
00:06:52.270 --> 00:06:53.660
Kevin Jones: With the voicemail system.

51
00:06:55.380 --> 00:06:56.179
Jerry Michalski: Thanks, Gil.

52
00:06:57.350 --> 00:07:02.939
Jerry Michalski: Kevin, are you still… You're in the queue, but I think you stepped away from your camera.

53
00:07:04.600 --> 00:07:09.069
Jerry Michalski: Why don't we go to Mike and come back to Kevin when he is back on.

54
00:07:10.230 --> 00:07:19.440
Mike Nelson: Thanks a lot. Thanks for a great topic and a good framing. I have several things to throw out, and …

55
00:07:19.560 --> 00:07:27.290
Mike Nelson: if we pick up one of these themes, great. If not… But first, I have a plea for the group.

56
00:07:27.490 --> 00:07:31.740
Mike Nelson: Tomorrow I leave for, Taipei.

57
00:07:31.910 --> 00:07:41.710
Mike Nelson: I'm speaking at the World Innovation Technology and Services Alliance, which is about 80 national IT associations.

58
00:07:42.770 --> 00:07:58.910
Mike Nelson: if any… but I know a lot of you have done things with people in Taiwan. I don't have a lot of time, but I'm particularly interested in names of people that I should look out for at this conference. I mean, there's always, you know, 2,000 people, and if I can…

59
00:07:59.170 --> 00:08:02.849
Mike Nelson: Identify people you say are interesting and different.

60
00:08:03.670 --> 00:08:06.079
Gil Friend • Sustainability OG • CxO Coach: Name of the conference again, Mike?

61
00:08:06.080 --> 00:08:17.310
Mike Nelson: WITSA, W-I-T-S-A, I will put it on the, in the chat. And then, more importantly, I'm going to Manila for the first time.

62
00:08:17.990 --> 00:08:30.909
Mike Nelson: And my schedule's almost open. Unfortunately, I'm there on a Saturday and on a pseudo-holiday, so I only have one day to meet corporate and government people.

63
00:08:31.210 --> 00:08:42.099
Mike Nelson: But again, if there's… if there's interesting people, people I could connect to and reach out to and say, hey, Jerry told me you're fun, you know?

64
00:08:42.440 --> 00:08:51.430
Mike Nelson: So anyway, that's, that ties a little bit into the theme. I'm, you know, I'm looking for people I can trust to tell me what's really going on in both countries.

65
00:08:51.430 --> 00:08:52.680
Jerry Michalski: This is how it works.

66
00:08:52.680 --> 00:08:58.120
Mike Nelson: This is exactly how it works. We all call up Jerry, and Jerry tells us to look at his brain and familiarize.

67
00:08:58.120 --> 00:08:59.180
Jerry Michalski: They go to the network.

68
00:08:59.350 --> 00:08:59.910
Mike Nelson: Yep.

69
00:09:00.090 --> 00:09:08.679
Mike Nelson: And if we don't talk to Jerry, we all trust Jerry, but we also trust Snopes, because that's the other place. So, two case studies, first.

70
00:09:09.150 --> 00:09:21.950
Mike Nelson: Some of you may have seen the news story that Elon Musk had offered a half a billion dollar sponsorship deal for the Baltimore Ravens football team.

71
00:09:22.280 --> 00:09:24.189
Mike Nelson: Did anybody… how many people saw this?

72
00:09:24.190 --> 00:09:24.740
Gil Friend • Sustainability OG • CxO Coach: Yep.

73
00:09:25.550 --> 00:09:26.250
Mike Nelson: And… Nope.

74
00:09:26.440 --> 00:09:33.359
Mike Nelson: The Ravens said, fuck you, no way, we're not gonna deal with a billionaire conspiracy, you know…

75
00:09:33.950 --> 00:09:35.460
Mike Nelson: Totally made up.

76
00:09:35.730 --> 00:09:36.420
Jerry Michalski: Wow.

77
00:09:36.680 --> 00:09:42.169
Mike Nelson: But all over the place, because it seemed too plausible.

78
00:09:43.610 --> 00:10:01.579
Mike Nelson: Snopes hasn't written it up yet, but another fact-finding group said, no, this is not true. There have been some other ones, other teams that had other offers from Musk and rejected them, but somehow Baltimore, the gritty people of Baltimore, seemed like the better

79
00:10:01.790 --> 00:10:09.000
Mike Nelson: … team to focus on. So that's one. Just, this is the last 24 hours.

80
00:10:09.420 --> 00:10:16.759
Mike Nelson: But the other case study I would love to get us to focus on a little bit are internet quotations.

81
00:10:17.880 --> 00:10:23.099
Mike Nelson: You all know I love a good bumper sticker, you know, 7, 8, 12 words.

82
00:10:23.200 --> 00:10:26.650
Mike Nelson: And when I hear a great one, I just have to share it.

83
00:10:27.130 --> 00:10:38.670
Mike Nelson: And half the time, I share it before I really do my research, because it's… it's attributed to somebody really good, or it's, you know, Voltaire, or George Bernard Shaw, or Mark Twain.

84
00:10:38.790 --> 00:10:45.550
Mike Nelson: you know, three of my literary heroes, and I just, you know, automatically want to share the words.

85
00:10:46.160 --> 00:10:52.870
Mike Nelson: And almost always, within 24 hours, I come back and think, oh, I better check this. And half the time.

86
00:10:53.910 --> 00:10:59.120
Mike Nelson: It's not true. I mean, the quote is true, But it's…

87
00:10:59.900 --> 00:11:02.440
Mike Nelson: Not attributed to the right person.

88
00:11:02.560 --> 00:11:08.680
Mike Nelson: … I found one lately by Marcus Aurelius that was just perfect.

89
00:11:08.870 --> 00:11:12.899
Mike Nelson: You know, far better to be…

90
00:11:13.190 --> 00:11:20.199
Mike Nelson: Rejected by society, then subsumed by a mob of idiots.

91
00:11:22.010 --> 00:11:27.730
Mike Nelson: Turned out Leo Tolstoy made that up and attributed it to Marcus Aurelius.

92
00:11:30.000 --> 00:11:36.020
Mike Nelson: It's just weird. I mean, some of these quote… the quote investigator is very good at sort of tracking.

93
00:11:36.020 --> 00:11:37.639
Jerry Michalski: Boat Investigator's great.

94
00:11:37.640 --> 00:11:50.860
Mike Nelson: It is awesome, and I don't know how many people are involved, but it… it is… but quote investigator's not the answer. I mean, it's… we all… we need something else. What I think we need is… is,

95
00:11:52.050 --> 00:11:56.420
Mike Nelson: it's… what is it called? There was… 20 years ago.

96
00:11:57.010 --> 00:12:01.260
Mike Nelson: A company in Singapore invented electronic graffiti.

97
00:12:01.450 --> 00:12:13.229
Mike Nelson: So you could use their app, go to any website in the world, and you could write graffiti on the website that would be available to anybody who saw

98
00:12:13.800 --> 00:12:15.439
Mike Nelson: Who use the same app.

99
00:12:15.750 --> 00:12:22.589
Mike Nelson: And so you could write, you know, actually, Marcus Aurelius didn't say this, Leo Tolstein did. Look here.

100
00:12:22.870 --> 00:12:31.859
Mike Nelson: Never got a business model, never worked, so that's a case study that I personally would very much like to have resolved.

101
00:12:31.990 --> 00:12:42.930
Mike Nelson: The other thing that we focus on at Carnegie a lot, and we have a whole effort on the information environment, there's a new book by the leader of that effort called The Information Animal.

102
00:12:43.600 --> 00:13:00.760
Mike Nelson: Really brilliant woman, Alicia Wanless, and she makes the case on how we need a all-disciplines, all-angles attack on this. She was part of a recent project to identify 11 ways to fight disinformation.

103
00:13:00.830 --> 00:13:07.720
Mike Nelson: Still the best thing I've seen on the topic. Unfortunately, of the 11, None.

104
00:13:08.140 --> 00:13:09.620
Mike Nelson: work really well.

105
00:13:10.520 --> 00:13:11.740
Mike Nelson: Some…

106
00:13:11.920 --> 00:13:21.610
Mike Nelson: It could work well, but there's no business model to pay for it. But I'll put that in the chat as well.

107
00:13:22.020 --> 00:13:27.369
Mike Nelson: My last thought is that we need to stop saying disinformation, misinformation.

108
00:13:27.700 --> 00:13:31.340
Mike Nelson: Because that gets us to focus on the source.

109
00:13:31.530 --> 00:13:35.910
Mike Nelson: And somehow filtering out the bad stuff.

110
00:13:36.220 --> 00:13:41.390
Mike Nelson: When the two things we should be focusing on, first and foremost, are the…

111
00:13:41.500 --> 00:13:58.869
Mike Nelson: the forwarders, the amplifiers. Sometimes they're malicious trolls from… funded by Russia, other times they're just stupid idiots up at 2 o'clock in the morning who are just not awake enough to know that something is manipulating their mind. And then the second

112
00:13:59.070 --> 00:14:03.059
Mike Nelson: I think the most important thing is brain defense.

113
00:14:03.310 --> 00:14:07.250
Mike Nelson: You know, don't call it media literacy, don't call it…

114
00:14:08.100 --> 00:14:19.930
Mike Nelson: critical reasoning… I mean, it's brain defense. It's how do you… repel… Hateful, ignorant, deceitful, deceiving.

115
00:14:20.240 --> 00:14:23.800
Mike Nelson: … memes and bumper stickers.

116
00:14:24.030 --> 00:14:28.259
Mike Nelson: How do you, and how do you identify when something is, is, is…

117
00:14:33.630 --> 00:14:34.490
Mike Nelson: environment.

118
00:14:34.490 --> 00:14:34.840
Jerry Michalski: True.

119
00:14:34.840 --> 00:14:38.010
Mike Nelson: I hope you can answer one or two of my 15 questions.

120
00:14:38.800 --> 00:14:41.239
Jerry Michalski: Damn, that was a virtuosic tour.

121
00:14:42.220 --> 00:14:56.599
Jerry Michalski: I had not heard of the book, I had not heard of many of those things, and I certainly hadn't heard of the Musk-Baltimore Ravens thing, which Googles poorly. It gets a Yahoo hit and something else, but otherwise kind of peters out quickly. Anybody else heard about these things?

122
00:14:57.630 --> 00:14:58.300
Judith Benham: No.

123
00:14:58.940 --> 00:15:00.240
Jerry Michalski: Gil ahead, but you're muted.

124
00:15:04.930 --> 00:15:10.010
Gil Friend • Sustainability OG • CxO Coach: Trying to be well-behaved here. I did hear about the Musk Ravens thing, but not about the reputation of it.

125
00:15:10.490 --> 00:15:18.250
Gil Friend • Sustainability OG • CxO Coach: So, you know, lies move fast, truth corrects slowly. I think somebody once said, maybe Abraham Lincoln.

126
00:15:18.250 --> 00:15:23.910
Jerry Michalski: A lie makes its way around the world five times before the truth gets its pants on, roughly

127
00:15:24.240 --> 00:15:26.130
Jerry Michalski: I will find the correct quote in a second.

128
00:15:26.130 --> 00:15:29.809
Gil Friend • Sustainability OG • CxO Coach: For sure, Abraham Lincoln, when in doubt, Abraham Lincoln said the quote.

129
00:15:29.960 --> 00:15:32.949
Gil Friend • Sustainability OG • CxO Coach: I, I, I did, I didn't….

130
00:15:32.950 --> 00:15:36.919
Mike Nelson: I also saw on the internet that Abraham Lincoln said.

131
00:15:36.960 --> 00:15:54.729
Gil Friend • Sustainability OG • CxO Coach: Never trust anything you read on the internet. Absolutely, yeah, go to him for that. I wasn't aware of Quote Investigator. Usually, when I'm in question on a quote, I will put the quote into the search and have multiple quote sites show up and cross-tabulate and make sure, you know, see if it's real, but you never know.

132
00:15:54.890 --> 00:16:05.660
Gil Friend • Sustainability OG • CxO Coach: Mike, I like what you said about, there's no business model for fighting disinformation. There certainly is a business model for spreading it, so there is a fundamental conflict in the economy of now.

133
00:16:05.830 --> 00:16:12.669
Gil Friend • Sustainability OG • CxO Coach: When you said we shouldn't call it disinformation, that echoed for me, because I've long been thinking we should call it lies.

134
00:16:13.700 --> 00:16:17.590
Gil Friend • Sustainability OG • CxO Coach: You know, and bullshit, which are different, of course.

135
00:16:17.590 --> 00:16:20.509
Mike Nelson: Well, but both of them imply intent.

136
00:16:21.310 --> 00:16:23.170
Gil Friend • Sustainability OG • CxO Coach: Bullshit, not necessarily.

137
00:16:23.170 --> 00:16:25.060
Mike Nelson: Oh, that's true.

138
00:16:25.060 --> 00:16:28.919
Gil Friend • Sustainability OG • CxO Coach: But then, yeah, so there's different kinds. There's the intentional, and there's the stupid.

139
00:16:29.000 --> 00:16:33.129
dbrei: Or, or lazy, or callous, and so forth, so all that. ….

140
00:16:33.200 --> 00:16:49.360
Gil Friend • Sustainability OG • CxO Coach: But anyhow, you know, too many questions to respond to. I will note that on Living Between Worlds next Wednesday, Ken and I are hosting a guy named Stuart Hill, Emeritus Professor in Social Ecology, who's going to talk about why he wants us to lie and lie boldly.

141
00:16:49.770 --> 00:16:54.529
Gil Friend • Sustainability OG • CxO Coach: That's an important social change strategy, so tune in next Wednesday if you want to hear more about that.

142
00:16:54.530 --> 00:16:56.870
Jerry Michalski: Nice. Can you give us a link to Stuart, please?

143
00:16:57.350 --> 00:16:59.280
Gil Friend • Sustainability OG • CxO Coach: … maybe.

144
00:16:59.460 --> 00:17:02.019
Jerry Michalski: Oh, okay, I'll Google him, don't worry.

145
00:17:02.020 --> 00:17:08.109
Gil Friend • Sustainability OG • CxO Coach: I learned this technology lesson on Monday that you really shouldn't pour protein drink onto your laptop.

146
00:17:08.119 --> 00:17:08.989
Jerry Michalski: Oh, shoot.

147
00:17:10.440 --> 00:17:18.400
Gil Friend • Sustainability OG • CxO Coach: So I'm working on Pad this week, which is actually not too bad. It's getting better. That's the Pad Central experience, but there's some things I can't do quickly.

148
00:17:18.930 --> 00:17:22.149
Jerry Michalski: We'll do tagline for them. Tad, not too bad.

149
00:17:22.750 --> 00:17:24.940
Jerry Michalski: Kevin, thank you for your patience.

150
00:17:25.109 --> 00:17:30.029
Kevin Jones: Yeah, thank you. Take my hand down. …

151
00:17:30.030 --> 00:17:54.930
Kevin Jones: I'm going back to your initial thing of East Germany and the trusted channel and non-trusted channel, I would suggest we try to make the OGM list a trusted channel, and there have been things on the OGM list that have been like, you know, GPT is the best thing since ever, but if you look there, it was on Medium by somebody with a made-up name with no external

152
00:17:54.930 --> 00:17:58.419
Kevin Jones: validation, and he looked at everything else that that person has written.

153
00:17:58.640 --> 00:18:08.960
Kevin Jones: There is no external validation, so don't send things to this list that you haven't validated, or you can see that that source

154
00:18:09.020 --> 00:18:26.729
Kevin Jones: doesn't validate. So, create a trusted channel here. And secondly, I'm with Tolstoy. You use the quote for the truth that it has. I mean, I've learned this from Indigenous folks. They often say, it may not have happened, but if you think about it, you can see that it's true.

155
00:18:26.740 --> 00:18:41.410
Kevin Jones: So listen for the truth, you know, and there's so many things you don't know… need to know whether it's true or not. I really don't pay much attention to all the things that people are wanting to say is scary, because

156
00:18:41.810 --> 00:18:59.040
Kevin Jones: scariness now is a collective, viral thing that comes to you, and you can be infected by other people's scaredness. And I, you know, I have no time for other people's scaredness, and so it doesn't bother me much. I, you know, I subscribe to

157
00:18:59.140 --> 00:19:10.189
Kevin Jones: The Atlantic, and the New Yorker, and the Post, and the Times, and actually, the Apple News app is really quite good and extensive. I'm surprised at it.

158
00:19:10.670 --> 00:19:13.699
Kevin Jones: And, you know, a few other things, you know, the…

159
00:19:14.000 --> 00:19:27.230
Kevin Jones: Politico, whatever, when I need to, but, you know, you don't need to know all these lies, and you don't need to follow them, and just, like, listen for the truth, and don't get scared, you know, but…

160
00:19:27.230 --> 00:19:41.390
Kevin Jones: Let's create a trusted channel at OGM and stop sending things that you haven't checked out that are unvalidated, and you haven't gone deep. You know, treat the… I wish people would treat this, the OGM list, with more respect.

161
00:19:41.730 --> 00:19:45.579
Kevin Jones: And that's my request. Don't send the crap that is often there.

162
00:19:47.230 --> 00:19:48.110
Jerry Michalski: …

163
00:19:48.340 --> 00:19:55.939
Jerry Michalski: Several things you've touched that are very worth bringing into the conversation. Kevin, thank you. The incident you just described is one of the reasons for this call.

164
00:19:56.550 --> 00:20:00.209
Jerry Michalski: I'm like, yep, we gotta sort of sort this out a little bit better ourselves together.

165
00:20:02.030 --> 00:20:09.799
Jerry Michalski: Ken, Ken Homer has started a separate list, where there's information being shared and all that.

166
00:20:09.800 --> 00:20:27.389
Jerry Michalski: on, Signal, which is the more… most of the secure messaging systems, as far as we can tell, because WhatsApp is owned by Facebook, Telegram is owned by Vicom Takte, which is Russian, oh my god. Never mind Facebook and, you know, all the other places, but… but,

167
00:20:27.980 --> 00:20:29.879
Jerry Michalski: But that one is actually,

168
00:20:30.700 --> 00:20:41.409
Jerry Michalski: probably the most secure one, which is why, of course, our Secretary of Defense was using it to plan military strategies and boast about things, early in the new administration.

169
00:20:41.590 --> 00:21:01.929
Jerry Michalski: Anyway, I never follow Apple News links, because I don't use any Apple software other than the damned operating system. I hate, like, iMusic, iTunes, Apple News, I don't use any of those, so I don't… whenever I click on an Apple News link, it says, hey, you want to start up Apple News? And I'm like, yeah, no, I don't, not for a moment.

170
00:21:02.770 --> 00:21:09.849
Jerry Michalski: So, if we could follow… if we could follow a protocol, and maybe one of the things we do in this call is we elaborate a protocol for better.

171
00:21:09.850 --> 00:21:26.870
Jerry Michalski: better, more trustworthy conversations in some sense, and we might even publish that. That would be a good thing. I think we would benefit from doing so. But maybe the protocol is, hey, publish a link, and if there's a paywall, go ahead and do the work of going to archive.is or .ph.

172
00:21:26.870 --> 00:21:45.159
Jerry Michalski: and offering the alternative, circumventing of the paywall link right there, as the contributor. Just do a tiny, tiny bit of extra work, so that… but have both, the naked link, so that we know where the actual source was, and the archive link, so that it's, reachable by everybody.

173
00:21:45.820 --> 00:21:46.810
Jerry Michalski: And then…

174
00:21:47.050 --> 00:22:09.870
Jerry Michalski: There are some cases in which we might, Kevin, we might want to forward a crazy-ass theory or something that's not very supported, but we should frame it as such. We should say, hey, I just saw this thing that might go viral, that might be another Baltimore Ravens story. It seems really unsupported, but here it is, so you know about it and can watch out for it, or comment on it, or whatever else. So I'm not sure we need a ban

175
00:22:09.870 --> 00:22:17.749
Jerry Michalski: on unsupported stuff, I think we need a frame… a frame around unsupported stuff that says, hey, this looks pretty uncredible.

176
00:22:17.750 --> 00:22:18.890
Jerry Michalski: But I like…

177
00:22:19.020 --> 00:22:27.280
Jerry Michalski: you know, I like the fact that it might actually travel, or it's going to affect us, or watch out for the truck, something like that.

178
00:22:27.880 --> 00:22:51.689
Jerry Michalski: And so, maybe a thing we can focus on is protocols. Just as we talk, let's just derive out what do we want to do as the OGM list, and make those available to whoever. I'm willing to bet that there are other online communities that have already done this, whose protocols we could find and adopt, or something like that. This is a seemingly… this is low-hanging fruit for anybody who wants to have a

179
00:22:51.690 --> 00:22:55.140
Jerry Michalski: A reasonable community conversation in these times online.

180
00:22:56.890 --> 00:23:03.080
Jerry Michalski: sorry, you just, Gil, you had your hand up, and then you took your hand down, because it's the iPad.

181
00:23:03.080 --> 00:23:13.709
Gil Friend • Sustainability OG • CxO Coach: I saw it was up from Legacy, but I wanted to raise it for just now. One of the things I've been really encouraging people to do on their various social media posts is to always post source.

182
00:23:14.760 --> 00:23:19.020
Gil Friend • Sustainability OG • CxO Coach: I see a lot of people who are, like, you know, just, like, quoting something from somewhere.

183
00:23:19.380 --> 00:23:26.009
Gil Friend • Sustainability OG • CxO Coach: But not, you know, not clarifying which words are theirs and which words are the ones they're quoting from, and also where is the source from.

184
00:23:26.130 --> 00:23:42.440
Gil Friend • Sustainability OG • CxO Coach: And I'm just not inclined to read anything that isn't sourced by somebody, so I'm sort of echoing Kevin in a variation of what Kevin was saying, and Kevin, I liked what you said. I would encourage us to do that, not just in the OGM community, but everywhere we… everywhere we read and write.

185
00:23:42.730 --> 00:23:53.199
Gil Friend • Sustainability OG • CxO Coach: Is, you know, be forces for good information hygiene. Brain protection. Oh, the other thing on brain protection, Mike Nelson's point earlier on, Finland has this as part of their public school curriculum.

186
00:23:54.220 --> 00:23:54.660
Jerry Michalski: Yes.

187
00:23:54.660 --> 00:23:57.380
Gil Friend • Sustainability OG • CxO Coach: A lot of things we could learn from Finland, that's a good one.

188
00:23:57.510 --> 00:23:58.300
Jerry Michalski: Yep, yep.

189
00:23:58.300 --> 00:24:02.370
Gil Friend • Sustainability OG • CxO Coach: In the United States, critical thinking is taken out of school curriculums.

190
00:24:02.570 --> 00:24:03.180
Judith Benham: Right.

191
00:24:03.530 --> 00:24:06.470
Jerry Michalski: Hi, Stacy! And, Judy, your turn.

192
00:24:06.980 --> 00:24:10.679
Judith Benham: Well, I guess I wanted to throw in the word credible.

193
00:24:10.780 --> 00:24:19.880
Judith Benham: Because it seems to me that credibility is at the heart of this, but there's discernment associated with credibility.

194
00:24:19.880 --> 00:24:39.509
Judith Benham: And there's sourcing, also. And so, I think if we could, in some mechanism, have a credibility factor that we could put on, on a… whatever scale you would want, or however we might want to do it, or at least share why someone thinks it's a credible source, so that we can

195
00:24:39.530 --> 00:24:44.039
Judith Benham: further evaluate on our own, I think that would be useful for the community.

196
00:24:46.410 --> 00:24:47.769
Jerry Michalski: Love that. Thank you, Judy.

197
00:24:48.770 --> 00:24:49.970
Jerry Michalski: Mr. Patello.

198
00:24:51.250 --> 00:25:00.569
Rick Botelho: Yeah, I'd like to throw maybe a different perspective for you guys something what Mike was talking about. Several things I reacted to, a sort of yes-and or yes-no sort of response to what you threw out, which is great.

199
00:25:00.670 --> 00:25:05.480
Rick Botelho: And my throw-out is, I think trust is overrated.

200
00:25:05.760 --> 00:25:12.840
Rick Botelho: And, you know, what Judith just talked about were… about discernment. I mean, I think in terms of,

201
00:25:12.940 --> 00:25:16.980
Rick Botelho: Ethical discernment, verification, And, …

202
00:25:18.450 --> 00:25:21.740
Rick Botelho: what was the word I was gonna come to? It's just going… I'm flying out of my mind.

203
00:25:22.030 --> 00:25:24.220
Rick Botelho: Truth-seeking.

204
00:25:24.610 --> 00:25:39.089
Rick Botelho: Because… True-seeking, true-seeking. The reason for that is you can trust somebody, but they may be giving you disinformation or misinformation. You trust them, so you think, okay, on the other hand, somebody you don't trust, they may be telling something that you should know about.

205
00:25:39.210 --> 00:25:46.189
Rick Botelho: So then you get into the, the bias of distrust versus trust. So to me, trust is an outcome.

206
00:25:46.190 --> 00:25:59.330
Rick Botelho: And it's circumstantial. It's a particular setting or a thing. It's not generic. It's not something that you, you know, overgeneralize, because that's where you start making mistakes, because

207
00:25:59.330 --> 00:26:05.420
Rick Botelho: You trust something more than what it's actually falling out to be on that particular issue.

208
00:26:05.540 --> 00:26:21.259
Rick Botelho: So, I'm just calling out trust as something that's overrated, and if we focus on discernment, verification, truth-seeking, then we can… and the other point that I want to respond to, Mike, about your brain defense, I have less issues about, you know, different domains of

209
00:26:21.470 --> 00:26:24.709
Rick Botelho: Expertise, media literacy, critical thinking, yeah, yeah.

210
00:26:24.910 --> 00:26:29.059
Rick Botelho: Which is, really about emancipatory learning.

211
00:26:29.310 --> 00:26:37.850
Rick Botelho: Which is, how do we help people become open-minded, Truth-seeking, virtuous freethinkers.

212
00:26:37.960 --> 00:26:42.069
Rick Botelho: Which creates a place where you can have much healthier disagreements.

213
00:26:42.370 --> 00:26:48.609
Rick Botelho: And I'll just tell you a brief story about something that happened to me. I've been being active on Indivisible

214
00:26:48.760 --> 00:26:52.360
Rick Botelho: And a highly sensitive person attacked me.

215
00:26:52.630 --> 00:26:53.320
Rick Botelho: Huh.

216
00:26:53.430 --> 00:26:55.629
Rick Botelho: and said I was a hostile bully.

217
00:26:56.000 --> 00:27:03.479
Rick Botelho: And so, I actually went into and did research on highly sensitive people and hostility responses.

218
00:27:03.890 --> 00:27:10.280
Rick Botelho: And lo and behold, I discovered something. There is something called hostility attribution bias.

219
00:27:11.070 --> 00:27:12.709
Rick Botelho: Have you heard that one before?

220
00:27:12.940 --> 00:27:19.420
Rick Botelho: Which is when sensitive people, you may say something slight, and it triggers their amygdala to go on to the attack.

221
00:27:19.740 --> 00:27:27.740
Rick Botelho: And we've all experienced it, but I thought, oh, that's a… that's a… that's… that's a new bias for my… my repertoire of describing biases, so…

222
00:27:27.790 --> 00:27:44.789
Rick Botelho: It was a… there was a lot more under the hood of that story, but it's interesting going to events where it's a very heterogeneous group of people, of educational backgrounds, and yada yada, because it gives you a much better feel for what's happening

223
00:27:45.890 --> 00:27:49.360
Rick Botelho: Outside the circles of symbolic capitalists.

224
00:27:49.920 --> 00:27:51.230
Rick Botelho: I'm done speaking.

225
00:27:53.200 --> 00:27:54.200
Jerry Michalski: Thanks, Rick.

226
00:27:54.470 --> 00:27:55.340
Jerry Michalski: Flow!

227
00:27:55.950 --> 00:27:57.200
Slow: Oh my goodness.

228
00:27:57.550 --> 00:27:58.640
Slow: So much.

229
00:27:59.200 --> 00:28:16.300
Slow: Just in response to that last bit, I wanted to mention that Marshall Rosenberg would often, the guy who started nonviolent communication, he would often, you know, share his experience that unexpressed emotions are very often interpreted as anger.

230
00:28:16.820 --> 00:28:22.329
Slow: So this is a reason to share one's emotions, even when one doesn't feel a need to.

231
00:28:22.330 --> 00:28:46.329
Slow: You know, and the NBC thing, which is supposed to make that… you know, a lot of people get freaked out when you share emotions, so the NBC advice is to immediately, when you share the emotion, I'm feeling blah blah blah, because… and then reveal the values or needs, you know, that led to that emotion coming up, positively or negatively, and that way people are… you're exposing the vulnerability about yourself.

232
00:28:46.570 --> 00:28:54.579
Slow: you know, where it came from, and they're less likely to be activated by that than just hearing that you're mad, or confused, or whatever it was. …

233
00:28:54.770 --> 00:29:14.379
Slow: I've been interested in this question of sources for a very, very, very long time, from, super explicitly from when I was, you know, in the early blogging days, following the development of RSS and everything. I, just recently, last month.

234
00:29:14.540 --> 00:29:20.699
Slow: started with… someone else started, but I joined very early, a national project

235
00:29:21.070 --> 00:29:26.290
Slow: That has evolved into… for a very broad audience, there's some…

236
00:29:26.360 --> 00:29:44.149
Slow: we're working out now whether it's going to be really everybody, or kind of a centrist to left thing, because it's been portrayed different ways, and we've drawn people who are enough interested in… include everybody, and I'm like, what about the right Christian nationalists? And they're like, that's not

237
00:29:44.920 --> 00:29:55.849
Slow: That's not what the Republican Party is about, and I'm saying, I'm not saying they all are, but they are there. Like, are we trying to include them in this process? You know, anyway, we'll have that drag-out conversation, but…

238
00:29:55.850 --> 00:30:15.179
Slow: regardless, this project, we will definitely have an ongoing thing of which sources are we putting in the system, and which sources are we not? So we'll be dealing with this question in a very practical way, over and over again. If anybody is interested to be in that, even as a thing by itself.

239
00:30:15.200 --> 00:30:24.180
Slow: that might, you know, be helpful to the project. In any case, I'd be interested by itself. If people from this group wanted to start a signal group just to talk about

240
00:30:24.220 --> 00:30:48.370
Slow: sources, and kind of develop a list together, you know, and individually, whatever we want to do with it, but to develop our sensibilities, you know, and some specific language, you know, and specific questions that we help us, you know, in evaluating sources. And I appreciate all the things people have said about that, that, that even the ones we decide are not on our

241
00:30:48.370 --> 00:31:00.109
Slow: we pretty much trust what we get, you know, when we see things from there. There's a ton of sources that I think it's really important to share, you know, with a framing. Yeah, this isn't on our list, but I'm sharing it because blah blah blah blah.

242
00:31:00.110 --> 00:31:11.160
Slow: And an obvious example around that right now would be in Gaza, you know, more journalists have been killed than in any of the other conflict, you know, in whatever history.

243
00:31:11.160 --> 00:31:31.050
Slow: The comments, you know, when stuff about this is posted anywhere, are so awful that the journalism subreddit on… which has been really good, and there's stuff about press freedom all the time, you know? But the comments got so bad that the moderators have banned all posts about the war.

244
00:31:32.040 --> 00:31:49.399
Slow: completely blows my mind. Anyway, so I'm in process with them, but … and the last thing is I was working for a little while with Eason on an idea. We ended up realizing we're not ready to go do it, but we had the idea of developing a newsreader

245
00:31:49.630 --> 00:31:50.630
Slow: …

246
00:31:50.880 --> 00:32:10.879
Slow: With two features we hadn't seen anymore, and I won't mention one, but the second one was kind of an anti-echo chamber feature. So you say, here are my feeds, now you, the newsreader, I turn on this button, show me 5% of my feed, make 5% of my feed things that are very different perspective.

247
00:32:11.280 --> 00:32:13.150
Slow: from all the sources I'm following.

248
00:32:13.560 --> 00:32:34.629
Slow: So, it would have to do… it would have to have access to a lot of feeds, and then it would have to do some algorithmic whatever, and then it would just throw into your mix, like, every once in a while, you're like, oh my god, here's a thing from a KKK publication, oh my god, here's a thing from Breitbart, you know, or whatever, whatever, all kinds of sources that are not the ones I'm subscribed to.

249
00:32:36.580 --> 00:32:37.390
Slow: Check.

250
00:32:37.900 --> 00:32:42.369
Jerry Michalski: Thank you very much. You just reminded me of a wishlist item I had back in the days.

251
00:32:42.720 --> 00:32:51.850
Jerry Michalski: of Twitter, which was, I wanted them to offer a feature that would let you say, hey, show me on my feed all the shit I've forwarded that's wrong.

252
00:32:52.200 --> 00:32:53.320
Judith Benham: That's just a joke.

253
00:32:53.750 --> 00:33:01.579
Jerry Michalski: that's just notably wrong. Just, you know, just put a… put… put my feed… put my feed or someone else's feed through the… through the washer.

254
00:33:01.580 --> 00:33:02.020
Slow: Yeah.

255
00:33:02.020 --> 00:33:07.069
Jerry Michalski: And let… let me… because if there's no… if there's no feedback loop.

256
00:33:07.430 --> 00:33:15.780
Jerry Michalski: None of us get better at this. None of us get wiser at this. The information just gets worse. The morass gets deeper and darker and stickier.

257
00:33:16.410 --> 00:33:27.160
Jerry Michalski: So we need to bake into the systems and services somehow, and part of our problem is that the platforms which are the largest

258
00:33:27.160 --> 00:33:39.489
Jerry Michalski: most profitable businesses on the planet. Well, not… there are some private companies that are more profitable, I'm sure, but the platforms that are the dominant players on the planet, that are now the broligarchs who are ruling a lot of things.

259
00:33:39.490 --> 00:33:47.250
Jerry Michalski: Are dominated by the consumer mass market business model, which means algorithm culture, which means addiction by design.

260
00:33:47.320 --> 00:33:57.119
Jerry Michalski: And that is a given in our environment, that we haven't had any say in fighting, but is now the reality on the ground, which is half of what's happening here, is that

261
00:33:57.460 --> 00:34:03.429
Jerry Michalski: Is that we're being led toward mis- and disinformation all the time by the platforms

262
00:34:03.570 --> 00:34:16.849
Jerry Michalski: Because it glues people to their screens. And that's it. You know, that's one big piece of the problem. And unless we have feedback loops that help us get smarter, and unless we disable the algorithmic

263
00:34:17.050 --> 00:34:23.169
Jerry Michalski: … addictive loop, we're kind of hosed here. I don't think this plays out very well.

264
00:34:23.590 --> 00:34:26.270
Jerry Michalski: End of rant, on to Mike.

265
00:34:26.840 --> 00:34:39.949
Mike Nelson: I have spoken almost as much as Jerry, and so I'm going to turn it over to Judith, and then you can come back to me. I would just add one thing, though, and we do need to restore shame.

266
00:34:40.469 --> 00:34:49.539
Mike Nelson: So a feedback loop only works if you're embarrassed when you're caught out. But anyway, Judith, you… I'd love to hear what you have to say, and then I'll come back in on a couple other things.

267
00:34:50.560 --> 00:34:53.940
Judith Benham: Well, I was just going to comment that something I started

268
00:34:54.060 --> 00:35:05.330
Judith Benham: probably a decade ago, because of my dissatisfaction with credibility and the issues, was I always search X, and then I immediately search anti-X.

269
00:35:05.330 --> 00:35:18.189
Judith Benham: on different platforms. And when you see the arguments on both sides laid out by multiple individuals, because if you search anti-X, you'll get a list of people, and you can pick who you want, it helps frame…

270
00:35:18.190 --> 00:35:20.509
Judith Benham: The scope of the disagreement

271
00:35:20.510 --> 00:35:30.119
Judith Benham: and the credibility, and it helped me to figure out, you know, is this believable or not? I want to understand both sides of the issue and form my own opinion based on

272
00:35:30.340 --> 00:35:44.950
Judith Benham: what both sides of an issue are saying. And it's a little time-consuming, but I still find it useful to do that. And if there were a way to systemize that, that would be helpful. But, I just thought I'd toss that out, because I think

273
00:35:45.250 --> 00:35:54.830
Judith Benham: We're responsible, in a sense, for our own belief of whatever we read, and that's a tool we could use individually to explore the zone.

274
00:35:58.050 --> 00:36:07.470
Mike Nelson: I love that, and I would add, though, one of the challenges is you might go searching for a buzzword.

275
00:36:07.670 --> 00:36:11.610
Mike Nelson: That is used by people on one side of the issue.

276
00:36:12.160 --> 00:36:14.800
Mike Nelson: And to hear the counter-arguments.

277
00:36:15.550 --> 00:36:20.980
Mike Nelson: You have to somehow find out what the buzzwords used by the other side are.

278
00:36:21.410 --> 00:36:24.889
Mike Nelson: So if you… if you didn't know that white genocide

279
00:36:25.220 --> 00:36:31.599
Mike Nelson: was how Elon Musk and racists in South Africa refer to

280
00:36:32.000 --> 00:36:36.500
Mike Nelson: Legitimate efforts to, tax white farmers.

281
00:36:36.690 --> 00:36:43.549
Mike Nelson: you know, you wouldn't… you wouldn't know, you wouldn't find those arguments. You might not. But it, it is… it is almost like…

282
00:36:43.720 --> 00:36:58.969
Mike Nelson: two or three different languages, and if we don't have the jargon, we won't find it. But let me get back to two other things that have to do with the business model. And the reason I have some hope

283
00:36:59.140 --> 00:37:01.960
Mike Nelson: That we'll find a way to…

284
00:37:02.990 --> 00:37:06.609
Mike Nelson: Make money and do good and spread truth.

285
00:37:06.940 --> 00:37:17.330
Mike Nelson: is because… A key piece of this puzzle is… attribution, attributes, and identity.

286
00:37:18.580 --> 00:37:21.579
Mike Nelson: and… I think there are businesses.

287
00:37:22.210 --> 00:37:30.549
Mike Nelson: models that would allow us to associate my identity With certain attributes about me.

288
00:37:30.920 --> 00:37:42.120
Mike Nelson: And allow me to use the well-established identity to kind of put my watermark or my fingerprint on something that I post.

289
00:37:43.200 --> 00:37:48.729
Mike Nelson: When I was at the White House in the Clinton years, we set up the White House website.

290
00:37:49.050 --> 00:37:53.570
Mike Nelson: And we had this feature that allowed people to send an email to the president.

291
00:37:55.140 --> 00:37:58.360
Mike Nelson: Can you guess what happened to those emails?

292
00:37:58.710 --> 00:37:59.590
Jerry Michalski: Hmm.

293
00:37:59.760 --> 00:38:06.080
Mike Nelson: They were printed out and put in the Big bin of regular mail.

294
00:38:06.730 --> 00:38:10.510
Mike Nelson: We always ask people for their regular postal address.

295
00:38:10.510 --> 00:38:11.260
Jerry Michalski: Wow.

296
00:38:11.260 --> 00:38:19.689
Mike Nelson: And the reason was… We could not have Bill Clinton sending emails to people in response to their comments.

297
00:38:19.900 --> 00:38:23.820
Mike Nelson: Over a regular… email service.

298
00:38:24.130 --> 00:38:28.290
Mike Nelson: It would just lead to too much mischief. You know, people could just…

299
00:38:29.760 --> 00:38:45.340
Mike Nelson: alter what they were sent. It would look like a real White House email, but the word not would be deleted, or, you know, some new sentence would be added. So, everything was done in the old-fashioned way. Even… even letters to Sox the Cat.

300
00:38:45.600 --> 00:38:47.629
Judith Benham: Had to be sent on….

301
00:38:48.030 --> 00:38:55.250
Mike Nelson: So, this is… this is partly about making… data… And…

302
00:38:55.400 --> 00:39:02.030
Mike Nelson: Content. Traceable, having a clear way to establish provenance.

303
00:39:02.320 --> 00:39:07.350
Mike Nelson: But I said something important earlier, and that was we don't just need identity.

304
00:39:07.910 --> 00:39:23.179
Mike Nelson: we needed some kind of management system for attributes that matter. And so, I might have 58 different attributes that, in one setting or the other, I would want to use and share.

305
00:39:23.460 --> 00:39:39.469
Mike Nelson: often without my true identity, and without my address and phone number and email address, I might want to just buy some alcohol online, and I just need to send some marker saying, I'm over 21.

306
00:39:39.560 --> 00:39:46.610
Mike Nelson: And I live in a state where I can buy alcohol online and have it delivered to me. I mean, that's…

307
00:39:46.910 --> 00:39:48.899
Mike Nelson: Part of the answer

308
00:39:49.550 --> 00:39:57.689
Mike Nelson: that works in a way that protects as much privacy as I need, and allows me to customize the services I buy.

309
00:39:58.790 --> 00:40:06.930
Mike Nelson: The other reason this is so useful in terms of truth, is… and trust, is because

310
00:40:07.450 --> 00:40:16.160
Mike Nelson: as I said earlier, the amplifiers are a big part of the problem. A lot of the crap that we're seeing on the social media platforms are

311
00:40:16.290 --> 00:40:21.109
Mike Nelson: promoted by the trolls, and it's a concerted

312
00:40:21.740 --> 00:40:28.729
Mike Nelson: $100 million effort to make sure things move up in the algorithms, and Elon Musk is helping it.

313
00:40:29.320 --> 00:40:38.590
Mike Nelson: If we know who's actually forwarding this stuff, and can track it back to trolls in St. Petersburg, we can eliminate

314
00:40:39.550 --> 00:40:40.380
Mike Nelson: half.

315
00:40:40.770 --> 00:40:54.759
Mike Nelson: 80% of the problem, and the manipulation of the algorithms. But you can only… you can do that so much faster if you have the equivalent of a real blue checkmark that, you know, allows me.

316
00:40:54.760 --> 00:40:58.680
Jerry Michalski: Do not even bring the blue checkmark up after Musk took over Twitter. Oh, God.

317
00:40:58.680 --> 00:41:00.610
Mike Nelson: Yeah, as I said, the real.

318
00:41:00.610 --> 00:41:05.099
Jerry Michalski: Yeah, I know, but that just pains me. It gives me a pain in the heart when you say that.

319
00:41:05.100 --> 00:41:16.100
Mike Nelson: Yeah, well, we need… we need a nice analogy, and… Yeah. The last thought I put in the chat is I'm… after… I'm gonna leave a little early to go hear Francis Fukuyama.

320
00:41:16.680 --> 00:41:21.019
Mike Nelson: And, he wrote a book more than 15 years ago, just called Trust.

321
00:41:22.120 --> 00:41:24.429
Mike Nelson: Some of you may have read it.

322
00:41:24.560 --> 00:41:26.989
Mike Nelson: I find it incredibly useful.

323
00:41:27.250 --> 00:41:34.520
Mike Nelson: A little outdated now, But he examines 6 countries, 3 high-trust countries, and 3 low-trust countries.

324
00:41:34.750 --> 00:41:39.579
Mike Nelson: and shows how the lack of trust in Italy, for instance.

325
00:41:39.800 --> 00:41:53.760
Mike Nelson: leads to certain types of corporate structures. A lot of the biggest companies in Italy are still built around a family firm, where people could trust their second cousins, and they could actually build fiat.

326
00:41:54.130 --> 00:41:58.359
Mike Nelson: And have the key positions controlled by one family.

327
00:41:58.960 --> 00:42:03.350
Mike Nelson: In contrast, you know, Japan, very high trust.

328
00:42:03.540 --> 00:42:04.960
Mike Nelson: You shake a hand.

329
00:42:05.200 --> 00:42:06.519
Mike Nelson: You can trust that

330
00:42:06.750 --> 00:42:17.720
Mike Nelson: that deal will… will… will be maintained. But anyway, lots going on, and this is an incredibly rich conversation for me, and I… I hope,

331
00:42:18.050 --> 00:42:29.300
Mike Nelson: if anybody has comments on what Carnegie is doing on disinformation and the information environment, or if you want an introduction to Alicia Wanless, just let me know.

332
00:42:30.430 --> 00:42:36.840
Jerry Michalski: Mike, thank you. I wanted to add one thing. You say a lot of things when you talk, so I have to keep notes.

333
00:42:37.800 --> 00:42:57.069
Jerry Michalski: One of the very good arguments for self-sovereign ID or other forms of digital ID that do what Mike just said is, and the scenario is memorable, and I love it. You're a young woman getting carded to go into a nightclub or a bar. Your driver's license contains all the information anybody needs to stalk you.

334
00:42:57.540 --> 00:43:02.019
Jerry Michalski: You should be able to pass through that doorway with just something that says, yes.

335
00:43:02.040 --> 00:43:18.819
Jerry Michalski: I am old enough to go into this space. Yes, I am a citizen and I can vote. You know, move it around, but the cool aspect of a functional, trustworthy ID system is that it reveals only the information that is needed about you at the moment.

336
00:43:19.150 --> 00:43:27.379
Jerry Michalski: and it's reliable, and blah blah blah blah blah. Those things are really complicated to do, and worse, they have no business model. So…

337
00:43:27.630 --> 00:43:35.840
Jerry Michalski: a lot of the startups that have tried to do really good context stuff, and Kalia Hamlin, who's often on calls here, Kalia Young.

338
00:43:36.480 --> 00:43:50.500
Jerry Michalski: identity woman. She has been working with a lot of these companies. She's been running the internet identity workshops for, oh, I think 15 or more years now with Joc Searles as an open space workshop, which has been highly fruitful.

339
00:43:51.010 --> 00:44:05.619
Jerry Michalski: And yet, our culture doesn't have, besides a couple, like, you know, Google ID and Apple ID-ish kind of things, or use your LinkedIn profile, we don't really have that. And the threat of national digital IDs under Trump or Xi

340
00:44:05.820 --> 00:44:18.090
Jerry Michalski: It's not a fun prospect. That doesn't feel really good. I don't know. You might… your mileage may vary on that one, but I don't feel good about that. So we really desperately need these kinds of things, and …

341
00:44:18.340 --> 00:44:20.590
Jerry Michalski: Off to you in the booth, Rick.

342
00:44:23.500 --> 00:44:34.330
Rick Botelho: Oh, thank you. Yeah, I just want to, again, throw another perspective. I think, looks like Mike's left already, but that's fine. A different perspective on what he just said, because I see trust as an outcome.

343
00:44:34.520 --> 00:44:46.689
Rick Botelho: And, that, that changed your attributions about issues. So, a different framing. In a psychopathic system, such as our political system currently.

344
00:44:46.880 --> 00:44:49.299
Rick Botelho: Then there is low trust.

345
00:44:49.660 --> 00:44:52.410
Rick Botelho: Whereas, if it's a system of integrity.

346
00:44:52.520 --> 00:44:56.300
Rick Botelho: You're more likely to have trust in those institutions.

347
00:44:56.360 --> 00:45:08.859
Rick Botelho: So, at the moment, there's an assault on all institutions of academia, institutes, policy institutes, they're all under siege because we have a systemic psychopathic system

348
00:45:08.860 --> 00:45:22.069
Rick Botelho: that's devoid of the typical shaming responses that would normally curtail that type of behavior. We're living in a situation where the reverse is the case. It's actually validated, and encouraged, and incited.

349
00:45:22.260 --> 00:45:40.340
Rick Botelho: And so, I just want to put that different perspective out there. I want to return to something which you're talking about from a different perspective about looking at the different points of view across an issue, which is, I think, incredibly important. And I've encountered this within this indivisible group.

350
00:45:40.410 --> 00:45:49.400
Rick Botelho: Where they are, you know, you know the old, group dynamics of, you know, forming, norming, storming, performing?

351
00:45:49.420 --> 00:46:02.880
Rick Botelho: This group is… has a case of arrested development because it's so cohesive and norming that it can't work through disagreements. It can disagree about what it's against. That's easy.

352
00:46:03.090 --> 00:46:05.369
Rick Botelho: But trying to say what they're for

353
00:46:05.420 --> 00:46:13.940
Rick Botelho: is much more difficult, and these were the progressive movements fall over each other, and are ineffective. And I'll give you an example from personal experience.

354
00:46:13.980 --> 00:46:26.020
Rick Botelho: I joined this group called Stand Up for Racial Justice, and in that, I started writing, and I started writing about deranged Trump mass psychosis as an explanatory model for a complex socio-political phenomenon.

355
00:46:26.200 --> 00:46:41.439
Rick Botelho: And I got taken to the cleanest by them, and they were… and even a highly sensitive psychiatrist accused me of being an ableist, another example of hostility attribution bias.

356
00:46:41.530 --> 00:46:50.840
Rick Botelho: And I was just, you know, thinking, where's this coming from, you know? And then they got to a point where they banned all of my references to that construct.

357
00:46:51.140 --> 00:46:52.589
Rick Botelho: And they took it out.

358
00:46:52.780 --> 00:46:59.610
Rick Botelho: And I pointed, yes, wow, I know, wow, talk about political, talk about PC, progressive wokeness!

359
00:46:59.690 --> 00:47:02.240
Judith Benham: And they… and I just simply said.

360
00:47:02.240 --> 00:47:21.949
Rick Botelho: look, take everything I've put in here out. I don't want any of my stuff in here. I'm saying goodbye, and this is an example of the… now, I have to give… you have to give credit to the unwoke… progressive unwoke brigade. They're far more effective than the progressive woke

361
00:47:21.950 --> 00:47:34.770
Rick Botelho: or PC Woke Brigade. But anyway, I just… I just share those examples of… oh, by the way, one thing, Stacey, you mentioned sensitivity, I noticed that. You have to watch the movie if you haven't seen it yet.

362
00:47:34.770 --> 00:47:44.330
Rick Botelho: called Sensitive. It's a brilliant documentary by the psychologist who described the construct, the psychological character trait of sensitivity.

363
00:47:44.410 --> 00:47:54.839
Rick Botelho: Absolutely brilliant, and she highlights Alanta Morissette as an example of that character trait. If you haven't seen it, you've got to watch it, because

364
00:47:54.840 --> 00:48:04.729
Rick Botelho: It'll enable you to identify people more quickly and understand how to act… re-interact with them a little more sensitively.

365
00:48:05.590 --> 00:48:07.459
Stacey Druss: Can I just say one thing real quick?

366
00:48:07.710 --> 00:48:08.400
Jerry Michalski: Please.

367
00:48:08.700 --> 00:48:16.750
Stacey Druss: Yeah, I just want… no, I know it's sensitive, I try to stay away from those people. I wanted to point out that oftentimes.

368
00:48:16.890 --> 00:48:20.620
Stacey Druss: That also goes for people who have had some sort of trauma in that.

369
00:48:20.620 --> 00:48:30.069
Rick Botelho: Exactly, yes, exactly. Absolutely. It's not always that she describes it as a distinct character trait that's made worse by trauma.

370
00:48:30.510 --> 00:48:34.109
Rick Botelho: So, you have to… it's not always trauma-informed.

371
00:48:34.230 --> 00:48:47.259
Rick Botelho: It's a particular… and if you hear the story of Atlantomoresis growing up, and her difficulties adapting to her sensitivity, why she felt alienated from her peers, it's an amazing story.

372
00:48:47.520 --> 00:48:54.980
Stacey Druss: I just want to say I'm a highly sensitive person, and I don't react that way, so I wanted to make the distinction that there's.

373
00:48:54.980 --> 00:48:55.760
Rick Botelho: Oh, yeah, yeah.

374
00:48:55.760 --> 00:48:58.409
Stacey Druss: It's sensitive, and it comes out in different ways.

375
00:48:58.850 --> 00:49:13.809
Rick Botelho: Yeah, well, I think the point… what the psychologist was talking about was she got into it to be able to do exactly what you said. Actually, there's another one on… I forgot, it's an Aspen talk about a woman who described her own psychopathy.

376
00:49:13.840 --> 00:49:28.130
Rick Botelho: And it's an absolutely brilliant book. I forgot the name, I have to look it up. But she describes how she identified herself as a psychopath, became a psychologist, and learned how to deal with a psychopath.

377
00:49:28.380 --> 00:49:30.540
Rick Botelho: Yeah, yeah. It's an amazing book.

378
00:49:30.540 --> 00:49:31.550
Stacey Druss: Around that, too, yeah.

379
00:49:31.550 --> 00:49:32.599
Rick Botelho: Yeah, yeah.

380
00:49:33.040 --> 00:49:41.979
Jerry Michalski: Let's… let's take a little pause for a second. We… this is a really rich conversation. We've got a lot of things swirling. A couple people had to drop off to go attend to their lives.

381
00:49:42.540 --> 00:49:48.540
Jerry Michalski: But let's take a pause and then step back in. So, I'll bring us back in in a sec.

382
00:50:00.570 --> 00:50:01.220
Judith Benham: Okay.

383
00:50:48.540 --> 00:51:06.409
Jerry Michalski: One thing I really like about moments when we have silence here is I do something sort of like what I used to do in Quaker Meeting in Connecticut when I attended back when, because those are meetings, silent meetings, so the… most of the hour goes by in quiet. It's just looking at the people in the room, and it got to where

384
00:51:06.580 --> 00:51:23.260
Jerry Michalski: with Wilton Monthly Meeting, which is the meeting I loved and lived nearby, it got to where I would intentionally arrive a little bit early, and I would sit on a far side, opposite the doors. There were sort of two doors to come into the room. They were just long, long benches that we all sat on.

385
00:51:23.470 --> 00:51:36.859
Jerry Michalski: And I would watch people come in, and as I got to know everybody, I would, you know, get weepy just watching people come in and take their seats, just quietly. And one fellow always came in with his son, who, lay down with his head in his lap.

386
00:51:36.970 --> 00:51:45.699
Jerry Michalski: Across the bench, and then 15 minutes into meeting, all the children would file out of meetings, but they were welcome for the first 15 minutes if they wanted to be there.

387
00:51:45.970 --> 00:51:50.510
Jerry Michalski: And it was just a… this rhythm, this really beautiful rhythm that I… that I loved.

388
00:51:51.740 --> 00:51:56.450
Jerry Michalski: And now back to our regularly scheduled program, which is already in progress.

389
00:51:57.470 --> 00:52:10.860
Jerry Michalski: Anybody who… along the way, as we go down on queue, if anyone wants to synthesize, summarize, or amplify, or ask questions to deepen some of our understandings of the things we've been putting on the table, because

390
00:52:10.860 --> 00:52:22.050
Jerry Michalski: So far, we're doing a lot of putting interesting items on the table, but I think the occasional crystallization of thought is really useful, so please, please do. Klaus.

391
00:52:23.600 --> 00:52:24.880
Klaus Mager: Yeah, so what I'm…

392
00:52:25.080 --> 00:52:34.980
Klaus Mager: Struggling with is that much of this misinformation, disinformation, outright lies, and so on, has an intentionality behind it.

393
00:52:35.530 --> 00:52:44.300
Klaus Mager: and is… is really rather quite directed towards very specific outcomes. So, so, …

394
00:52:44.490 --> 00:52:53.770
Klaus Mager: We get… we get drawn into conversations and get excited about, you know, something that's obviously wrong and so on, but, …

395
00:52:53.980 --> 00:52:57.130
Klaus Mager: When we… when we respond, …

396
00:52:57.330 --> 00:53:07.670
Klaus Mager: And we respond to this, like, one particular misstatement, that doesn't really deal with what the intention of this misinformation was.

397
00:53:07.740 --> 00:53:24.450
Klaus Mager: So, the best example was during the last election, when you had Elon Musk deploy his AI to strategically misinform population groups. So, you had…

398
00:53:24.460 --> 00:53:38.379
Klaus Mager: Latinos and African Americans and Arabs, and the most unlikely of groups voting for Trump, because he captured them with just one

399
00:53:38.380 --> 00:53:45.319
Klaus Mager: One… one issue that was… that was really important to this particular group.

400
00:53:45.510 --> 00:53:47.469
Klaus Mager: So, …

401
00:53:47.680 --> 00:54:03.389
Klaus Mager: So, I mean, I observed some stunning conversations before the election where a Palestinian man was defending, voting for Trump because he's good, he's the right person to solve this.

402
00:54:03.640 --> 00:54:07.329
Klaus Mager: So when you… so we are really in…

403
00:54:07.870 --> 00:54:20.589
Klaus Mager: my god, this is biblical, right? The fight of principalities. When you really, get… get into it from… from this perspective.

404
00:54:20.820 --> 00:54:29.439
Klaus Mager: then… then… how do you deal with that now? You have to… I mean, we have to really step back and engage on a completely different level.

405
00:54:29.620 --> 00:54:44.379
Klaus Mager: Now, and, and I'm not sure we have captured that thought, you know, that we are, we are, we are really, the, the, the impact of this,

406
00:54:44.500 --> 00:55:00.619
Klaus Mager: targeted, influence over people's worldview and way of thinking and understanding reality is incredibly powerful. I mean, the reason why we are in the mess we're in right now is because

407
00:55:00.760 --> 00:55:16.790
Klaus Mager: You know, someone succeeded, a group of people succeeded in shifting millions of people, you know, into a direction that's really against their interest in all of this, and we… you know, the sort of inclination is to call them stupid, or…

408
00:55:16.790 --> 00:55:24.969
Klaus Mager: You know, but that's not really the point. This is… this is incredibly powerful, you know, under the surface.

409
00:55:25.060 --> 00:55:44.990
Klaus Mager: We are all, you know, vulnerable to it ourselves, and I come back to it is principle, right? It's the battle of principalities here that's referred to, so it's not a new phenomenon. This is, like, as old as humanity.

410
00:55:47.410 --> 00:55:48.440
Jerry Michalski: Thanks, Klaus.

411
00:55:48.970 --> 00:55:50.000
Jerry Michalski: Thank you.

412
00:55:54.650 --> 00:56:01.630
Gil Friend • Sustainability OG • CxO Coach: Yeah, thanks, Jerry and everybody. A few things. Jerry, you speak so beautifully about Quaker Meeting.

413
00:56:01.930 --> 00:56:06.509
Gil Friend • Sustainability OG • CxO Coach: And your clear love for it. Have you been recently?

414
00:56:07.110 --> 00:56:23.329
Jerry Michalski: I went to… I have not been much. I went to the Portland meeting, I don't know how many there are here, I think there may only be one. It was a bit of a drive. And then, before every message or comment, people said, hi, I'm Jerry, my pronouns are he, him.

415
00:56:23.330 --> 00:56:24.100
Gil Friend • Sustainability OG • CxO Coach: Oh, dear.

416
00:56:24.590 --> 00:56:35.789
Jerry Michalski: And I was turned off instantaneously. Like, you can wear a badge, and I'll look at your badge, and I'll honor that. You… the mo… and they had… it's Portland plus Quaker.

417
00:56:35.920 --> 00:56:37.870
Jerry Michalski: And they had gone, like, too far for me.

418
00:56:37.870 --> 00:56:46.760
Gil Friend • Sustainability OG • CxO Coach: So that's not the… that's not the mood that I wanted to invoke by asking you this. I just would love… I would love to see you find a Quaker meeting that's… Me too. Or… or…

419
00:56:47.230 --> 00:56:56.539
Gil Friend • Sustainability OG • CxO Coach: bring that practice more and more into these calls and other calls you participate in, because there's something very beautiful and rich there, so I just want to thank you for invoking it.

420
00:56:56.540 --> 00:56:57.050
Jerry Michalski: Products.

421
00:56:57.050 --> 00:57:08.860
Gil Friend • Sustainability OG • CxO Coach: Number two, whoever it was who spoke about the person who observed their own psychosis and kind of, you know, worked themselves out of that. I'd love the reference for that book. It reminds me of a book a few years ago.

422
00:57:09.070 --> 00:57:15.920
Gil Friend • Sustainability OG • CxO Coach: By a person, I believe was a neuroscientist, who had a stroke and observed the process of her stroke.

423
00:57:16.490 --> 00:57:17.970
Jerry Michalski: It's my stroke of genius.

424
00:57:17.970 --> 00:57:21.830
Gil Friend • Sustainability OG • CxO Coach: By stroke of Genius, which is a fascinating, fascinating, approach.

425
00:57:21.830 --> 00:57:22.830
Ken Homer • SF Bay Area: Taylor, I think.

426
00:57:22.830 --> 00:57:24.219
Jerry Michalski: And it's Jill Bolty Taylor, exactly.

427
00:57:24.220 --> 00:57:33.870
Gil Friend • Sustainability OG • CxO Coach: Yes, thank you, Ken. Exactly so. But, so this, this, you know, this one about the, about the, psychopathy resonated for that. I'd love to see that as well.

428
00:57:33.870 --> 00:57:36.630
Jerry Michalski: Book of Insight is actually what it's called, not my strip of genius, sorry about that.

429
00:57:36.630 --> 00:57:38.350
Gil Friend • Sustainability OG • CxO Coach: Okay, okay. …

430
00:57:39.450 --> 00:57:55.610
Gil Friend • Sustainability OG • CxO Coach: And then, in terms of thematically, I think if we… if we drop this conversation into our AIs, like somebody's probably already done, the… the phrase that my mind keeps hearing in this conversation over and over again, standing out more than any other.

431
00:57:55.980 --> 00:57:58.799
Gil Friend • Sustainability OG • CxO Coach: In addition to trust, is business model.

432
00:58:00.190 --> 00:58:09.670
Gil Friend • Sustainability OG • CxO Coach: hearing again and again is there's not a business model, there's not a business model, there's not a business model, there's not a business model. We're in the world that is run by business models. The business models for media

433
00:58:09.670 --> 00:58:22.380
Gil Friend • Sustainability OG • CxO Coach: and information and information technology and all the stuff that we're talking about was nailed, I think, beautifully by Gerry Mander in Four Arguments for the Elimination of Television in 1980, what, 1985, or something like that.

434
00:58:22.790 --> 00:58:34.980
Gil Friend • Sustainability OG • CxO Coach: And one of the things that Jerry, I thought, nailed exactly is that the nature of this business is that it's selling attention. The product in media is not the information, the product is us and our eyeballs and our attention.

435
00:58:35.130 --> 00:58:44.870
Gil Friend • Sustainability OG • CxO Coach: And that's been elaborated to a great degree by the social media world, which is layered on top of, or, you know, layered through attention, the addiction model.

436
00:58:44.970 --> 00:58:57.269
Gil Friend • Sustainability OG • CxO Coach: And, you know, boatloads of scientists working on developing addictive processes that make us hunt and seek and stick to these things. And that's, you know, the business model is tied up with that, with attention and addiction.

437
00:58:57.390 --> 00:59:01.539
Gil Friend • Sustainability OG • CxO Coach: So I think the question of what could be another business model

438
00:59:02.130 --> 00:59:06.909
Gil Friend • Sustainability OG • CxO Coach: Or maybe there's a better phrase than that, because that already surrenders to the game.

439
00:59:07.760 --> 00:59:21.639
Gil Friend • Sustainability OG • CxO Coach: But what is a model that can enable the kind of information ecosystems that we're talking about? And I would suggest that not-for-profit is not sufficient, because that depends on philanthropy, which depends on wealth accumulation, which depends on the same system that we're talking about.

440
00:59:21.640 --> 00:59:28.860
Gil Friend • Sustainability OG • CxO Coach: So I don't know where this goes, but for me, that's the powerful, resonant question thread through this conversation.

441
00:59:29.300 --> 00:59:30.230
Gil Friend • Sustainability OG • CxO Coach: I'm complete.

442
00:59:36.970 --> 00:59:43.680
Jerry Michalski: Thank you very much, Gil. Sorry, I forgot I was muted, because I was busy finding the current link to the Jill Bolsey Taylor talk. Jesse.

443
00:59:46.610 --> 00:59:50.279
jessie: Thanks, Jerry. Sorry, I'm not able to get on the camera.

444
00:59:51.730 --> 00:59:57.000
jessie: So, and again, thank you for bringing the Quaker experience here in your own flavor.

445
00:59:57.140 --> 01:00:00.020
jessie: Because I, too, appreciate it.

446
01:00:00.690 --> 01:00:10.769
jessie: And it seems like, to me, that distrust And misunderstanding, or an argument, it often comes from

447
01:00:11.410 --> 01:00:13.999
jessie: to certain people.

448
01:00:14.540 --> 01:00:24.090
jessie: And… which then takes on a new form with triangulation of a third person, and then observers coming into the fold. It takes on a new form, and…

449
01:00:24.290 --> 01:00:27.559
jessie: And I really love the talking about the surrendering.

450
01:00:27.700 --> 01:00:33.300
jessie: to the game, because what you did, Jerry, earlier, when there was a…

451
01:00:34.120 --> 01:00:39.469
jessie: a lot of energy around a specific topic. You brought in a story.

452
01:00:39.810 --> 01:00:46.220
jessie: And it inspires… I really do see how stories inspire the mind to enter into uncertainty.

453
01:00:47.700 --> 01:00:54.990
jessie: and ambiguity, And the preference for inquiry instead of order.

454
01:00:57.100 --> 01:01:05.349
jessie: And… that's what I love about these calls, you know, they… we bring in, we evoke stories and, …

455
01:01:05.980 --> 01:01:07.560
jessie: And even the poems.

456
01:01:07.730 --> 01:01:17.879
jessie: That are brought to this call. It really does evoke this… this uncertainty principle, and I just want to share that, the more… more of that, please. Thank you.

457
01:01:19.180 --> 01:01:20.880
Jerry Michalski: Super. Yeah, thank you, Jesse.

458
01:01:21.900 --> 01:01:22.740
Jerry Michalski: Judy.

459
01:01:23.360 --> 01:01:23.995
Judith Benham: Well…

460
01:01:25.290 --> 01:01:38.240
Judith Benham: I'm gonna come back to a topic that I've mentioned before, but what's very concerning is the reliance on oral-only communication of the mass majority of the public.

461
01:01:38.530 --> 01:01:40.800
Judith Benham: Because they're not really literate.

462
01:01:40.990 --> 01:01:51.940
Judith Benham: and are not able to discern and seek sources of information other than through verbal contact. And I don't know how to address that, it's just so huge.

463
01:01:52.100 --> 01:01:57.920
Judith Benham: But, to me, We're just… we're not helping people

464
01:01:58.080 --> 01:02:05.919
Judith Benham: To learn to be discerning in some process, and finding the right way to enable that or to facilitate it

465
01:02:06.310 --> 01:02:16.730
Judith Benham: To me, is really important if we want to influence opinions, because 80% of the people are not in that discernment mode at all.

466
01:02:18.740 --> 01:02:25.949
Jerry Michalski: And is there a difference to you, Judy, between literacy and critical thinking? Are those neighboring concepts.

467
01:02:26.630 --> 01:02:31.310
Judith Benham: They don't have to be a line, but frequently.

468
01:02:31.350 --> 01:02:50.380
Judith Benham: if you want more information, the literacy becomes a factor, because you can only go out to a verbal group or to listen to things. It's either broadcast to you and it's already canned, or you have to find a group to have discussion that will facilitate that discussion. So I see literacy as a…

469
01:02:50.410 --> 01:02:53.490
Judith Benham: A significant failure point for all of this.

470
01:02:53.750 --> 01:03:04.439
Jerry Michalski: Thank you. And then there's a whole other call we can have about critical thinking. I mean, there's… there's a bunch of neighboring issues we've been kind of sliding past that would be really nice subjects for OGM calls, and…

471
01:03:04.760 --> 01:03:07.090
Jerry Michalski: Hopefully things that get fixed in the world.

472
01:03:07.330 --> 01:03:09.640
Jerry Michalski: … But…

473
01:03:10.070 --> 01:03:27.309
Jerry Michalski: Just one of my… just to respond to what you're saying, one of my beliefs is that people are actually way smarter than we think they are, but our conditions, our settings, our contexts often make us act and be pretty stupid, and also that people will often live up to how you treat them.

474
01:03:27.580 --> 01:03:38.980
Jerry Michalski: they will… if you assume that they're smart, often they will work hard to, like, do the smarter thing, or the better thing, or whatever else. People will rise to what you see in them in different ways.

475
01:03:39.280 --> 01:03:56.129
Jerry Michalski: And it's one of the reasons why I think that assuming good intent is good, because it tends to bring about good intent, and assuming bad intent means you treat everybody with suspicion, which wins you back suspicion and bad behavior. I think that those are sort of opposing loops in sort of human social behavioral settings.

476
01:03:56.690 --> 01:03:59.240
Jerry Michalski: And how we re… go ahead.

477
01:03:59.240 --> 01:04:04.760
Judith Benham: I was just going to say, my favorite phrase in conversation with people is, help me understand.

478
01:04:04.760 --> 01:04:08.110
Jerry Michalski: Right. Because… I think I've got that in my brain under you.

479
01:04:08.260 --> 01:04:26.560
Judith Benham: Because I think if you can… it's much more effective than being polarizing or saying, well, that disagrees with whatever, or here's this source. I think people are intuitive about these things, and to some extent, they believe what they want to believe, or what fits with their value framework, but

480
01:04:26.560 --> 01:04:30.119
Judith Benham: But if you ask a question that's open and genuine.

481
01:04:30.240 --> 01:04:37.420
Judith Benham: Seeking understanding of their point of view, that leads you to a conversation that's deeper and more focused.

482
01:04:38.030 --> 01:04:41.219
Jerry Michalski: Some degree of mutual respect and curiosity. Yep.

483
01:04:41.590 --> 01:04:42.520
Jerry Michalski: Thank you.

484
01:04:42.900 --> 01:04:43.720
Jerry Michalski: Rick?

485
01:04:44.790 --> 01:04:57.470
Rick Botelho: You know, just to build on what you were just saying there, I'll just share another story about being involved with this Indivisible group. So, I have to write things… I write at the highest level to start off with, because it's just my way my brain works.

486
01:04:57.620 --> 01:05:03.980
Rick Botelho: And then I… and then I, you know, I bring it down to different levels. And, you know, I… I've…

487
01:05:04.200 --> 01:05:12.780
Rick Botelho: I've coined this phrase called hen, Which means, humanist-guided, That's the H.

488
01:05:12.940 --> 01:05:14.790
Rick Botelho: Ai enabled.

489
01:05:16.590 --> 01:05:18.750
Rick Botelho: emancipatory neolarning.

490
01:05:19.240 --> 01:05:32.660
Rick Botelho: And, I'm… I'm using what, you know, everyone knows the KISS principle, keep it simple, stupid. The other one is keep sophistication inside of simplicity, which means you can… you can use AI

491
01:05:32.660 --> 01:05:48.269
Rick Botelho: to change things, as Klaus has spoken. You can change it to different orientations, whatever. That's the beauty of it. Now, what's happening in this group is that I've actually cascaded them to start using AI, and this is a lot of baby boomers, and they're using it.

492
01:05:48.270 --> 01:06:00.509
Rick Botelho: And the lead guy came, and I wrote something in a post, and he, to him, and he says, well, I put it in AI and asked it to reduce it down to a 6th grade reading level, and then sent it back to me.

493
01:06:00.510 --> 01:06:12.980
Rick Botelho: And I said, yeah, that's pretty good. And then I said, what was missing, or whatever. But, you know, the idea of how we can use AI as an emancipatory learning process.

494
01:06:13.080 --> 01:06:19.090
Rick Botelho: And one of the triggers that, you know, some people say, oh, you're just using big words, or you're just using jargon.

495
01:06:19.300 --> 01:06:26.100
Rick Botelho: And, you know, that's a conditioned replex from a system that's indoctrinated people not to be curious about new things.

496
01:06:26.560 --> 01:06:41.100
Rick Botelho: What if, instead of seeing jargon as an object of rejection, instant rejection, that people were actually encouraged to be curious about it? Maybe this is something I can learn from this new word, rather than just simply, oh, it's jargon.

497
01:06:41.210 --> 01:06:44.409
Rick Botelho: Put it in the, trash can of ignorance, so to speak.

498
01:06:44.540 --> 01:06:53.820
Rick Botelho: And to me, that… that's, you know, how do we… and the story that I think, I think it was Gil was mentioning what's happening in Finland, I mean.

499
01:06:53.820 --> 01:07:05.109
Rick Botelho: that is, you know, an element of emancipatory learning, from what I heard. I mean, I've read the book by Adam Grant on hidden talents, and certainly their educational system is,

500
01:07:05.130 --> 01:07:14.740
Rick Botelho: It was the best in the world, but now a lot of Asian countries have replicated, and they've slipped down the chain of education.

501
01:07:14.810 --> 01:07:26.679
Rick Botelho: So, I really think it comes back to how do we create a culture of emancipation opposed to a culture of indoctrination, which is what the attention economy is all about.

502
01:07:28.820 --> 01:07:29.730
Jerry Michalski: … Dispute.

503
01:07:29.730 --> 01:07:32.530
Rick Botelho: Dispute anything. Dispute anything I just said, by the way.

504
01:07:32.720 --> 01:07:37.809
Jerry Michalski: And I'm gonna address you right now. Rick, do you think I'm a curious person?

505
01:07:40.200 --> 01:07:41.280
Rick Botelho: Yes. I, I…

506
01:07:41.560 --> 01:07:47.610
Rick Botelho: I would… I would ask you… I would ask you, tell me… tell me more about why you think you're a curious person.

507
01:07:47.610 --> 01:07:54.800
Jerry Michalski: No, no, no, I'm asking you an opinion. I'm not asking you to do a Eliza back to me. I'm like, do you think I'm curious?

508
01:07:55.770 --> 01:08:00.129
Rick Botelho: I invited you to respond to your own question, I'm evading it deliberately.

509
01:08:00.130 --> 01:08:01.359
Jerry Michalski: Oh, perfect, good.

510
01:08:01.490 --> 01:08:11.339
Jerry Michalski: And do you… and just generally, do you think I'm relatively open-minded? I don't think I'm completely open-minded, because we're all pretty… we cling to our beliefs, we're pretty stuck on them.

511
01:08:12.330 --> 01:08:25.109
Jerry Michalski: And Rick, I say this full of love for you, and I will say, you're unusual in this for me and all the people I know, in that when you speak in meetings we have on Zoom, and we've never met in person.

512
01:08:25.229 --> 01:08:28.039
Jerry Michalski: I love what you say, and I'm completely on board.

513
01:08:28.170 --> 01:08:41.240
Jerry Michalski: I cannot read things you write because you use jargon and really long words and emancatory neo-something. I just tried to write down, by the time I got back to remembering what to write down, I had forgotten the phrase.

514
01:08:41.529 --> 01:08:45.640
Jerry Michalski: You're… I find… I find your writing not readable.

515
01:08:46.529 --> 01:08:59.440
Jerry Michalski: And I've told you this before, and it's just my opinion, and I therefore don't go in and read your things, but when you hear this from other people, they're probably people who feel like I do when they encounter your writing.

516
01:09:00.340 --> 01:09:02.930
Rick Botelho: Oh, you're absolutely spot on.

517
01:09:03.100 --> 01:09:14.020
Rick Botelho: And I accept that feedback altogether, but I'll go, yes, and. Okay, that's where you are, and then I can adjust it to whatever. That's the KISS principle. So, if you have difficulties with….

518
01:09:14.020 --> 01:09:14.840
Jerry Michalski: Interesting.

519
01:09:15.170 --> 01:09:16.489
Jerry Michalski: You're not adjusting.

520
01:09:16.790 --> 01:09:20.119
Jerry Michalski: No, your writing today is the same as you're writing a couple years ago.

521
01:09:20.120 --> 01:09:27.759
Rick Botelho: No, no, no, no, no. You haven't read my stuff recently, then. I even go into AI and ask it to do notebook.

522
01:09:27.760 --> 01:09:28.350
Jerry Michalski: Well, that's good.

523
01:09:28.359 --> 01:09:42.019
Rick Botelho: at 6th grade leading, 12th grade leading, graduate level. There's a whole series of ways in which you can create outputs for different reading levels, and then I will take a complex question and say, write at 6th grade level, 12th grade level.

524
01:09:43.419 --> 01:09:46.499
Rick Botelho: So, that's what I'm doing.

525
01:09:46.670 --> 01:09:54.820
Jerry Michalski: Cool. Can you send me a couple of new links that do that? Because I want access to your writing that I don't have right.

526
01:09:55.340 --> 01:10:03.870
Rick Botelho: Okay, that's fine. I will send it to you. I'm working on one at the moment that's directed towards Indivisible. I haven't finished it yet, and it deals with these issues.

527
01:10:05.810 --> 01:10:09.819
Jerry Michalski: Love that. Thank you. And thanks for being so gracious about that.

528
01:10:10.190 --> 01:10:11.750
Jerry Michalski: Mr. Nelson.

529
01:10:12.420 --> 01:10:17.160
Mike Nelson: This isn't meant to pile on, but let me….

530
01:10:17.160 --> 01:10:18.670
Judith Benham: Let me….

531
01:10:18.670 --> 01:10:23.690
Mike Nelson: just, say exactly what Jerry said and turn it up to 11.

532
01:10:23.920 --> 01:10:31.420
Mike Nelson: I have people who have heard me talk over the last 35 years, and some of them

533
01:10:31.830 --> 01:10:35.619
Mike Nelson: Will meet me for the first time in 20 years, and they'll say.

534
01:10:35.810 --> 01:10:41.899
Mike Nelson: Yeah, I heard a talk you gave 22 years ago, and I'll say, what was it about?

535
01:10:42.310 --> 01:10:48.750
Mike Nelson: And they'll say, I'm not sure, but you started off by saying, you gotta have a good bumper sticker.

536
01:10:48.930 --> 01:10:53.749
Mike Nelson: In Washington. And by that, I mean 4 syllables.

537
01:10:53.960 --> 01:10:56.890
Mike Nelson: To define a, a buzzword.

538
01:11:01.910 --> 01:11:09.429
Jerry Michalski: Mike's internet connection is unreliable, which is what we are experiencing right now. I think if we're patient, he'll be back with us in just a second.

539
01:11:10.210 --> 01:11:13.619
Mike Nelson: If it's really well cracked, buzzword, 7 words for….

540
01:11:14.910 --> 01:11:18.810
Jerry Michalski: Mike, your internet, your net connection is crapping out on you right now.

541
01:11:19.980 --> 01:11:33.099
Mike Nelson: I'm turning off the video, does that make things better? It should. Keep going, and it should help. I'm sorry about that. My wife's also using our limited bandwidth. But anyway, buzzword.

542
01:11:33.590 --> 01:11:38.010
Mike Nelson: 4 or 5 syllables, bumper sticker, 7 or 8 words.

543
01:11:38.010 --> 01:11:53.779
Mike Nelson: that is memorable. It's something that sticks in the brain, it's visual, it's emotional, and this goes back to our truth problem. I mean, the reason Trump is winning the information battle is because he's really good at emotional…

544
01:11:54.150 --> 01:11:58.619
Mike Nelson: visual Bumper stickers.

545
01:11:59.050 --> 01:12:04.030
Mike Nelson: He doesn't care about statistics, he makes them up and completely undermines things.

546
01:12:05.000 --> 01:12:15.970
Mike Nelson: And that's the thing I really wanted. The other thing I wanted to pick on you about, Rick, was you said, we have a choice, emancipation or indoctrination.

547
01:12:16.190 --> 01:12:19.710
Mike Nelson: What Trump is doing is not indoctrination.

548
01:12:20.300 --> 01:12:26.960
Mike Nelson: He's reaching out to all sorts of groups. They've been indoctrinated in all sorts of different ways.

549
01:12:27.350 --> 01:12:28.700
Mike Nelson: What he's done…

550
01:12:29.170 --> 01:12:36.170
Mike Nelson: And this is the brilliant part. It's what Putin really taught the world how to do starting 35 years ago.

551
01:12:36.470 --> 01:12:54.040
Mike Nelson: He didn't do the Stalin disinformation trick, which was to just flood everybody with one consistent message. You know, the Soviet Union is all-powerful, we are here to, you know, liberate the peasants, and, you know, it wasn't one story.

552
01:12:54.380 --> 01:13:02.659
Mike Nelson: For 35 years, Putin and his digital Rasputin, he has this guy who's worked with him his entire career.

553
01:13:03.000 --> 01:13:12.010
Mike Nelson: Who is an expert at throwing 5 or 6 different messages, contradictory messages, Out through mass media.

554
01:13:12.880 --> 01:13:21.760
Mike Nelson: This is what Steve Bannon has done with the War Room. He brings in all these wacko people who tell totally different stories, some of them quite anti-Trump.

555
01:13:22.250 --> 01:13:26.359
Mike Nelson: A lot of them… Fantasies about the dark state.

556
01:13:27.180 --> 01:13:35.860
Mike Nelson: After 30 minutes of this, people have no idea what's going on in the world, and they have no confidence in their ability to figure it out.

557
01:13:35.970 --> 01:13:45.210
Mike Nelson: And that… is the third option. So it's… Emancipation, indoctrination.

558
01:13:45.330 --> 01:13:48.310
Mike Nelson: Or… Discombobulation.

559
01:13:49.860 --> 01:14:00.600
Mike Nelson: That's where we are now. And what both Trump and Putin have done is confuse the hell out of people, including my very wise rocket scientist father.

560
01:14:01.090 --> 01:14:05.060
Mike Nelson: Who, after 10 years of Fox News, just started…

561
01:14:05.190 --> 01:14:06.659
Mike Nelson: I don't know what's going on.

562
01:14:07.420 --> 01:14:16.959
Mike Nelson: So, anyway, long, long talk, but for some reason, Trump has convinced people emotionally that he's on their side. So, you don't know what's going on.

563
01:14:17.060 --> 01:14:18.220
Mike Nelson: I'll take care of it.

564
01:14:20.340 --> 01:14:21.510
Rick Botelho: Sorry, respond.

565
01:14:21.960 --> 01:14:23.370
Jerry Michalski: Please. Yes, please.

566
01:14:23.370 --> 01:14:33.199
Rick Botelho: Yeah, … yeah, what I… the framing I gave was not meant to be a complete synopsis of the complexity of the systemic psychopathy, and I don't dispute anything we just said.

567
01:14:33.250 --> 01:14:50.569
Rick Botelho: And certainly, he hasn't done the indoctrination. This is the indoctrination process that's been going on for 50 years since, Ronald Reagan turned over the… or enabled to turn over the fairness doctrine. And it sort of allowed the alt-right neo-terrorism, enterprise to flourish.

568
01:14:50.600 --> 01:15:10.200
Rick Botelho: So, he's just a poster boy who is an emotional savant to people's brains, and he's able to resonate with their angst and frame things in such a way that he commands their blind, unconditional loyalty. He's a… he is a brilliant marketing maverick.

569
01:15:10.530 --> 01:15:22.109
Rick Botelho: Unequivocally. Now, going back to the issue of the simplification thing, I, you know, that has merit, but to me, it's a sort of reductionist way of

570
01:15:22.110 --> 01:15:31.140
Rick Botelho: Looking at complexity, and while it has a certain degree of surface validity from my point of view, it doesn't deal with complexity.

571
01:15:31.250 --> 01:15:35.660
Rick Botelho: And so, you have to give… Trump, his…

572
01:15:36.310 --> 01:15:49.690
Rick Botelho: full credit for exploiting something that he didn't invent, which was MAGA, using it for his own purposes, manipulating people around it in such a masterful way, that he has people

573
01:15:49.690 --> 01:16:00.260
Rick Botelho: believing everything he says, and they will not question him. I mean, I go and listen to these stories that these, Indivisible will have when talking to Trump supporters. I mean, it's water off a duck's back.

574
01:16:00.320 --> 01:16:16.040
Rick Botelho: I mean, it just doesn't, you know… So, when I was framing that, that was a reductionist frame to thinking about, we have to shift towards helping people to start learning how to discern, to verify, and seek truth.

575
01:16:16.070 --> 01:16:22.999
Rick Botelho: Collaborative together without the divisiveness, polarization, etc.

576
01:16:23.000 --> 01:16:36.100
Rick Botelho: And one of the big problems we have is that people's political identities, and this happens so often, is attached to their point of view. You challenge their point of view, or trigger their point of view, and it creates this

577
01:16:36.190 --> 01:16:39.289
Rick Botelho: Binary, emotional backlash.

578
01:16:39.410 --> 01:16:42.370
Rick Botelho: And we don't have a language to

579
01:16:42.640 --> 01:16:56.500
Rick Botelho: educate people. That's what I'm interested in, what I just… those five words, it was five words, it was a mouthful, I know, and you have to read it, not… you have to read it, not read it out loud, to simulate the meaning of it, and how can we create

580
01:16:56.500 --> 01:17:03.340
Rick Botelho: Beloved learning communities, where people… the content… it's focusing on process, not on content.

581
01:17:03.880 --> 01:17:13.110
Rick Botelho: We've got to get the process right, otherwise we won't get the content right. And so much of marketing has to do with content delivery. We live in a content creator's world, and we're not focusing on process enough.

582
01:17:14.640 --> 01:17:15.470
Jerry Michalski: Thank you, Eric.

583
01:17:17.710 --> 01:17:29.550
Jerry Michalski: Gil, Jose, Stacey, and then I'd like to… actually, I'd like to share my notes, by screen sharing that I've been doing during this call, just because there's a lot of really good stuff here, and it's worth seeing for a second. So, Gil, please.

584
01:17:29.890 --> 01:17:32.239
Gil Friend • Sustainability OG • CxO Coach: Gil, your breakfast lasts a long time.

585
01:17:33.610 --> 01:17:35.520
Gil Friend • Sustainability OG • CxO Coach: That's because I stop periodically.

586
01:17:35.720 --> 01:17:36.190
Jerry Michalski: Bah.

587
01:17:36.210 --> 01:17:39.659
Gil Friend • Sustainability OG • CxO Coach: And write… they use my hands to write things.

588
01:17:39.660 --> 01:17:40.120
Jerry Michalski: Cool.

589
01:17:40.610 --> 01:17:42.119
Gil Friend • Sustainability OG • CxO Coach: And go off camera.

590
01:17:42.230 --> 01:17:46.199
Gil Friend • Sustainability OG • CxO Coach: … golly, couple things here. …

591
01:17:48.690 --> 01:17:54.589
Gil Friend • Sustainability OG • CxO Coach: Mike, you said something about discombobulation. I felt from the beginning that the Reagan… the… the… the…

592
01:17:56.280 --> 01:18:01.939
Gil Friend • Sustainability OG • CxO Coach: Trump, Bannon, etc. strategy has been to reduce the opposition to sputtering apoplexy.

593
01:18:03.380 --> 01:18:09.860
Gil Friend • Sustainability OG • CxO Coach: Like, you know, for years, the main response was, do you believe what he said? How could he say that? This is, you know, just like…

594
01:18:10.190 --> 01:18:28.490
Gil Friend • Sustainability OG • CxO Coach: reactive and not active, so there's that. Someone, Rick, I think, said, or someone said this began with… oh, Klaus, I guess you said, began with Reagan. It actually began 10 years earlier. Reagan was a follow-up to what began in the early 70s with the Powell Memorandum and the work of the U.S. Chamber of Commerce to set in motion

595
01:18:28.490 --> 01:18:36.100
Gil Friend • Sustainability OG • CxO Coach: this, what, yeah, 50-plus year strategy that has been consistently followed across this time in contrast to what the Dems have done.

596
01:18:36.350 --> 01:18:41.389
Gil Friend • Sustainability OG • CxO Coach: But that's not what I wanted to speak about. What was I wanted to speak about? Oh, yeah, yeah. So, …

597
01:18:41.950 --> 01:18:47.089
Gil Friend • Sustainability OG • CxO Coach: there was a post in OGM recently, I don't know if it was from me or from somebody else,

598
01:18:47.290 --> 01:18:58.270
Gil Friend • Sustainability OG • CxO Coach: On Mo Goddat's conversation about the future of AI, which is very much worth watching if you haven't seen. It's long, but it's worth going into the whole thing. Ken's got a digest of it.

599
01:18:58.580 --> 01:19:06.090
Gil Friend • Sustainability OG • CxO Coach: But it doesn't capture the whole mood. This is a conversation on a podcast called Diary of a CEO.

600
01:19:06.690 --> 01:19:14.049
Gil Friend • Sustainability OG • CxO Coach: DOAC is the code word for it. It turns out it's, like, one of the 10 most popularly watched podcasts in the world.

601
01:19:14.280 --> 01:19:22.089
Gil Friend • Sustainability OG • CxO Coach: Run by a guy named Steven Bartlett, who is trying to build a billion-dollar media empire. He's in the hundreds of millions at this point.

602
01:19:22.410 --> 01:19:28.070
Gil Friend • Sustainability OG • CxO Coach: And he's got another parallel stream called, … Behind the diary.

603
01:19:29.470 --> 01:19:39.499
Gil Friend • Sustainability OG • CxO Coach: And, in the episode I watched last night, he was showing how his organization monitors their YouTube analytics of everything they publish down to the minute.

604
01:19:40.590 --> 01:19:49.990
Gil Friend • Sustainability OG • CxO Coach: YouTube apparently provides this, I didn't know this, showing, you know, showing the attention and drop-off rates during the course of a very long podcast, and they then tune

605
01:19:50.230 --> 01:20:02.099
Gil Friend • Sustainability OG • CxO Coach: Their broadcast, to learn… to constantly learning from how is the audience reacting, how do they tailor their material so that they can get more and… more and tighter attention and addiction.

606
01:20:02.290 --> 01:20:09.620
Gil Friend • Sustainability OG • CxO Coach: And Bartlett, who is the CEO of this 100 and some odd person operation, says he spends two and a half hours a day

607
01:20:10.060 --> 01:20:12.150
Gil Friend • Sustainability OG • CxO Coach: monitoring Google Analytics.

608
01:20:12.220 --> 01:20:13.410
Jerry Michalski: Great, Scott.

609
01:20:13.610 --> 01:20:15.920
Gil Friend • Sustainability OG • CxO Coach: Great Scott. Yes, indeed.

610
01:20:16.490 --> 01:20:18.440
Gil Friend • Sustainability OG • CxO Coach: So, if he's doing that….

611
01:20:18.440 --> 01:20:18.970
Mike Nelson: Excellent.

612
01:20:18.970 --> 01:20:21.799
Gil Friend • Sustainability OG • CxO Coach: how much you want to bet that Bannon and crew are doing that, too?

613
01:20:24.400 --> 01:20:25.609
Gil Friend • Sustainability OG • CxO Coach: And, …

614
01:20:25.750 --> 01:20:39.940
Gil Friend • Sustainability OG • CxO Coach: I'll commend to people's attention, Bannon did an interview with Ross Duth out of the Times, I think somewhere in the last 6 months, where he is very forthrightly laying out the strategy that he's been cultivating for many years.

615
01:20:40.100 --> 01:20:42.690
Gil Friend • Sustainability OG • CxO Coach: Long before Trump.

616
01:20:43.890 --> 01:20:56.619
Gil Friend • Sustainability OG • CxO Coach: And… and talks about the moment in 2015 or 2016, whenever it was, where he encountered Trump, or they encountered Trump in some sort of situation and said, that's the guy we've been looking for.

617
01:20:58.090 --> 01:21:00.150
Gil Friend • Sustainability OG • CxO Coach: Casting, hire him.

618
01:21:02.080 --> 01:21:05.509
Gil Friend • Sustainability OG • CxO Coach: But the strategy was already there, long before Donald showed up.

619
01:21:08.590 --> 01:21:09.750
Gil Friend • Sustainability OG • CxO Coach: Should say to you.

620
01:21:11.160 --> 01:21:11.950
Jerry Michalski: Over to you.

621
01:21:14.760 --> 01:21:15.449
Jose Leal: Thank you.

622
01:21:16.020 --> 01:21:20.760
Jose Leal: … I've been struggling with this conversation because I…

623
01:21:22.440 --> 01:21:34.039
Jose Leal: I think we kind of feel like we all have to have an opinion about everything that's happening in the world. We have to know it all, and we have to have an opinion on it, and we have to know what's right and what's wrong.

624
01:21:34.570 --> 01:21:38.680
Jose Leal: … And I wonder if that's really necessary.

625
01:21:39.470 --> 01:21:42.930
Jose Leal: From a… from a human nature perspective.

626
01:21:44.230 --> 01:21:49.479
Jose Leal: We knew about our village, we knew about the people around us, and everything else was sort of like…

627
01:21:49.780 --> 01:21:56.360
Jose Leal: We just didn't know, and we couldn't care, for the most part, because we had no means of knowing.

628
01:21:56.710 --> 01:22:03.559
Jose Leal: And I wonder if the way we've evolved is to be able to make sense of what we can touch.

629
01:22:04.230 --> 01:22:07.199
Jose Leal: And that making sense of what we can't touch.

630
01:22:07.350 --> 01:22:16.110
Jose Leal: is really, really difficult, and so we end up looking for stories, That validate what we're feeling.

631
01:22:17.330 --> 01:22:28.160
Jose Leal: … And we forget that what's judging that is the feeling part, and what we just said about Trump…

632
01:22:31.110 --> 01:22:36.569
Jose Leal: I don't… think people who follow Trump are following Trump because he makes sense.

633
01:22:37.190 --> 01:22:52.459
Jose Leal: Or because he's got a good story. Or because he, … his… narrative… is well done.

634
01:22:52.570 --> 01:22:55.969
Jose Leal: Accurate. Real. Connected.

635
01:22:57.290 --> 01:23:00.219
Jose Leal: I think follow… people follow Trump because…

636
01:23:00.450 --> 01:23:05.579
Jose Leal: He makes them feel like they're not wrong about what they're feeling about the world.

637
01:23:07.300 --> 01:23:10.060
Jose Leal: that… they feel…

638
01:23:11.210 --> 01:23:25.880
Jose Leal: all kinds of stuff is wrong, and this guy keeps saying all kinds of stuff is wrong. And right, wrong, or indifferent in how he says it, and the stats, and the numbers, and the accuracy, who cares?

639
01:23:26.320 --> 01:23:29.039
Jose Leal: Because it's so distant from me.

640
01:23:29.230 --> 01:23:39.860
Jose Leal: It doesn't really matter if his numbers are right, or his story is right. What I feel is consistent with the way he makes me think I should feel.

641
01:23:40.980 --> 01:23:47.280
Jose Leal: And… When we argue it from… logic, and…

642
01:23:48.240 --> 01:23:52.550
Jose Leal: And stories that are accurate, and the numbers are accurate, and all of that stuff.

643
01:23:52.760 --> 01:23:54.459
Jose Leal: I think we're missing the point.

644
01:23:55.950 --> 01:24:03.309
Jose Leal: And… a lot of what I see coming through on OGM and other lists

645
01:24:04.020 --> 01:24:11.679
Jose Leal: Is this idea that if we could just get to the accuracy of it, then other people would see it our way.

646
01:24:11.820 --> 01:24:18.280
Jose Leal: And I don't think that that's gonna happen. In fact, I think we're just burying ourselves in a deeper hole.

647
01:24:19.500 --> 01:24:20.220
Jose Leal: ….

648
01:24:20.220 --> 01:24:22.879
Stacey Druss: So, I don't know if that makes sense to any of you guys, but….

649
01:24:23.300 --> 01:24:29.090
Jose Leal: It, it, … It seems like this argument isn't gonna get us very far.

650
01:24:30.480 --> 01:24:36.310
Jose Leal: By this argument, I mean that we need to have more logic and more accuracy and more truth.

651
01:24:36.790 --> 01:24:38.410
Jose Leal: So….

652
01:24:39.890 --> 01:24:40.670
Jerry Michalski: Thank you.

653
01:24:41.170 --> 01:24:43.519
Jose Leal: And thank you for this topic, Jerry.

654
01:24:44.010 --> 01:24:54.369
Jerry Michalski: Thanks, Jose. Can you just expand a little bit more on what you mean? Because we… these call… this community is a sense-making community, kinda, sort of. Like, that's what we do.

655
01:24:54.370 --> 01:25:05.810
Jerry Michalski: We're busy piecing together collectively the things we see about different phenomena in the world, and we're doing a lot. We're guilty of a lot of what you just said that we're doing, but I'm not clear what you're saying

656
01:25:05.860 --> 01:25:11.350
Jerry Michalski: We ought to be doing instead, or the alternative is, or where you're… where you're pointing.

657
01:25:13.200 --> 01:25:18.350
Jose Leal: I'm pointing towards stepping back from… like…

658
01:25:19.000 --> 01:25:29.249
Jose Leal: How much do we need to… Know about things that Don't necessarily involve us directly.

659
01:25:30.420 --> 01:25:31.200
Jose Leal: Right?

660
01:25:31.370 --> 01:25:35.599
Jerry Michalski: So should we not be sticking our noses into the Trump apocalypse, for example?

661
01:25:36.150 --> 01:25:40.030
Jose Leal: … That's a case of where I think

662
01:25:40.150 --> 01:25:45.510
Jose Leal: Maybe it's not about avoiding it and trying to, you know, follow what's going on and so forth.

663
01:25:45.670 --> 01:25:58.129
Jose Leal: But I think we need to look at it for what it is, rather than to try to correct… like, posting a post here, there, anywhere, on social media or anything else.

664
01:25:58.290 --> 01:26:07.389
Jose Leal: He misspoke. He told the wrong fact. He… the number is actually 500, it's not 250. You know, …

665
01:26:07.730 --> 01:26:12.799
Jose Leal: Whatever the reality of the situation is.

666
01:26:14.960 --> 01:26:21.880
Jose Leal: The people who are ready are invigorated by his message, his emotional message.

667
01:26:23.330 --> 01:26:25.630
Jose Leal: Don't know and don't care about the number.

668
01:26:26.910 --> 01:26:30.729
Jose Leal: And that us harping on the number.

669
01:26:31.150 --> 01:26:37.100
Jose Leal: you know, I say us, assuming most of us are on that That side of the spectrum.

670
01:26:37.930 --> 01:26:43.880
Jose Leal: … aren't… We're doing ourselves a disservice.

671
01:26:45.310 --> 01:26:51.670
Jose Leal: And them a disservice by talking about the number rather than talking about the feeling.

672
01:26:53.120 --> 01:27:00.120
Jose Leal: The feeling that they feel is accurate. The feeling he speaks to is accurate.

673
01:27:01.460 --> 01:27:06.079
Jose Leal: Figuring out how to change things so that they don't feel that way.

674
01:27:07.670 --> 01:27:10.959
Jose Leal: Is what we should be talking about, not the number.

675
01:27:11.360 --> 01:27:20.359
Jose Leal: Not the story he's laying out. Because he's gonna keep laying out the story that they… that resonates with… with what they're feeling.

676
01:27:22.000 --> 01:27:24.270
Jose Leal: That's… That's accurate.

677
01:27:24.720 --> 01:27:26.280
Jose Leal: That's what he's gonna do.

678
01:27:27.140 --> 01:27:30.829
Jose Leal: And if he has to change facts to do that, he will.

679
01:27:31.650 --> 01:27:33.030
Jose Leal: We all do that.

680
01:27:33.710 --> 01:27:37.059
Jose Leal: Every single one of us change facts slightly.

681
01:27:37.180 --> 01:27:42.600
Jose Leal: In order to make our story More believable, more rigid, more…

682
01:27:42.930 --> 01:27:46.039
Jose Leal: In line with what the message we want to deliver.

683
01:27:49.730 --> 01:27:51.430
Jerry Michalski: I think I'm… Does that help?

684
01:27:51.430 --> 01:27:52.580
Jose Leal: To answer your question, I'm sorry.

685
01:27:52.580 --> 01:27:54.950
Jerry Michalski: It does, and I wanna, I wanna reply to you.

686
01:27:55.280 --> 01:27:58.439
Jerry Michalski: I think I'm mostly on board with what you're saying.

687
01:27:58.740 --> 01:28:01.979
Jerry Michalski: And I just made myself some notes in the chat, so I remember what I want to say.

688
01:28:02.170 --> 01:28:22.009
Jerry Michalski: For me, and this is… I think what we're doing is we're sharing our interpretations of what's going on, and I think that when we're all spun up that a number was inaccurate, or, oh my god, that's such a lie, we are playing into the game that… that Trump and MAGA and the far right and Steve Bannon are all perpetrating and doing completely intentionally.

689
01:28:22.010 --> 01:28:28.689
Jerry Michalski: Like, when they say, look, the moon is in the sky, you know, and Elizabeth Taylor's like, no, that's the sun. It's like, no, no, no, it's the moon.

690
01:28:28.690 --> 01:28:40.539
Jerry Michalski: This is, in fact, a debate over power. And for me, Trump's use of modern media is an exercise in power, and Trump has more elbow room.

691
01:28:40.740 --> 01:28:43.450
Jerry Michalski: Than any politician in my lifetime.

692
01:28:43.800 --> 01:28:58.860
Jerry Michalski: He has created for himself the ability to reverse himself, lie like a rug, do whatever, and still be in charge the next day and make a comeback. His inaugural speech the second time is like, hey, most of you thought this was a real long shot, but here I am.

693
01:28:59.160 --> 01:29:06.480
Jerry Michalski: You know, nobody thought I would come back and do this, but this is, like, one of the great comebacks of American political history, and it kinda is.

694
01:29:06.580 --> 01:29:17.049
Jerry Michalski: Right? So a lot of people… and we each have different theories about why people follow him and what's going on, and I'm very interested in all the different facets or readings of what's going on, because

695
01:29:17.050 --> 01:29:28.829
Jerry Michalski: Because I think that that really matters, because it's going to affect our future like crazy. So when you start by saying, we're kind of poking our nose into things that don't have direct impact on us, I'm like, which one doesn't? Like, like…

696
01:29:28.830 --> 01:29:37.059
Jerry Michalski: The educational system, regenerative agriculture, all the… to me, it's all a big hairball, a big nexus of things that are deeply intertwingled.

697
01:29:37.060 --> 01:29:53.140
Jerry Michalski: That respond a lot to the dominant narratives or stories that people hoist on us, that we then buy or don't buy, that win or lose elections, that then change everything on the ground because the government is now being completely ripped into a different shape

698
01:29:53.290 --> 01:30:07.140
Jerry Michalski: in a way that most of us didn't expect, but were afraid might happen, and it's actually, like, underway right this second. So it feels to me like all that stuff that is actually pretty distant, policy-level stuff, et cetera, et cetera, feels like it's going to affect

699
01:30:07.190 --> 01:30:15.890
Jerry Michalski: me and us, and so forth, a lot. So, I don't know, that… that's kind of… and I think we're doing ourselves a disservice when we take the bait.

700
01:30:16.230 --> 01:30:24.850
Jerry Michalski: But we're not doing ourselves a disservice when we talk about this dynamic. I think… I think understanding this dynamic and figuring out countermeasures is desperately important.

701
01:30:25.720 --> 01:30:30.459
Jose Leal: Yeah, and I… Yeah, I'd love to dig deeper a little bit, but we're out of time, so….

702
01:30:30.870 --> 01:30:36.989
Jerry Michalski: Shoot! Damn, there's the 50-minute hour, we're over, like, therapy is done for the day. Shit.

703
01:30:37.670 --> 01:30:53.149
Jerry Michalski: Stacy, therapy is not so done that you can't have your say. I'm hoping Mr. Homer has a poem for us, and I will postpone… I'll send a link to my… while you're talking, I'll send a link to my Nexus, for my notes, and I'll show them maybe next week if anybody's interested, but go ahead, please.

704
01:30:53.980 --> 01:31:08.050
Stacey Druss: Thank you, Jose. I'm all riled up now, though. Let me calm down a minute. But my comments pertain to what was said about identity and process, and what Jose just said.

705
01:31:08.190 --> 01:31:13.130
Stacey Druss: So, a revelation that I recently had about my upbringing is that

706
01:31:13.290 --> 01:31:18.959
Stacey Druss: I realized that I learned by watching my mother educate my father.

707
01:31:19.980 --> 01:31:21.139
Stacey Druss: That's cool.

708
01:31:21.390 --> 01:31:39.180
Stacey Druss: It actually is kind of cool. So, like, when I was in… so both of my parents are Republican, and when I was in college, I remember a professor saying to me, how… because clearly I'm not conservative, and he said, how did you get to be the way you are? And I said, they taught me good values.

709
01:31:39.530 --> 01:31:51.569
Stacey Druss: Because they did. They didn't tell me how to think, but because I was observing their dialogue, and I was allowed to interject and question, and I was curious, that's how I learned.

710
01:31:51.790 --> 01:31:56.849
Stacey Druss: So, I try to take that approach onto social media when I can.

711
01:31:57.970 --> 01:32:04.609
Stacey Druss: So, I had an experience the other day. I put up what I thought was a benign post, and I said.

712
01:32:04.780 --> 01:32:10.900
Stacey Druss: Can we at least admit that America has an anger management problem.

713
01:32:13.560 --> 01:32:18.529
Stacey Druss: Thought that would be an easy kind of thing to not fight about.

714
01:32:18.810 --> 01:32:29.679
Stacey Druss: Anyway, long story short, I have a cousin. She's not that bright, she's never been political until right before the last election, and I spoke to her privately, and she was like.

715
01:32:30.050 --> 01:32:32.830
Stacey Druss: I don't know any of these things, but I can't… whatever.

716
01:32:33.230 --> 01:32:39.410
Stacey Druss: Well, all of a sudden, I woke up yesterday to, like, a really personal attack

717
01:32:39.680 --> 01:32:45.060
Stacey Druss: You never worked a day in your life, like, all this… And I…

718
01:32:45.990 --> 01:32:51.810
Stacey Druss: Like, I was shocked. Let me just… I'm shocked now, reliving it in my… in my mind.

719
01:32:51.920 --> 01:33:07.980
Stacey Druss: But I wanted to speak to that identity piece, and also, Jose, the one piece you left out is, in addition to why they like Trump, it's also an outlet for them to let their anger out.

720
01:33:10.270 --> 01:33:25.090
Stacey Druss: I can't emphasize that part enough. And yes, there is anger on both sides, and it does need to be recognized everywhere, which is why I spoke about anger management. I wasn't putting it to one party or the other.

721
01:33:25.410 --> 01:33:30.689
Stacey Druss: … What else did I want to say? The process, okay.

722
01:33:32.280 --> 01:33:34.919
Stacey Druss: So I was just on a call before this.

723
01:33:35.740 --> 01:33:49.230
Stacey Druss: And, the person made a… was talking about the Constitution, made a mistake twice. My normal thing is, if it doesn't interfere with the intent of something, like when it comes to words.

724
01:33:49.310 --> 01:34:03.880
Stacey Druss: I think we should be quiet, because you know what a person meant. But in this case, I brought it up mostly because we were talking about process and a similar conversation, and I wanted to express how hard it is for a person

725
01:34:04.500 --> 01:34:06.820
Stacey Druss: To speak to authority.

726
01:34:07.130 --> 01:34:21.770
Stacey Druss: and to let them know, oh, you know, you made a mistake, or this is not accurate. I mean, I have that experience on Facebook where somebody I really respect will put something up, and it takes everything I have to just say.

727
01:34:22.300 --> 01:34:24.089
Stacey Druss: Are you sure this is true?

728
01:34:24.500 --> 01:34:26.419
Stacey Druss: Are you sure this is real?

729
01:34:26.800 --> 01:34:30.630
Stacey Druss: And I'm happy to do it, because I think it needs to be done, but…

730
01:34:30.890 --> 01:34:36.699
Stacey Druss: When it comes to process, there's a voice inside my head that keeps saying, it takes a village.

731
01:34:36.900 --> 01:34:47.610
Stacey Druss: Because, unfortunately, Most of us… I mean, all of us, but some of us Check ourselves with it.

732
01:34:49.090 --> 01:34:54.070
Stacey Druss: We don't treat people we like the same as we treat people we don't like.

733
01:34:54.870 --> 01:34:58.040
Stacey Druss: That part, I think, is a default.

734
01:34:58.410 --> 01:35:09.850
Stacey Druss: I disagree with things like we all do it, because some of us have taken precautions and have worked to make sure they don't automatically do that.

735
01:35:10.300 --> 01:35:15.160
Stacey Druss: But for the most part, something happens in us And so…

736
01:35:17.120 --> 01:35:36.559
Stacey Druss: I guess I'm speaking to reliability, and depending on where it's coming from, and moving to the business model part of it, and I'll give one example of something from my side that goes to this need to create content that's gonna get eyeballs. This whole, Freedom Medal for big balls.

737
01:35:37.150 --> 01:35:38.819
Stacey Druss: It's a non-story.

738
01:35:38.980 --> 01:35:48.909
Stacey Druss: It's an absolute non-story, because at the root of its beginnings is one conservative guy, Benny Johnson.

739
01:35:49.170 --> 01:36:07.399
Stacey Druss: asking, which is really planting a story. I mean, and that's how they play us. They ask a question to plant a story for something that was never intended, so that now everybody can go and share all these stories to distract from what we should be paying attention to.

740
01:36:07.560 --> 01:36:14.180
Stacey Druss: So… and the other thing is, in terms of the numbers and, you know, correcting facts.

741
01:36:14.970 --> 01:36:26.139
Stacey Druss: I think those mistaken facts are put in on purpose, again, using the Benny… the, big ball story, and who he was beat up by.

742
01:36:27.010 --> 01:36:31.549
Stacey Druss: I hear 14 years old, I hear 15 years old. I'm like.

743
01:36:31.630 --> 01:36:45.110
Stacey Druss: how is it we're getting two different ages? Like, is it deliberate? I mean, it's something so small like that, but I've been noticing a lot of really small errors that people normally wouldn't pick up on, and I can't help but think.

744
01:36:45.110 --> 01:36:52.669
Stacey Druss: Do they purposely put these errors in? Because once you focus on that small error, you've lost the bigger point.

745
01:36:53.010 --> 01:37:00.979
Stacey Druss: So, I think I covered everything I wanted to talk about. Other… other… the most important thing is, though.

746
01:37:01.220 --> 01:37:05.489
Stacey Druss: How do we make it easier to question

747
01:37:05.860 --> 01:37:11.350
Stacey Druss: with, as Judith is always saying, that curiosity, but genuine curiosity.

748
01:37:11.770 --> 01:37:19.299
Stacey Druss: Not, you know, not… Condescending curiosity, because there's a big difference between the two.

749
01:37:19.760 --> 01:37:20.560
Stacey Druss: Over.

750
01:37:24.740 --> 01:37:34.769
Jerry Michalski: That was stellar, Stacey, thank you. Really appreciate it. And I believe that was the last substantive word for this call, although poems are of a substance slightly different.

751
01:37:35.940 --> 01:37:36.920
Jerry Michalski: Mr. Homer.

752
01:37:38.180 --> 01:37:43.720
Ken Homer • SF Bay Area: So, I apologize in advance, this poem has absolutely nothing to do with anything we've talked about on this call today.

753
01:37:43.970 --> 01:37:44.780
Jerry Michalski: Excellent!

754
01:37:44.780 --> 01:37:48.329
Ken Homer • SF Bay Area: But I wanted to share, if you were alive.

755
01:37:49.190 --> 01:37:51.999
Ken Homer • SF Bay Area: This is not Jack Benny, this is my father.

756
01:37:52.000 --> 01:37:52.810
Jerry Michalski: He would be 100.

757
01:37:52.810 --> 01:37:54.120
Ken Homer • SF Bay Area: 110 today.

758
01:37:55.250 --> 01:38:04.540
Ken Homer • SF Bay Area: So I have a poem about fathers. I just found it this week, and it moved me to tears when I read it, and I might do so again as I read it. It's called March 8th by David Lehman.

759
01:38:05.290 --> 01:38:08.160
Ken Homer • SF Bay Area: Every so often, my father comes over for a visit.

760
01:38:08.440 --> 01:38:11.239
Ken Homer • SF Bay Area: He hangs his overcoat and hat on my hat rack.

761
01:38:11.500 --> 01:38:17.709
Ken Homer • SF Bay Area: I brief him on recent developments and serve us coffee. He's surprised that I like to cook.

762
01:38:18.150 --> 01:38:24.190
Ken Homer • SF Bay Area: Once, when he made an omelet, he flipped it in the air, much to my delight, and it landed on the floor.

763
01:38:24.370 --> 01:38:25.260
Ken Homer • SF Bay Area: Yes.

764
01:38:25.560 --> 01:38:27.959
Ken Homer • SF Bay Area: That was the summer of 1952.

765
01:38:28.320 --> 01:38:33.189
Ken Homer • SF Bay Area: He remembered the high breakers, and how furious I was running into the ocean anyway.

766
01:38:33.830 --> 01:38:41.529
Ken Homer • SF Bay Area: The important thing is to see how you're doing so well, he said, and took his coat and hat and left before I remembered that he was dead.

767
01:38:44.260 --> 01:38:44.870
Jerry Michalski: Oof.

768
01:38:48.660 --> 01:38:49.350
Jerry Michalski: Oof.

769
01:38:54.400 --> 01:38:56.850
Ken Homer • SF Bay Area: So, in memories of father, fathers everywhere.

770
01:38:57.490 --> 01:38:58.500
Jerry Michalski: Thank you so much.

771
01:39:00.130 --> 01:39:01.400
Jerry Michalski: That was beautiful.

772
01:39:02.260 --> 01:39:02.960
Jerry Michalski: Wow.

773
01:39:05.390 --> 01:39:07.259
Jerry Michalski: Hey, everybody.

774
01:39:08.480 --> 01:39:10.050
Jerry Michalski: Thank you for an awesome call.

775
01:39:11.200 --> 01:39:12.420
Jerry Michalski: Really appreciate it.

776
01:39:15.110 --> 01:39:16.800
Jerry Michalski: More of this, okay?

777
01:39:17.940 --> 01:39:18.690
Jerry Michalski: Cool.

778
01:39:19.730 --> 01:39:22.259
Jerry Michalski: See you around. Let's be careful out there.

779
01:39:23.750 --> 01:39:24.780
Jerry Michalski: Bye for now.

780
01:39:24.780 --> 01:39:25.690
Gil Friend • Sustainability OG • CxO Coach: Bye, everybody.

