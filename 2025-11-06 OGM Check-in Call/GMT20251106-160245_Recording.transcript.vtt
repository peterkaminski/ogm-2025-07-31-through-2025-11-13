WEBVTT

1
00:00:00.050 --> 00:00:02.130
Alex Kladitis: It's just… It's just…

2
00:00:03.440 --> 00:00:11.980
Jerry Michalski: This is the OGM weekly call for Thursday, November 6th, 2025, and we're talking about switching from left-hand drive to right-hand drive.

3
00:00:13.120 --> 00:00:13.930
Jerry Michalski: Oops.

4
00:00:19.490 --> 00:00:23.149
Jerry Michalski: And inconveniently, I was just receiving a call, which I declined.

5
00:00:25.470 --> 00:00:28.709
Gil Friend • Sustainability OG • CxO Coach: Actually, it's about driving on the left side of the road to driving on the right side of the road.

6
00:00:28.710 --> 00:00:30.369
Jerry Michalski: Thank you, yeah, I was trying to…

7
00:00:30.370 --> 00:00:33.560
Gil Friend • Sustainability OG • CxO Coach: And left-hand drive is the reverse thing, it's where the wheel is.

8
00:00:33.750 --> 00:00:39.380
Jerry Michalski: And left-hand drive… right. Exactly.

9
00:00:40.490 --> 00:00:44.779
Jerry Michalski: And I… can anybody Google whether other countries have done this besides Sweden?

10
00:00:45.040 --> 00:00:46.350
Jerry Michalski: Or even clawed.

11
00:00:46.350 --> 00:00:46.950
Pete Kaminski: Cool.

12
00:00:46.950 --> 00:00:47.810
Jerry Michalski: Somebody else?

13
00:00:48.070 --> 00:00:50.769
Pete Kaminski: Nigeria, Ghana, Saudi Arabia, Yemen.

14
00:00:51.380 --> 00:00:52.260
Jerry Michalski: I mean, there were so much.

15
00:00:52.260 --> 00:00:52.870
Alex Kladitis: 19.

16
00:00:53.150 --> 00:00:54.240
Gil Friend • Sustainability OG • CxO Coach: Pizza does all do a.

17
00:00:54.240 --> 00:00:55.260
Alex Kladitis: Overnight?

18
00:00:58.620 --> 00:00:59.240
Gil Friend • Sustainability OG • CxO Coach: implies a lot.

19
00:00:59.560 --> 00:01:01.550
Gil Friend • Sustainability OG • CxO Coach: Coordination, that's probably rare.

20
00:01:05.060 --> 00:01:10.679
Pete Kaminski: It's… I haven't quite got to how fast it was, but it was pretty fast, too.

21
00:01:11.890 --> 00:01:12.839
Jerry Michalski: You kind of have to do…

22
00:01:12.840 --> 00:01:13.210
Alex Kladitis: They can go.

23
00:01:13.210 --> 00:01:14.580
Jerry Michalski: You have to do it all at once. Go ahead.

24
00:01:14.580 --> 00:01:15.610
Gil Friend • Sustainability OG • CxO Coach: Yeah.

25
00:01:15.820 --> 00:01:20.549
Alex Kladitis: The main reason was it was the British Empire, and they drove on the left.

26
00:01:20.870 --> 00:01:25.700
Alex Kladitis: So why did the US… Go to the right from the start.

27
00:01:26.020 --> 00:01:27.580
Alex Kladitis: They were supposed to be colonies.

28
00:01:27.580 --> 00:01:30.749
Pete Kaminski: It's whether you descend from England or Germany.

29
00:01:31.050 --> 00:01:33.560
Pete Kaminski: So, Germans were on the right.

30
00:01:33.880 --> 00:01:37.360
Alex Kladitis: So, was there a time where the…

31
00:01:37.660 --> 00:01:42.819
Alex Kladitis: different states in the US, the carriages were driving in the wrong… the different sites.

32
00:01:42.820 --> 00:01:43.570
Pete Kaminski: question.

33
00:01:44.010 --> 00:01:59.970
Jerry Michalski: A good question. We, you know, we could look this up. And I seem to remember that at one point, left or right-hand drive had to do with which hand was free when you were in the carriage with horses. And so if you… most people are right-handed, so you wanted to be able to use your right hand

34
00:02:01.190 --> 00:02:04.259
Jerry Michalski: Defensively, I have no idea, but I think that has something to do…

35
00:02:04.260 --> 00:02:08.059
Gil Friend • Sustainability OG • CxO Coach: I heard that it stems from… from… from tournaments, from Knights tournaments.

36
00:02:08.060 --> 00:02:08.719
Jerry Michalski: From jousting?

37
00:02:08.720 --> 00:02:14.040
Gil Friend • Sustainability OG • CxO Coach: And free for your jousting harness, for your right hand free for your weapon, but that may be totally crap, I don't know.

38
00:02:14.390 --> 00:02:15.000
Jerry Michalski: Yep.

39
00:02:15.270 --> 00:02:18.229
Jerry Michalski: So here we go. We have our amateur historical theories.

40
00:02:18.370 --> 00:02:24.680
Pete Kaminski: To warm up our space. I think another one is what side of the ship you unload.

41
00:02:25.260 --> 00:02:26.570
Jerry Michalski: Well, that's port and starboard.

42
00:02:26.570 --> 00:02:27.590
Gil Friend • Sustainability OG • CxO Coach: Huh.

43
00:02:27.590 --> 00:02:30.340
Pete Kaminski: But I… I think that descended, or it…

44
00:02:30.340 --> 00:02:31.120
Jerry Michalski: Do you think it's connected?

45
00:02:31.120 --> 00:02:31.940
Pete Kaminski: To, yeah.

46
00:02:32.280 --> 00:02:34.939
Jerry Michalski: Oh, now I'm… Latigo is a whip.

47
00:02:35.220 --> 00:02:36.170
Jerry Michalski: Victoria.

48
00:02:36.500 --> 00:02:44.989
Jerry Michalski: The, Briefly, we… before they invented the rudder at the stern of the ship.

49
00:02:45.150 --> 00:02:50.190
Jerry Michalski: There were steer boards, and this is the origin of the words, cyborg.

50
00:02:50.320 --> 00:03:06.750
Jerry Michalski: Because it was actually the… well, more interestingly, perhaps, the steer board, because most people were right-handed, was usually on the right side of your ship, and you didn't want to damage the steer board when you docked, so you would dock on… you would put the port on the port side.

51
00:03:07.150 --> 00:03:11.280
Jerry Michalski: And that's why we have steerboard and port, which became port and starboard.

52
00:03:11.710 --> 00:03:12.760
Alex Kladitis: Oh, boy.

53
00:03:13.700 --> 00:03:16.900
Jerry Michalski: So that's why The port was always on your, on your left.

54
00:03:17.110 --> 00:03:19.760
Gil Friend • Sustainability OG • CxO Coach: I'm so glad I come to OGM.

55
00:03:20.180 --> 00:03:21.110
Gil Friend • Sustainability OG • CxO Coach: kinds of things.

56
00:03:21.300 --> 00:03:27.750
Jerry Michalski: Some days I feel like this clavering, I'm full of shit like that, so… Off we go.

57
00:03:27.750 --> 00:03:34.950
Pete Kaminski: different countries switched in one day. The difference is actually the lead time. Sweden took 3 or 4 years, and other countries took 1 or 2.

58
00:03:36.930 --> 00:03:37.740
Jerry Michalski: Thank you.

59
00:03:38.980 --> 00:03:48.829
Jerry Michalski: Fabulous. Today, we are on the first Thursday of November, which typically means we do a check-in routine.

60
00:03:48.970 --> 00:03:53.930
Jerry Michalski: Mmm… Victoria, I don't think you've been through one of our check-in rounds, so I'll explain the protocol.

61
00:03:54.380 --> 00:04:09.590
Jerry Michalski: I am going to not moderate until everybody has checked in only once. We don't want any conversation between the check-ins. Each of us is just going to check in and say, what OGME thing is on our minds, or have we done, or is, like, active for us? It's not…

62
00:04:09.900 --> 00:04:17.350
Jerry Michalski: The idea is not to sort of dump the whole laundry list of everything you're doing into the call, but rather to pick something that feels

63
00:04:18.610 --> 00:04:22.819
Jerry Michalski: exemplifies what you've been up to in some sense. And

64
00:04:22.920 --> 00:04:42.030
Jerry Michalski: we don't use the chat, and this is difficult, I know, but especially for Pete and for me, who adore following things up with, oh, I found it, I found it, here's… look, look what the link is. But Pete and I will probably do that privately, and then once everybody has checked in once, we switch back into normal conversational mode.

65
00:04:42.220 --> 00:04:53.289
Jerry Michalski: But I… during the check-in, whoever wants to talk next, raise your hand, or just, if nobody has their hand raised, just step in, step into the call. Whenever you're moved to,

66
00:04:53.420 --> 00:04:54.410
Jerry Michalski: Check in.

67
00:04:54.830 --> 00:04:58.030
Jerry Michalski: And, what am I forgetting about the protocol?

68
00:05:00.630 --> 00:05:18.390
Jerry Michalski: I think that's most of it. So, try not to engage in conversation during the check-in round, it's just one person at a time, and then just make notes on a piece of paper in the chat without hitting enter, without hitting return, so it doesn't go to the chat, but make notes about stuff you'd like to talk about when we go back into conversation mode, that's all.

69
00:05:19.310 --> 00:05:21.910
Jerry Michalski: There you go Excellent.

70
00:05:22.080 --> 00:05:28.679
Jerry Michalski: So, with that, I am going to mute myself, and

71
00:05:28.820 --> 00:05:30.750
Jerry Michalski: Oh, oh, I know what I forgot.

72
00:05:30.850 --> 00:05:49.080
Jerry Michalski: we… during check-in rounds, we like, empty space. We like quiet. So it's perfectly fine for us to be still and just hang out with each other, kind of quicker meeting style. That is a big plus, because this is a bit of an antidote to our normally frenzied pace.

73
00:05:49.240 --> 00:05:50.860
Jerry Michalski: Through conversations.

74
00:05:51.150 --> 00:05:52.499
Jerry Michalski: That's the thing I forgot.

75
00:05:52.500 --> 00:05:59.989
Gil Friend • Sustainability OG • CxO Coach: Jerry, could I just suggest, before we go into a formal check-in round, that, silence is not a terrible thing even during a normal call? We might…

76
00:06:00.330 --> 00:06:01.369
Gil Friend • Sustainability OG • CxO Coach: A little more breath.

77
00:06:01.760 --> 00:06:09.299
Jerry Michalski: Agreed, and I occasionally… I occasionally take us into silence during a normal call, but I often… I often forget for too long, so…

78
00:06:09.480 --> 00:06:11.030
Gil Friend • Sustainability OG • CxO Coach: I welcome it when you do it.

79
00:06:11.030 --> 00:06:22.830
Jerry Michalski: Thank you. And I love taking groups into silence, and they always welcome it. Everybody's like, oh, okay, good. And you can just see people tune in differently, so it's lovely. Thank you for the reminder again.

80
00:06:23.500 --> 00:06:28.380
Jerry Michalski: With that, I'm gonna mute myself and see who wants to be our first, check-in.

81
00:06:35.760 --> 00:06:37.459
Scott Moehring: I think I'm gonna go first.

82
00:06:38.620 --> 00:06:40.010
Scott Moehring: Yeah,

83
00:06:41.880 --> 00:06:48.390
Scott Moehring: So… Thinking about my personal projects and stuff.

84
00:06:48.600 --> 00:06:52.110
Scott Moehring: I stumbled onto a guy named Kevin McLeod.

85
00:06:53.080 --> 00:06:57.190
Scott Moehring: Through a documentary that I haven't watched yet. So, Kevin McCloud.

86
00:06:57.940 --> 00:07:02.400
Scott Moehring: Has made 2,000 pieces of royalty-free music.

87
00:07:02.540 --> 00:07:05.349
Scott Moehring: and made them available under the Creative Commons.

88
00:07:06.540 --> 00:07:12.379
Scott Moehring: He did this just… He was a composer and decided to start doing this.

89
00:07:12.700 --> 00:07:19.059
Scott Moehring: And as a result, his music has been picked up and used in the backgrounds of

90
00:07:19.290 --> 00:07:25.020
Scott Moehring: countless TikTok videos and Facebook Reels, and all that kind of stuff.

91
00:07:25.690 --> 00:07:31.379
Scott Moehring: One of his songs has been played 31.6 billion times.

92
00:07:33.110 --> 00:07:38.809
Scott Moehring: He's the most prolific composer you've never heard of, according to the New York Times.

93
00:07:39.000 --> 00:07:46.700
Scott Moehring: And what it made me think about… Was the idea of… Remixability.

94
00:07:46.940 --> 00:07:48.699
Scott Moehring: Of what you are producing.

95
00:07:50.220 --> 00:07:56.140
Scott Moehring: And instead of making meals, making Perhaps ingredients or recipes.

96
00:07:56.630 --> 00:08:03.230
Scott Moehring: And things that other people can and want to use to create things that are theirs.

97
00:08:03.970 --> 00:08:12.929
Scott Moehring: As opposed to… Spending most of my time creating something that's… Perfect and self-contained and…

98
00:08:13.370 --> 00:08:21.350
Scott Moehring: It's its own thing, never to be taken apart and…

99
00:08:22.300 --> 00:08:24.120
Scott Moehring: Given a life of its own.

100
00:08:24.670 --> 00:08:26.870
Scott Moehring: Through interactions with other people.

101
00:08:27.820 --> 00:08:34.820
Scott Moehring: So, that relates to the projects that I've been working on, and it's kind of given me a different sort of perspective on

102
00:08:35.030 --> 00:08:36.399
Scott Moehring: that idea.

103
00:08:36.549 --> 00:08:44.190
Scott Moehring: How can I make the things that I'm doing more… Remixable.

104
00:08:44.470 --> 00:08:46.759
Scott Moehring: By the people who might want to use them.

105
00:08:46.950 --> 00:08:49.130
Scott Moehring: I've talked about my universals.

106
00:08:49.680 --> 00:08:57.689
Scott Moehring: set of… frameworks, and I've turned them into a little deck that I've been playing around with.

107
00:08:57.800 --> 00:09:06.529
Scott Moehring: And the ability to take several and move them around and recombine them and all of that has changed it for me.

108
00:09:06.700 --> 00:09:10.239
Scott Moehring: And I hope that it's going to make it actually more usable.

109
00:09:10.480 --> 00:09:12.720
Scott Moehring: for other people, should I publish this?

110
00:09:13.540 --> 00:09:14.699
Scott Moehring: And that's it for me.

111
00:09:45.490 --> 00:09:51.999
Gil Friend • Sustainability OG • CxO Coach: Well, I'll go. I'll confess, I'm finding it very hard not to comment on what Scott just said, but I won't.

112
00:09:56.170 --> 00:10:12.499
Gil Friend • Sustainability OG • CxO Coach: Most OGM-y things for me, and Alex will like hearing about this, that I've been working on vibe coding, a technique for scrubbing, for automatically de-identifying, transcripts of coaching conversations that I'm having with my clients. As most of you know, I'm

113
00:10:12.920 --> 00:10:15.810
Gil Friend • Sustainability OG • CxO Coach: Primarily working these days as an executive coach.

114
00:10:17.110 --> 00:10:18.630
Gil Friend • Sustainability OG • CxO Coach: Excuse me.

115
00:10:18.750 --> 00:10:24.119
Gil Friend • Sustainability OG • CxO Coach: For CXOs and founders, who are purpose-driven, I,

116
00:10:24.980 --> 00:10:46.119
Gil Friend • Sustainability OG • CxO Coach: I de-identify the conversations, I feed them into one of two bots that I've got running, one on Delphi and one that Alex and I have been building custom in the lab, which is more cool and sophisticated. And of course, to use that, I have to get any personally identifying information out of the scheme. I've done that with manual global search and replace for a long time, and now I've got Claude

117
00:10:46.130 --> 00:10:48.050
Gil Friend • Sustainability OG • CxO Coach: Has built me a drag and drop.

118
00:10:48.600 --> 00:10:56.370
Gil Friend • Sustainability OG • CxO Coach: finished configuring it yesterday, and today I'll test it on a handful of clients and see how well it does, and if it works well, then we're off to the races.

119
00:10:56.790 --> 00:11:07.020
Gil Friend • Sustainability OG • CxO Coach: And, aside from the specifics of that, it's cool for me to be diving in… this is my first re-entry into the world of coding in, like, 100 years.

120
00:11:07.410 --> 00:11:18.380
Gil Friend • Sustainability OG • CxO Coach: And so, kind of remarkable what, what's possible, both out of the box and with the kind of fine-tuning that becomes available. I know that a number of you have done way more than me.

121
00:11:18.730 --> 00:11:22.789
Gil Friend • Sustainability OG • CxO Coach: But that's the OGME report from me for today.

122
00:11:25.770 --> 00:11:30.109
Gil Friend • Sustainability OG • CxO Coach: And I'm still here, but I'm gonna hide, because I'm preparing breakfast, and you don't want to watch that.

123
00:12:14.420 --> 00:12:15.880
Stacey Druss: I'm passing today.

124
00:12:49.390 --> 00:12:50.320
Alex Kladitis: I'll do it.

125
00:12:51.570 --> 00:12:53.039
Alex Kladitis: If I may,

126
00:12:53.610 --> 00:13:03.260
Alex Kladitis: Something that surprised me, happened this week. I came across this week. Like, sadly, it's going to be an AI thing, but slightly different, I hope.

127
00:13:03.930 --> 00:13:09.980
Alex Kladitis: So, ever since LLMs have become so powerful, I have set up a number of them where

128
00:13:10.760 --> 00:13:17.900
Alex Kladitis: They scan the finance pages, and they give me a report every morning, they scan lots of medical things, give me reports, etc.

129
00:13:18.450 --> 00:13:22.990
Alex Kladitis: So, won't go into it, but I just wanted to know what China was saying.

130
00:13:23.990 --> 00:13:30.879
Alex Kladitis: And I repurposed I'll use the usual… the same format, more or less. I repurposed it so that

131
00:13:31.130 --> 00:13:35.119
Alex Kladitis: its task, as I asked it, was to…

132
00:13:35.780 --> 00:13:41.890
Alex Kladitis: scan all the Chinese, or as many Chinese, Big daily website papers, whatever.

133
00:13:42.080 --> 00:13:44.219
Alex Kladitis: And tell me what the Chinese are talking about.

134
00:13:44.640 --> 00:13:45.560
Alex Kladitis: Just that.

135
00:13:47.190 --> 00:13:50.030
Alex Kladitis: And what I've found, cut a long story short, is that

136
00:13:50.860 --> 00:13:54.339
Alex Kladitis: I'm using ChatGPT. It would not give me

137
00:13:54.570 --> 00:14:04.700
Alex Kladitis: what the Chinese papers are saying, without telling me why they're either wrong, or this is a true situation, or whatever. It was as if, inbuilt into the AI,

138
00:14:05.080 --> 00:14:06.980
Alex Kladitis: Is this bit that says.

139
00:14:07.540 --> 00:14:19.839
Alex Kladitis: whatever the Chinese say is not true, this is the interpretation that we're giving. And I've tried really hard to say to it, do not comment on it, do not do anything. All I want is a strapline of what they're saying. That is it.

140
00:14:20.240 --> 00:14:24.619
Alex Kladitis: And the reason I found it interesting was because, in the end, I gave up, because I just wouldn't do it.

141
00:14:24.840 --> 00:14:29.920
Alex Kladitis: The reason I find it interesting is because when DeepSea came up, which is the Chinese thing.

142
00:14:30.750 --> 00:14:35.650
Alex Kladitis: the whole press was saying it's biased towards pro-China stuff.

143
00:14:36.220 --> 00:14:44.720
Alex Kladitis: And it was. If you… if you tried to do anything, anything politics with it, it would, and particularly Tiananmen Square, it would actually not…

144
00:14:44.860 --> 00:14:47.380
Alex Kladitis: Not behave in our biostanders.

145
00:14:47.730 --> 00:14:49.569
Alex Kladitis: But I was surprised to see how

146
00:14:50.290 --> 00:14:54.700
Alex Kladitis: Obviously, in my opinion, it's inbuilt into the training data.

147
00:14:55.090 --> 00:14:59.829
Alex Kladitis: that we have our biases as well. And more to the point.

148
00:15:00.230 --> 00:15:05.790
Alex Kladitis: It's, you know, we talk about AI being safe and whatever, you know, all this.

149
00:15:06.640 --> 00:15:13.919
Alex Kladitis: And I'm surprised that it's so inbuilt. I could talk about this for a lot further, but basically, it just surprised me that our

150
00:15:14.130 --> 00:15:21.630
Alex Kladitis: AIs are also quite Opinionated in certain directions, particularly that one.

151
00:15:22.560 --> 00:15:24.530
Alex Kladitis: So…

152
00:15:39.050 --> 00:15:40.780
Pete Kaminski: I'll go.

153
00:15:42.750 --> 00:15:58.599
Pete Kaminski: As you may know, I've been using, Agentic AI assistance for software development for, I don't know, 6-8 months, probably longer than that. So I'm using… I've been using Cloud Code, and it…

154
00:15:58.600 --> 00:16:04.989
Pete Kaminski: Is… it works with me about at the level of a human software developer.

155
00:16:04.990 --> 00:16:14.759
Pete Kaminski: Agentic means lots of crazy things, and AI means lots of crazy things, but when I say agentic AI, coding assistant, what I mean is…

156
00:16:14.900 --> 00:16:28.779
Pete Kaminski: something like an LLM, but set up so that it can read and write many files, source code files, or text files, or whatever. And also, one of the things it does is, you can tell it

157
00:16:28.860 --> 00:16:47.119
Pete Kaminski: to do something, and it will make a little list for itself. Here's a bunch of to-dos that I need to get Pete's bug fixed, or feature implemented, or something like that. And then it goes down that list, and checks them off, and does another thing, checks it off, does another thing. So it takes the LLM experience and kind of up-levels it towards

158
00:16:47.540 --> 00:16:59.460
Pete Kaminski: an autonomous human kind of level, and I don't mean that it's smart like a human, or thoughtful like a human, or creative like a human, although it kind of is, all of those.

159
00:17:00.360 --> 00:17:09.489
Pete Kaminski: So, a thing that I've been tracking for a long time is doing the same thing with, text, either non-fiction or fiction.

160
00:17:09.520 --> 00:17:22.749
Pete Kaminski: Having a bot understand, like, a big chunk of, serial fiction, for instance, and being able to write new episodes, or going over the whole thing and making a plan for

161
00:17:22.900 --> 00:17:42.690
Pete Kaminski: you know, new volume in a series, something like that. It does that really well. I think it would also work great for a legal team, doing a bunch of work with lots of documents, and they kind of all need to fit together and change, you know, together, and…

162
00:17:42.690 --> 00:17:58.540
Pete Kaminski: Tell me, you know, tell me the overarching meaning of these documents together. Tell me which ones don't quite fit right. How would you rewrite it? Go ahead and try rewriting that and see if I like the draft that you do. No, I don't like that one. Throw it away, do another one.

163
00:17:58.930 --> 00:18:01.140
Pete Kaminski: Nonfiction, big…

164
00:18:01.140 --> 00:18:23.010
Pete Kaminski: research studies, fiction, legal stuff, anything where you're using a corpus of documents and multiple people. So I've been super excited about this for a while, and have a hard time explaining to people why it's cool and why they should be doing it as the same like me. I finally got the first taste of… I've worked with a couple people.

165
00:18:23.010 --> 00:18:26.140
Pete Kaminski: Adopting these tools, and…

166
00:18:26.190 --> 00:18:42.600
Pete Kaminski: it's kind of a mix of, like, terror, because, you know, there's a lot of overhead to get to be able to work with the tool. It doesn't feel like a lot of overhead to me, because software developers built these things, so they built it, so it just fits… slots perfectly into a software development thing.

167
00:18:42.600 --> 00:18:46.420
Pete Kaminski: And for me, it feels like there's a little bit of a difference between

168
00:18:46.420 --> 00:19:04.279
Pete Kaminski: building software and building an Obsidian Vault, and yada yada, and using Git, and it gets more and more complicated, kind of. So, so the newest one, Cloud Code, now has a web version. You don't have to use your terminal. There's another one called Droid from a company named Factory AI, which also has a web.

169
00:19:04.390 --> 00:19:19.020
Pete Kaminski: the web versions also work on mobile, kind of, so we're right at the cusp of these things not being where you need a geek to… you don't need to be a geek to use it. However, I've also… I've also kind of had a…

170
00:19:19.370 --> 00:19:22.930
Pete Kaminski: I… I was really disappointed in the…

171
00:19:23.110 --> 00:19:39.379
Pete Kaminski: in the difficulty with which a non-technical person had to kind of, like, muscle their way through it. I was watching somebody use it, and they were happy, but I could see it was kind of like they had oven mitts on, and they were trying to do, you know, fine-grained tasks.

172
00:19:39.640 --> 00:19:40.710
Pete Kaminski: So…

173
00:19:40.830 --> 00:19:52.339
Pete Kaminski: Anyway, long story short, I think… I think Cloud Code Web, I… at least some people are getting a $250 credit if you go to your Cloud… Cloud Pro account.

174
00:19:52.340 --> 00:20:02.459
Pete Kaminski: it might say, you should try Cloud Code Web, and here's $250 for you to spend by November 18th. Droid has another big promotion right now.

175
00:20:02.560 --> 00:20:20.680
Pete Kaminski: More people should be doing this, and I'm kind of pulling back from saying everybody should do it right away, because it's a little hard, and using the web version is actually even a little bit more hard than the terminal version, I think. It's an easier on-ramp, but goes…

176
00:20:21.080 --> 00:20:37.980
Pete Kaminski: goes… it's harder… it's more like molasses or something like that. Everybody's gonna be doing this kind of stuff in 18 months, 24 months. You could be doing it today. I would love to show some people how to do that more.

177
00:20:37.980 --> 00:20:43.930
Pete Kaminski: Kind of as a coda, one of the things that I've been doing, a lot of times from an OGM call.

178
00:20:44.020 --> 00:20:51.159
Pete Kaminski: Somebody will mention something, I think it was last week, actually, Stacy mentioned something about,

179
00:20:51.300 --> 00:21:07.390
Pete Kaminski: people in her building working together, and that makes a… a little society that protects itself from outsiders and things like that. And I said, huh, that reminds me of the broken windows theory, that reminds me of this, that reminds me of that. And so…

180
00:21:07.540 --> 00:21:10.980
Pete Kaminski: a thing I've been doing for about 6 months is…

181
00:21:11.450 --> 00:21:14.590
Pete Kaminski: I know there's a bunch of frameworks around a topic.

182
00:21:15.080 --> 00:21:31.390
Pete Kaminski: hey, AI, help me put together a wiki, you know, a 20-page kind of paper slash thesis slash framework slash overview of sometimes things that are a little bit different, that people have kind of been looking at the same part of the world.

183
00:21:31.550 --> 00:21:46.259
Pete Kaminski: And coming up with different ways to do it, and they don't talk to each other. So the most fun for me is when me and the AI can kind of smash frameworks together that ought to go together, but never have before. And so I've been building these 10, 20-page websites like that.

184
00:21:46.260 --> 00:21:51.960
Pete Kaminski: Collective efficacy, social cyclic theories, the intention-action gap.

185
00:21:51.990 --> 00:22:07.809
Pete Kaminski: strategy and management frameworks, government… governance principles, commons stewardship by communities, and I… I just do these kind of one-off things, and they're half, like, amazing, and I've… I've pointed out a couple of them in the OGM list.

186
00:22:08.070 --> 00:22:21.739
Pete Kaminski: And half, I'm embarrassed by it, because it's mostly built by AI. So, I'm working with the AIs now to kind of frame that into what is good about these things, and what is bad.

187
00:22:22.070 --> 00:22:37.119
Pete Kaminski: how do humans work with them? Why is it okay that the AI has kind of organized things in a way that makes sense? So, I'm working on kind of a meta-collection of all these things together called

188
00:22:37.160 --> 00:22:42.979
Pete Kaminski: field of sheaths. The, each of these things I'm thinking of as a sheaf.

189
00:22:43.040 --> 00:23:03.149
Pete Kaminski: So now I've got, you know, 6 or 8 sheaves of, of whatever these things knowledge together. And, so I'm working on that. I'm too embarrassed to release it, and, yet, but I am working on it. If this is something that's like, oh my gosh, Pete, could I help, you know, could I do that? Could you help me do one of those myself?

190
00:23:03.150 --> 00:23:05.800
Pete Kaminski: I'd love to… love to hear from you.

191
00:23:07.280 --> 00:23:08.160
Pete Kaminski: Thanks.

192
00:23:12.170 --> 00:23:23.219
Kevin Jones: I'll go next, quickly. If you guys got the Plex, I think, you should,

193
00:23:23.490 --> 00:23:31.090
Kevin Jones: I'm really happy about the two people I asked to write for it, and they wrote this one, and I didn't have anything, and I was very happy with it, so…

194
00:23:31.260 --> 00:23:32.060
Kevin Jones: That's…

195
00:24:06.750 --> 00:24:08.830
David Witzel: Well, I'll toss in what I've been thinking about.

196
00:24:09.020 --> 00:24:15.420
David Witzel: been playing with the notion of growth. The framework is kind of like, how do I…

197
00:24:15.500 --> 00:24:31.670
David Witzel: integrate, regenerative notions that I've been, you know, developing over the last few years, or have been, you know, that have been emerging, I think, in many spaces over the last few years, into kind of the technocratic public policy background that I've… that I've always, thought of myself in.

198
00:24:31.670 --> 00:24:48.430
David Witzel: And one of the key underlying things is that in the public policy world, we don't really focus on living systems. You know, we may talk about systems, we may, you know, and maybe not often even that. We just talk about optimization and efficiencies, and kind of, you know, there's,

199
00:24:48.430 --> 00:24:53.660
David Witzel: these other values that get, get used as proxies for something good, I think.

200
00:24:53.660 --> 00:25:04.000
David Witzel: And so we, you know, you tend to get trapped into making things a little bit better, and rarely can you get to kind of a positive sum outcome abundance kind of notions.

201
00:25:04.120 --> 00:25:20.350
David Witzel: And so I've been like, well, how would I retrofit some of that into, you know, our old… the old framings? And, you know, what… another version of that is how… what can we learn from traditional economics? Like, a lot of people kind of want to just throw it away and say economists are fools, and…

202
00:25:20.420 --> 00:25:36.219
David Witzel: you know, they're all… they're funny, I like to make fun of them and stuff. But, you know, there's a lot of wisdom in economics, too, so I wouldn't… I don't think we should toss it away. We should think of it as Indigenous knowledge, probably. So what… what is it, you know, how do these two things get together? And one of the core words, anyway, I think, is growth.

203
00:25:36.220 --> 00:25:45.959
David Witzel: Traditional economics kind of relies on growth as… certainly as one of the answers to, like, do you want to get rid of poverty, how do you get rid of poverty? Well, you grow.

204
00:25:46.010 --> 00:26:05.399
David Witzel: So there's… there's that kind of component, but the other is just kind of, like, economies tend to be robust while they're growing. They tend to get very fragile while they're not. So there's… there's this kind of key brain in there. And living systems are kind of critical. Growth matters a ton to living systems, and so I've just been wondering

205
00:26:05.420 --> 00:26:13.649
David Witzel: Like, what are the kinds of growth and the perspectives on growth that would inform, kind of, traditional economics in a more of a living systems?

206
00:26:13.720 --> 00:26:15.420
David Witzel: way.

207
00:26:15.780 --> 00:26:28.030
David Witzel: And are there kind of examples, you know, basically, metaphors that we can take from biological systems around growth that would inform traditional economics in new ways?

208
00:26:28.030 --> 00:26:45.020
David Witzel: And for, you know, part of what provoked this was somebody talking about a story of how they had submitted for their Stanford application decades ago. They had submitted… they had to write a review of the book they'd written, and the book that she had written was about the history of sourdough.

209
00:26:45.050 --> 00:27:01.810
David Witzel: And how people, like, in the, as they were crossing the country, would keep their sourdough alive, and what it would take to keep your sourdough starter alive, and how, you know, the women pioneers were keeping the sourdough next to their breasts to keep it warm as they crossed the mountains.

210
00:27:01.810 --> 00:27:08.590
David Witzel: And I was just thinking about, like, the lively… the sourdough being alive, and the growth of sourdough as being a metaphor for something.

211
00:27:08.590 --> 00:27:18.780
David Witzel: So that's as far as I've got, and I need to take it to, Pete's, much more… much smarter AI now to try and evaluate it, but growth, what's… what's… what sort of insights can he get from growth?

212
00:27:30.450 --> 00:27:34.160
Klaus Mager: Yeah, I got… I got an email yesterday from…

213
00:27:35.300 --> 00:27:38.619
Klaus Mager: a mailing list, the Union of Concerned Scientists.

214
00:27:39.090 --> 00:27:44.220
Klaus Mager: Sending out yet another, you know, last urgent warning.

215
00:27:44.400 --> 00:27:50.829
Klaus Mager: And, and, and… This has been going on for, like, 3, 4 years.

216
00:27:51.000 --> 00:28:02.779
Klaus Mager: And it's, from, from Oregon, actually, Portland's… I think Portland State, or Oregon State is, is the hub where now they're consolidating the data and,

217
00:28:02.890 --> 00:28:08.820
Klaus Mager: And, you know, explain how we are passing one tipping point after the next.

218
00:28:09.650 --> 00:28:10.610
Klaus Mager: Ann.

219
00:28:11.230 --> 00:28:22.130
Klaus Mager: it will be ignored, you know, it's just yet another alert that people pay attention to, and people who know what it is, saying, okay, yeah, I know.

220
00:28:22.240 --> 00:28:28.269
Klaus Mager: And others will dismiss it, like, you know, it has been done for, you know, a really long time.

221
00:28:28.540 --> 00:28:30.169
Klaus Mager: So the… the…

222
00:28:30.970 --> 00:28:44.120
Klaus Mager: the… what is really missing here is a transition to what can you actually do about this? And because there are lots of things, you know, we can do if we put our mind to it, but…

223
00:28:44.390 --> 00:29:02.770
Klaus Mager: It just… it just doesn't… doesn't seem to want to work. And of course, what we're doing, with our Food with Thought effort here is to engage, not the neglected part of the market.

224
00:29:02.990 --> 00:29:12.850
Klaus Mager: So you have small to medium-sized farmers, the… what's called the agriculture of the middle, who

225
00:29:13.880 --> 00:29:21.380
Klaus Mager: who can't get into the market. And the reason that is important is because…

226
00:29:21.630 --> 00:29:29.920
Klaus Mager: Food and agriculture is probably the most damaging part of our interaction with the natural world.

227
00:29:29.960 --> 00:29:47.199
Klaus Mager: destroying, you know, watersheds and soils and biodiversity, and so it's incredibly damaging, but it has the capacity to turn that. It has the capacity, you know, to recover and to heal and to regenerate.

228
00:29:48.540 --> 00:30:07.130
Klaus Mager: And the… in the… in the large companies, you know, whether that's Monsanto or Nestle or, you know, everybody in there, they're working to automate this. They're working to reduce nitrogen by, smart, technology.

229
00:30:07.130 --> 00:30:13.720
Klaus Mager: Climate smart technology, and… and, Hugely capital intensive.

230
00:30:13.980 --> 00:30:28.080
Klaus Mager: Even going to exclude more farmers who cannot get into the investment cost required to build, you know, these automations, and it's not really necessary.

231
00:30:28.170 --> 00:30:42.120
Klaus Mager: you can absolutely, shift, you know, into regenerative practices at scale if farmers are being incentivized and paid for. So the…

232
00:30:42.690 --> 00:30:55.690
Klaus Mager: what we would need to know about it, of course, is a farmer who wants to put in a cover crop and then put his cash crop in needs different pieces of equipment to make that work. Well, they're expensive.

233
00:30:55.750 --> 00:31:06.670
Klaus Mager: Now, so… so how is… is a small to medium-sized farmer going to shift into environmental farming, you know, without, having

234
00:31:06.670 --> 00:31:17.399
Klaus Mager: the support structure for it. And so what we are arguing as a core point here is there are measurements

235
00:31:17.490 --> 00:31:21.289
Klaus Mager: That, make it possible to…

236
00:31:21.480 --> 00:31:37.070
Klaus Mager: to monitor what a farmer is doing, what the farmer has promised to do, so put a contract out there, and then monitor the outputs of that. So there are already some great examples, for example, in the Mississippi with Delta,

237
00:31:37.540 --> 00:31:45.410
Klaus Mager: there are groups of farmers who have been combined to reduce nitrogen runoff into the Mississippi River.

238
00:31:45.410 --> 00:32:00.229
Klaus Mager: which then, of course, runs down into the Gulf of Mexico. Well, that's easy to measure, you know? It doesn't take much to track that. And the incentives have been sufficient enough for those farmers

239
00:32:00.230 --> 00:32:19.059
Klaus Mager: who are, very conservative and typically not interested, in, in experimenting a lot, there's just so much money at stake that they are going to do it. And, and so what's these… what… and there are all kinds of, of little, like, one point…

240
00:32:20.300 --> 00:32:32.069
Klaus Mager: measurements that can… that have the capacity to shift the entire system, because in order to reduce the nitrogen one-off, you have to find other ways to fertilize your crop.

241
00:32:32.200 --> 00:32:34.919
Klaus Mager: Now, and so, in order… in the most…

242
00:32:35.080 --> 00:32:53.539
Klaus Mager: A basic thing is soil health. So if you have to increase your soil health, now we're talking about putting a cover crop on it. Now, maybe doing a no-till, maybe rotating your crop into a different type of crop, maybe a perennial or so, in order to do this. So all these other things fall into place.

243
00:32:53.590 --> 00:33:07.409
Klaus Mager: So by building a secondary revenue stream, the farmer now has two crops. He has his one, his food crop, and then he has an environmental benefit crop, you know, that comes out, that is linked together.

244
00:33:07.830 --> 00:33:21.979
Klaus Mager: So we're saying this is one. I mean, in every industry, there are ways where we could insert ourselves and provide the incentives at scale, you know, to catch

245
00:33:22.100 --> 00:33:28.189
Klaus Mager: the environmental deterioration that is, like, so obvious in your face now.

246
00:33:28.300 --> 00:33:45.700
Klaus Mager: And in the… but what we are focusing on is the mostly NGOs, nonprofit, small, medium-sized farms. So… and the reason for that is that if you combine all of the farmers markets and the CSAs.

247
00:33:45.740 --> 00:33:48.909
Klaus Mager: You know, and, and the,

248
00:33:49.520 --> 00:33:59.409
Klaus Mager: The low-level, output of the sector. It's a single digit of total sales.

249
00:33:59.630 --> 00:34:10.980
Klaus Mager: You know, last time I checked was, like, 3% of total sales, is if you combine all CSAs, farmers markets, into, into one… together.

250
00:34:11.150 --> 00:34:25.859
Klaus Mager: So, to get these farmers into the wholesale market, that's our objective, to assist bundling and aggregating small farmers so they can participate in wholesale markets.

251
00:34:26.040 --> 00:34:32.779
Klaus Mager: So I was just… I was just… and I wrote another newsletter. I mean, I was just thinking about how easy it would be

252
00:34:33.170 --> 00:34:41.400
Klaus Mager: You know, to really make an impact, and how stubbornly resistant the market is to engage in change.

253
00:36:12.490 --> 00:36:27.650
Jerry Michalski: I've got, one small thing and a bigger thing. The small thing is, I'm wearing a t-shirt, It's a beautiful day in this neighborhood, because April gave a talk in Pittsburgh, and after the talk, she went and visited the Mr. Rogers Museum.

254
00:36:27.770 --> 00:36:35.239
Jerry Michalski: In Pittsburgh. And Mr. Rogers has a super interesting history, but Mr. Rogers played a really important role in April's childhood.

255
00:36:37.280 --> 00:36:43.639
Jerry Michalski: Because she had a difficult relationship with her mom, and Mr. Rogers was telling her that she was okay just the way she is.

256
00:36:43.990 --> 00:36:53.369
Jerry Michalski: And was one of the few entities that was doing that for her back in the day, and I didn't… I didn't realize how special that was until we had talked about it some years back. And

257
00:36:53.610 --> 00:37:04.369
Jerry Michalski: So it was lovely that she got to visit the museum, and she came back with a road sign that says, this is a kindness zone, and it looks exactly like a street sign that, you know, you would put in a neighborhood.

258
00:37:04.580 --> 00:37:08.419
Jerry Michalski: And we're gonna put it up on the wall here, so that's kind of cool. So that's tiny.

259
00:37:08.600 --> 00:37:15.909
Jerry Michalski: Then, separately, I've been reading Dan Brown's latest potboiler, The Secret of Secrets.

260
00:37:16.490 --> 00:37:31.389
Jerry Michalski: And there'll be a tiny plot spoiler in what I say now, so if you really, really, really want to read a meh-written book, because Brown is not Faulkner, but one thing I love about Brown is, from The Da Vinci Code.

261
00:37:31.570 --> 00:37:37.969
Jerry Michalski: I picked up in, you know, two-thirds of the way through the book, he has this thesis about the marginalization of the divine feminine.

262
00:37:38.190 --> 00:37:51.230
Jerry Michalski: And I think I've told this story here before, but I loved that thesis, and I read Leonard Schlane's book, The Alphabet vs. the Goddess, within a couple months of the Da Vinci Code, and it has the same thesis in an academic work.

263
00:37:51.260 --> 00:37:53.160
Klaus Mager: From a completely different perspective.

264
00:37:53.160 --> 00:38:09.259
Jerry Michalski: And I had been chasing the word consumer for already a couple decades then, and I had gotten to the same place myself, thinking about yin and yang and a bunch of other stuff, all of which is a longer story. But all those three stories kind of clicked in my head in a really wonderful way, and I'm like, oh.

265
00:38:09.260 --> 00:38:14.300
Jerry Michalski: And now I start reading Dan Brown, and one of the major plot points of The Secret of Secrets

266
00:38:14.490 --> 00:38:17.360
Jerry Michalski: Is, non-local consciousness.

267
00:38:18.330 --> 00:38:29.939
Jerry Michalski: And that snapped together with what Sunil Malhotra, a friend of OGM's, had said to me, I don't know, a year and a half ago. He basically said, hey, and I mentioned it on an OGM call somewhere.

268
00:38:29.940 --> 00:38:42.569
Jerry Michalski: He said, hey, Jerry, what if consciousness isn't a thing that is… that materializes out of neurons in our heads when we have some kind of funny critical mass of neurons and activity, but what if consciousness is the background radiation

269
00:38:42.570 --> 00:38:43.670
Jerry Michalski: of the universe.

270
00:38:43.700 --> 00:38:45.300
Jerry Michalski: And we occupy it.

271
00:38:45.760 --> 00:38:48.219
Jerry Michalski: We sort of… we snap in… we… we… we…

272
00:38:48.320 --> 00:38:57.570
Jerry Michalski: tune into it, like a radio station, and then when we die, we sort of leave it, or meld back into it, something like that. And that really sort of

273
00:38:57.760 --> 00:39:05.859
Jerry Michalski: changed the way I look at things. And then, by reading Dan Brown, I realized I had already put non-local consciousness in my brain from a completely different

274
00:39:05.860 --> 00:39:18.259
Jerry Michalski: bit of research, and that it was the same general idea, that consciousness is, in fact, not a materialistic phenomenon of something that's happening in the chemistry in our heads, but is in fact a much broader thing.

275
00:39:18.260 --> 00:39:24.669
Jerry Michalski: a thing I like, which gets you into woo-woo noetic science territory, which I'm beginning to respect a lot more.

276
00:39:27.500 --> 00:39:43.739
Jerry Michalski: That… and so one of the interesting things in the plot point is, so Tuesday night, I stop someplace and have a meal and keep reading in the book, and I get to the point where they talk about GABA, the basically inhibitor or receptor in our heads.

277
00:39:43.740 --> 00:39:53.579
Jerry Michalski: GABA is, gamma-aminobutyric acid, GABA, or GABA. And it appears to be the primary inhibitor

278
00:39:53.600 --> 00:40:01.470
Jerry Michalski: in our neural system, which means that high levels of GABA keep us from being overwhelmed by the world's stimuli.

279
00:40:02.030 --> 00:40:11.740
Jerry Michalski: And when you have low GABA, you're suddenly, like, you take in a whole lot more. And apparently, one of the mechanisms of epilepsy

280
00:40:11.920 --> 00:40:17.180
Jerry Michalski: is a precipitous drop in GABA, which then causes your brain to be overwhelmed, and you go into seizure.

281
00:40:17.240 --> 00:40:32.269
Jerry Michalski: Epileptics, when they come back out of a grand mall seizure, have moments of bliss, of like, you know, connection with the infinite, and then they sort of reboot their brains back into being online with the usual constraints and filters on and everything else.

282
00:40:32.450 --> 00:40:34.779
Jerry Michalski: And then she's,

283
00:40:35.020 --> 00:40:45.139
Jerry Michalski: the protagonist's other half, who's part of the plot, says, you know, have you ever done, like, hallucinogens? They seem to reduce your GABA

284
00:40:45.140 --> 00:40:55.529
Jerry Michalski: levels so that you can perceive what's always happening around you. It's not that they generate a hallucination in your head, it's that they lower your filters so that you can see what's going on around you.

285
00:40:55.680 --> 00:41:06.180
Jerry Michalski: Which was all very funny, because yesterday afternoon, I had my first psilocybin journey ever, which, in fact, for me, is my first trip ever. I've never done drugs.

286
00:41:06.540 --> 00:41:22.729
Jerry Michalski: And a friend of mine in the neighborhood posted me at his place. It was incredibly safe and lovely. He's completely trustworthy, just, it was great, and I had a phenomenal experience. I felt connected to, like, the mycelial networks of the world.

287
00:41:22.730 --> 00:41:34.740
Jerry Michalski: He had, grown the shrooms himself. You can order kits, et cetera, et cetera. It works fine. He had tested it on himself, so he knew it was gonna be good. And…

288
00:41:34.740 --> 00:41:49.970
Jerry Michalski: it was really transformative. I had an incredible experience, and I'm still processing it. I tried during the experience to leave myself little dorms or candles or gateways to step back into the experience without doing the whole room thing. I don't know if that's going to work.

289
00:41:50.280 --> 00:41:52.260
Jerry Michalski: But I had… I had a…

290
00:41:53.860 --> 00:41:59.010
Jerry Michalski: Many different things happened. I felt like I was everything all at once-ish.

291
00:41:59.300 --> 00:42:07.989
Jerry Michalski: I now understand special effects, and, you know, in movies a lot better, because they're basically just, like, copying what happens to you when you see everything.

292
00:42:08.250 --> 00:42:17.520
Jerry Michalski: And, trying to, you know, map that into graphics and visuals. But, you know, the overwhelm you feel

293
00:42:17.710 --> 00:42:33.250
Jerry Michalski: in the moment is not easily replicable in a movie theater when you're sitting staring at a screen far away, and you know that there's no immediate consequences to you. Anyway, I feel like I'm transformed a bit, I don't know how, but it was thrilling.

294
00:42:33.850 --> 00:42:35.679
Jerry Michalski: And with that, I am complete.

295
00:42:43.150 --> 00:42:45.459
Mike Nelson: Well, Jerry, I'll step up and

296
00:42:46.120 --> 00:42:52.029
Mike Nelson: I had 1,700 different ways to go this week, but I'll start with where you left off.

297
00:42:52.440 --> 00:42:57.549
Mike Nelson: It is gratifying to know that I am not the only person

298
00:42:57.720 --> 00:43:05.410
Mike Nelson: from the West Coast, who has never done a trip of any type. I,

299
00:43:05.610 --> 00:43:14.000
Mike Nelson: don't even… haven't even smoked a cigarette, which is… goes to show you how I… I did a total bypass past adolescence.

300
00:43:15.340 --> 00:43:21.810
Mike Nelson: when I joined the Clinton administration in January of 20… January 1993,

301
00:43:22.040 --> 00:43:26.910
Mike Nelson: We all had to get security clearances, and in my case, at a pretty high level.

302
00:43:27.060 --> 00:43:31.140
Mike Nelson: And, of course, they have the… People interviewing you.

303
00:43:31.340 --> 00:43:45.779
Mike Nelson: And because it was the Clinton administration, and we had a whole bunch of people in their late 20s and early 30s, the first question was, okay, how often have you done drugs, and are you still doing them? And I said, well, no, I've never done drugs.

304
00:43:45.940 --> 00:44:03.800
Mike Nelson: And they just refused to believe this, because not only were, you know, every other person they had interviewed had done drugs in college, and several of them were doing drugs currently, and actually had to go on some kind of, you know, withdrawal plan, and had to be

305
00:44:03.800 --> 00:44:07.380
Mike Nelson: Tested every couple weeks to make sure they were on it.

306
00:44:08.820 --> 00:44:16.800
Mike Nelson: I didn't know how much they didn't believe me until I had two random drug tests in the next 10 days.

307
00:44:18.660 --> 00:44:35.739
Mike Nelson: But apparently, you know, they chose to believe me, or maybe they just talked to somebody else who had worked with me when I was working with Gore in the Senate, and they were assured that, yeah, Mike Nelson, if there's anybody in the Clinton administration who hasn't done drugs, it's Mike Nelson.

308
00:44:36.330 --> 00:44:38.520
Mike Nelson: But anyway, I, I am…

309
00:44:39.270 --> 00:44:52.960
Mike Nelson: gonna riff on what you said about avoiding overstimulation, because the topic I was gonna lay out there is how I am feeling completely, absolutely, intellectually, politically.

310
00:44:53.330 --> 00:45:01.219
Mike Nelson: overstimulated. This is partly because I spent Saturday at TEDx Mid-Atlantic.

311
00:45:01.530 --> 00:45:17.810
Mike Nelson: And we had some very powerful talks by everyone from Joe Trippi to Michael McFaul, our former ambassador to Russia, to a rapper in DC who has sort of reinvented the business model.

312
00:45:18.730 --> 00:45:26.599
Mike Nelson: He signed up with a major studio, he was doing cool stuff with other cool rappers, and he was going all over the country.

313
00:45:26.730 --> 00:45:32.380
Mike Nelson: And he said, forget this, and he started Front Porch. And he just…

314
00:45:32.780 --> 00:45:38.740
Mike Nelson: Goes out on his front porch, and he gets guests to come by, and they do things, and the neighborhood loves it.

315
00:45:39.070 --> 00:45:54.460
Mike Nelson: And, not as lucrative, but a lot less exhausting. So, just one example of some of the wild ideas that came out. But I've also just been overstimulated by the politics of what's happened in the last 3 days.

316
00:45:54.920 --> 00:45:59.810
Mike Nelson: Here in Virginia, of course, we've gotten a lot more political advertising than most.

317
00:45:59.950 --> 00:46:06.010
Mike Nelson: You've all seen that one of my favorite former congresspeople,

318
00:46:06.150 --> 00:46:26.049
Mike Nelson: Abigail Spanberger got elected. We've known her personally for about 6 years, and she's very close with one of our best friends. So we're just delighted that a moral, decent, smart, kind person could… and a young person,

319
00:46:26.360 --> 00:46:35.440
Mike Nelson: become our new governor. Virginia's blessed. We have a term limit system. You only get to be governor for one four-year term.

320
00:46:35.640 --> 00:46:39.500
Mike Nelson: And as a result, there's a lot of focus on building

321
00:46:39.900 --> 00:46:56.010
Mike Nelson: talent, and being sure there's several choices. But we won the governorship, Lieutenant Governor, and Attorney General, which is going to be incredibly important, given all the lawsuits that we're going to have to file against the Trump administration.

322
00:46:56.380 --> 00:47:03.030
Mike Nelson: But most importantly, we went from having a one-person majority for the Democrats in the legislature

323
00:47:03.460 --> 00:47:06.479
Mike Nelson: To having a 14 seat.

324
00:47:07.510 --> 00:47:11.460
Mike Nelson: Majority. So it's, you know, like, 52% to…

325
00:47:11.720 --> 00:47:24.880
Mike Nelson: 61%, something like that. And around the country, we had other very impressive landslide victories. So, it's clear that people are fed up with what Mr. Trump has been doing.

326
00:47:25.440 --> 00:47:39.089
Mike Nelson: In some cases, they're also saying they're fed up with what the Democrats are doing, but they know that the Democrats are unable to get organized, and therefore unable to do as much damage as Trump and his minions.

327
00:47:41.610 --> 00:47:51.680
Mike Nelson: I'm also tracking a lot of technology developments around the world, and that's, again, overstimulating. There's just too much going on. The Chinese are…

328
00:47:52.130 --> 00:47:59.160
Mike Nelson: Pushing forward a digital identity system, which they're gonna try to get other Countries, or at least

329
00:47:59.350 --> 00:48:02.450
Mike Nelson: multinationals to use.

330
00:48:03.140 --> 00:48:11.230
Mike Nelson: Clearly, that's going to have surveillance built in, so you're not going to be able to go anonymously from service to service on the internet.

331
00:48:12.840 --> 00:48:22.320
Mike Nelson: the Internet Engineering Task Force was meeting this week, and one of the most lively debates was about a new

332
00:48:22.750 --> 00:48:33.889
Mike Nelson: authentication system for… Digital transactions, and also a tool that would allow a website to prevent…

333
00:48:34.080 --> 00:48:37.860
Mike Nelson: People from coming and scraping all the content on the website.

334
00:48:38.210 --> 00:48:51.470
Mike Nelson: And this led to a lot of interest in discussions about, you know, well, can McDonald's use this to make sure that Burger King can't go on its website and find out what its prices are, or what its sales are by…

335
00:48:51.650 --> 00:49:10.329
Mike Nelson: by store? I mean, it's really… it's… it's a good thing, because you do have these companies, these bots, that are scraping content and looking for a stray social security number, and they're just putting a huge demand on all these websites, including non-profit ones.

336
00:49:10.500 --> 00:49:21.579
Mike Nelson: So, but how do you stop a bad person from abusing the openness of the internet while allowing a researcher to span,

337
00:49:22.090 --> 00:49:28.769
Mike Nelson: 100,000 websites, and go search through terabytes of data.

338
00:49:30.150 --> 00:49:41.859
Mike Nelson: tough, tough, tough issues, and we're… we're having all these discussions, but they're all kind of below the radar, because we're all fixated on whether AI will destroy world civilization.

339
00:49:42.170 --> 00:49:46.859
Mike Nelson: So, I've been overstimulated, I don't know if I've stimulated you, but,

340
00:49:47.490 --> 00:49:53.419
Mike Nelson: I am intrigued by your experience and this promise of…

341
00:49:53.570 --> 00:50:01.499
Mike Nelson: finding a way to turn off the brain a little bit. I usually do that by going for long bike rides, which is my next,

342
00:50:01.720 --> 00:50:05.569
Mike Nelson: my next thing, and I have to say, we have had

343
00:50:05.840 --> 00:50:11.929
Mike Nelson: more than a week of spectacular weather. It is peak, peak, peak fall foliage here in Washington.

344
00:50:12.100 --> 00:50:17.040
Mike Nelson: And I'm trying to enjoy that. Being semi-retired, I can enjoy that.

345
00:50:17.820 --> 00:50:20.589
Mike Nelson: But, yeah, there's just… there's so much going on.

346
00:50:20.900 --> 00:50:31.070
Mike Nelson: And I'd urge you to check out the Carnegie Endowment website, some of the sessions that we've had over the last two weeks on everything from

347
00:50:31.330 --> 00:50:40.079
Mike Nelson: Information Ecology, a book by one of my new… by one of my colleagues, to a session I just listened to on the radical right.

348
00:50:40.420 --> 00:50:49.360
Mike Nelson: And the New World Order. I mean, it's just a lot of big thinking going on, a lot of change happening, which, of course, could mean…

349
00:50:49.750 --> 00:50:51.020
Mike Nelson: That we could have.

350
00:50:51.180 --> 00:50:52.400
Mike Nelson: an improvement.

351
00:50:53.630 --> 00:50:58.080
Mike Nelson: Nelson's law of life is that nothing Improves unless it changes.

352
00:50:59.180 --> 00:51:03.709
Mike Nelson: Murphy's corollary to Nelson's Law is that not every change is an improvement.

353
00:51:04.300 --> 00:51:05.849
Mike Nelson: But I'm gonna try to help.

354
00:51:05.990 --> 00:51:08.179
Mike Nelson: Push people in the direction of improvement.

355
00:51:08.280 --> 00:51:11.920
Mike Nelson: Thank you. Long, long, somewhat optimistic.

356
00:51:13.230 --> 00:51:14.180
Mike Nelson: Screed.

357
00:51:28.050 --> 00:51:33.529
Victoria (Spain): I really don't know what to say after all your tickings, because…

358
00:51:34.120 --> 00:51:37.520
Victoria (Spain): I have just been writing my novel.

359
00:51:37.690 --> 00:51:41.480
Victoria (Spain): Researching about how people seek certainty.

360
00:51:42.270 --> 00:51:49.110
Victoria (Spain): And attending a workshop, to which now I have a session, so I will have to leave early.

361
00:51:49.820 --> 00:51:53.779
Victoria (Spain): In which I am observing a curious thing.

362
00:51:53.960 --> 00:51:59.990
Victoria (Spain): If people doesn't have curiosity, even if they are adults, willingly.

363
00:52:02.110 --> 00:52:07.649
Victoria (Spain): engaged in a workshop. They say they are interested in learning something.

364
00:52:08.150 --> 00:52:19.580
Victoria (Spain): But in the end, when… in the community side, I post, some questions or some things to trigger conversations.

365
00:52:19.780 --> 00:52:21.620
Victoria (Spain): Nobody responds.

366
00:52:22.350 --> 00:52:23.970
Victoria (Spain): Nice!

367
00:52:25.150 --> 00:52:30.740
Victoria (Spain): a former, what's the name? Because sometimes…

368
00:52:31.200 --> 00:52:37.499
Victoria (Spain): I… I have been designing courses for companies and everything, so…

369
00:52:37.660 --> 00:52:50.399
Victoria (Spain): For me, it's incredible how curiosity is dropping in our world. So this is one of the things I'm also, like, taking notes about and trying to understand.

370
00:52:51.220 --> 00:52:59.070
Victoria (Spain): But I'm not as engaged as you are in all those great things and experiences.

371
00:52:59.600 --> 00:53:02.409
Victoria (Spain): So, that's my little check-in.

372
00:53:06.880 --> 00:53:10.150
Stacey Druss: Can I… can I use my check-in to answer, Victoria?

373
00:53:10.320 --> 00:53:10.930
Stacey Druss: since I didn'.

374
00:53:10.930 --> 00:53:12.060
Jerry Michalski: No.

375
00:53:12.250 --> 00:53:19.430
Jerry Michalski: Although Victoria's gonna drop off the call at the top of the hour, so she won't hear you if you do it later, so yes.

376
00:53:20.020 --> 00:53:22.490
Stacey Druss: Okay, thank you for rethinking.

377
00:53:22.490 --> 00:53:23.080
Jerry Michalski: Yeah.

378
00:53:23.390 --> 00:53:34.530
Stacey Druss: I would love to answer your question about how I approach certainty, if you're interested, because I've been… I'm really clear on how I do it.

379
00:53:35.010 --> 00:53:41.759
Stacey Druss: I think of every story as being… there's two truths and a lie in it.

380
00:53:42.070 --> 00:53:57.899
Stacey Druss: So I'm always… that helps me to keep an open mind, and I'm always trying to figure out which is the lie. So it's a constant process of, well, it could be that, or it could be this. And that's how I keep going every step of the way.

381
00:53:58.110 --> 00:54:00.180
Stacey Druss: So I just wanted to share that.

382
00:54:02.130 --> 00:54:03.080
Jerry Michalski: Thanks, Stacey.

383
00:54:04.590 --> 00:54:08.679
Jerry Michalski: We now return to our regular program, which is already in progress.

384
00:54:08.920 --> 00:54:11.159
Jerry Michalski: And we have a couple people who haven't checked in yet.

385
00:54:20.580 --> 00:54:26.060
Doug Breitbart: So I'll go next. So this is sort of…

386
00:54:26.860 --> 00:54:29.619
Doug Breitbart: Zooming way, way, way, way, way in.

387
00:54:30.920 --> 00:54:34.259
Doug Breitbart: And zooming way, way, way out at the same time.

388
00:54:34.640 --> 00:54:43.090
Doug Breitbart: So, at the tender age of 69, as an ADD person, my executive function is finally fully matured.

389
00:54:43.540 --> 00:54:47.200
Doug Breitbart: And, with that, I have found a…

390
00:54:47.400 --> 00:54:51.629
Doug Breitbart: I think, huge expansion of capacity, capability.

391
00:54:51.930 --> 00:54:56.640
Doug Breitbart: To grapple with things, and play with things, and do things, and use things.

392
00:54:57.150 --> 00:55:02.370
Doug Breitbart: That, you know, were opaque and impenetrable and unusable before.

393
00:55:05.010 --> 00:55:19.780
Doug Breitbart: So, one of the phenomena of my daily reality up until about a week ago was, 4 to 5 browser windows with 3 monitors, each one with 45 to 50 tabs.

394
00:55:20.200 --> 00:55:24.170
Doug Breitbart: And, because on one hand.

395
00:55:25.480 --> 00:55:31.739
Doug Breitbart: That was my way of… having things up.

396
00:55:31.930 --> 00:55:44.990
Doug Breitbart: That I didn't want to lose. Of course, when you have 250 tabs that you can't see because they're Microsoft small, you can't differentiate. It's not as if you can find anything that way either. But, emotionally.

397
00:55:45.330 --> 00:55:48.220
Doug Breitbart: They're there. They exist.

398
00:55:49.440 --> 00:55:55.429
Doug Breitbart: And… So, approaching your computer first thing in the morning with that.

399
00:55:55.970 --> 00:55:58.240
Doug Breitbart: Was always not fun.

400
00:55:58.770 --> 00:56:02.650
Doug Breitbart: it was always, like, this overwhelming crap. Like…

401
00:56:02.870 --> 00:56:06.720
Doug Breitbart: what do I do now? And…

402
00:56:07.610 --> 00:56:25.469
Doug Breitbart: And I was thinking about, you know, browser functionalities, and I saw something that sort of flashed that I hadn't really… I knew the capability had to be in there, but I didn't know what it was called, or where to find it, and then I noticed in a menu

403
00:56:25.590 --> 00:56:29.810
Doug Breitbart: Something called… Tab Groups.

404
00:56:31.840 --> 00:56:38.399
Doug Breitbart: And it was, like… Okay, I think, I think, this is my path to…

405
00:56:38.570 --> 00:56:48.339
Doug Breitbart: emancipation, to tab emancipation. And, and because of newly vested executive function, I was able to, like.

406
00:56:49.000 --> 00:56:55.629
Doug Breitbart: open it up and learn it and master it very, very quickly, and I was able to take those 250 tabs and

407
00:56:55.730 --> 00:57:14.969
Doug Breitbart: sort and bucket them into groups, and I was able to then close those groups, and then I was able to close the browser windows of those groups, and freed up my machine's resources. And all of a sudden, like, my computer… my poor little laptop that could, that I was asking, you know, mini-computer demands on.

408
00:57:15.080 --> 00:57:24.429
Doug Breitbart: was able to function again, and, like, things started to flow, and I could close all that stuff, and only have the one or two tab groups

409
00:57:24.550 --> 00:57:30.719
Doug Breitbart: Up that were relevant in that moment to actually do work and not be in overwhelm.

410
00:57:33.930 --> 00:57:36.490
Doug Breitbart: This is life-changing.

411
00:57:37.370 --> 00:57:48.839
Doug Breitbart: For someone like me, And… Pulling way, way, way, way, way out.

412
00:57:51.560 --> 00:58:07.020
Doug Breitbart: It goes under the heading in my… You know, working reality of… the… The most meaningful questions

413
00:58:07.800 --> 00:58:10.419
Doug Breitbart: For me, in terms of what am I…

414
00:58:10.660 --> 00:58:14.429
Doug Breitbart: devoting my attention to what am I engaging with and why?

415
00:58:15.320 --> 00:58:23.789
Doug Breitbart: is, Based on the belief that every human being

416
00:58:24.280 --> 00:58:27.000
Doug Breitbart: Has value to add and contribute.

417
00:58:31.310 --> 00:58:41.310
Doug Breitbart: And… The only meaningful operative question inquiry If you buy that, is…

418
00:58:42.270 --> 00:58:49.570
Doug Breitbart: What do each one of those individuals need in order to be enabled to contribute?

419
00:58:52.160 --> 00:58:57.810
Doug Breitbart: And then the third and closing inquiry is, Based on that.

420
00:58:58.370 --> 00:59:01.170
Doug Breitbart: How do we meet everyone's needs?

421
00:59:05.420 --> 00:59:09.910
Doug Breitbart: And… Pretty much everything else.

422
00:59:10.470 --> 00:59:17.550
Doug Breitbart: If it is not germane to those inquiries, is… substantially irrelevant.

423
00:59:21.720 --> 00:59:23.690
Doug Breitbart: So that's the zooming out.

424
00:59:24.990 --> 00:59:31.190
Doug Breitbart: So, I found what I needed, In tab groups?

425
00:59:31.370 --> 00:59:37.509
Doug Breitbart: It unleashed a flood tide of capacity, Of organizing, of…

426
00:59:37.760 --> 00:59:50.080
Doug Breitbart: moving into action of unleashing the hounds in all sorts of ways, and now I'm going to close with the immediate nexus that OGM represents in my world.

427
00:59:50.800 --> 01:00:03.269
Doug Breitbart: So, what's starting to happen is I'm touching a bunch of projects that a lot of you are familiar with, because they're projects of OGM people, or people that have dipped in and out of here.

428
01:00:03.520 --> 01:00:10.230
Doug Breitbart: And… They're starting… 2.

429
01:00:10.400 --> 01:00:11.680
Doug Breitbart: cross-connect.

430
01:00:13.200 --> 01:00:27.159
Doug Breitbart: via individuals and conversations. They're starting to coalesce, where, you know, two completely independent projects have managed to connect, and all of a sudden they're talking, and…

431
01:00:27.270 --> 01:00:34.750
Doug Breitbart: I have awareness of the one, and I have awareness of the other, and I have awareness of how they complement and fit into a larger context.

432
01:00:35.630 --> 01:00:37.940
Doug Breitbart: And…

433
01:00:39.390 --> 01:00:47.349
Doug Breitbart: I'd say 60% of them… there's a list of about 12. 60% of them all have OGM in common.

434
01:00:47.470 --> 01:00:52.499
Doug Breitbart: So… Jerry, a hat tip.

435
01:00:52.880 --> 01:00:58.730
Doug Breitbart: For… Being, you know, the energetic center of gravity.

436
01:00:59.120 --> 01:01:04.140
Doug Breitbart: Out of which, a lot of these things, for me.

437
01:01:04.410 --> 01:01:10.930
Doug Breitbart: We're connected with and met, and now coalesceable.

438
01:01:11.150 --> 01:01:13.609
Doug Breitbart: In some fashion that I have yet to…

439
01:01:13.970 --> 01:01:18.759
Doug Breitbart: No completely, but that I'm working on. So, anyway, with that, I'm complete.

440
01:01:45.370 --> 01:01:50.259
Jerry Michalski: Rick, I don't know if you're just listening in, or if you want to step in,

441
01:01:51.200 --> 01:01:54.529
Jerry Michalski: But I don't want to hold up our transition to conversation.

442
01:01:55.840 --> 01:01:58.059
Jerry Michalski: So I'm gonna pause a moment.

443
01:01:58.840 --> 01:02:01.599
Jerry Michalski: Am I missing anybody else? Did somebody else not go?

444
01:02:03.280 --> 01:02:04.450
Jerry Michalski: We're all set, good.

445
01:02:05.000 --> 01:02:07.190
Rick Botelho: I'll pass, Joey, go ahead.

446
01:02:07.420 --> 01:02:08.789
Jerry Michalski: Okay, thanks, Rick.

447
01:02:10.130 --> 01:02:13.450
Jerry Michalski: We are now officially in conversation mode.

448
01:02:13.940 --> 01:02:24.380
Jerry Michalski: Excellent. Mike took that cue instantaneously, and put the Carnegie calendar onto the chat, which I was going to ask you to do, so fabulous, thank you so much.

449
01:02:24.750 --> 01:02:31.340
Jerry Michalski: And, anybody, we've got, like, five, four hands up immediately. Okay, so Mike, you're first.

450
01:02:32.050 --> 01:02:35.240
Mike Nelson: Well, I not only acted immediately, I jumped the gun.

451
01:02:35.560 --> 01:02:44.840
Mike Nelson: with my hand, but one of the issues I didn't raise, and this is weighing on… weighing on me a great deal,

452
01:02:47.090 --> 01:02:50.169
Mike Nelson: Our library at Carnegie gets the Wall Street Journal.

453
01:02:50.350 --> 01:02:53.150
Mike Nelson: I used to subscribe to it.

454
01:02:53.940 --> 01:03:04.120
Mike Nelson: But the fact that it's owned by Murdoch makes it morally ambiguous to… Pay, support his profits.

455
01:03:04.250 --> 01:03:10.399
Mike Nelson: But I do like to support the journalists, and I do try to publicize the great articles that I read.

456
01:03:12.230 --> 01:03:15.560
Mike Nelson: From the hard copy that we have at our library.

457
01:03:16.480 --> 01:03:24.110
Mike Nelson: Every day for the last 3 weeks, there's been at least one article along the lines of, oh my god, it's an AI bubble!

458
01:03:24.460 --> 01:03:28.009
Mike Nelson: And different versions on why it is.

459
01:03:28.340 --> 01:03:39.750
Mike Nelson: It's been clear to me for at least 2 years that we're on this track. It's so reminiscent of what happened with, Time Warner and it being bought by AOL.

460
01:03:39.990 --> 01:03:47.460
Mike Nelson: I mean, OpenAI is the AOL of our day, has a completely overinflated stock price, and

461
01:03:48.100 --> 01:03:55.560
Mike Nelson: Altman's doing exactly the right thing, and using it to buy all sorts of, do all sorts of deals.

462
01:03:56.370 --> 01:03:59.330
Mike Nelson: Which are gonna not look very good in a year or two.

463
01:04:00.060 --> 01:04:07.690
Mike Nelson: But… This bubble is… Going to deflate, either

464
01:04:08.350 --> 01:04:18.359
Mike Nelson: in a few weeks, or in a few years, it's clear we're not going to maintain the kind of growth that we've seen from NVIDIA and some of these other companies.

465
01:04:18.810 --> 01:04:26.879
Mike Nelson: And the questions I have for anybody here who has the courage to give financial advice

466
01:04:27.230 --> 01:04:34.889
Mike Nelson: Is it going to drag down just the AI-related companies? Or is it going to drag down the entire tech sector?

467
01:04:35.570 --> 01:04:40.799
Mike Nelson: Or is it gonna drag down the entire market? One of the articles in the Wall Street Journal was.

468
01:04:42.530 --> 01:04:44.850
Mike Nelson: The U.S. economy is now AI.

469
01:04:45.260 --> 01:04:51.950
Mike Nelson: It made the point that all of the growth that we've seen in more than 12 months.

470
01:04:52.740 --> 01:04:59.069
Mike Nelson: is because of all the investment that's going into these data centers. I mean, if you just take that out.

471
01:04:59.560 --> 01:05:01.730
Mike Nelson: The economy's been flat.

472
01:05:02.040 --> 01:05:17.370
Mike Nelson: And Trump is doing everything he can to make it go down. So, my question, because I was very lucky, I was employing number 130 at Cloudflare before it went IPO,

473
01:05:18.160 --> 01:05:22.660
Mike Nelson: A good chunk of my retirement portfolio is still in Cloudflare stock.

474
01:05:23.010 --> 01:05:28.280
Mike Nelson: And other parts of my portfolio are in IBM stock.

475
01:05:28.640 --> 01:05:32.909
Mike Nelson: And other tech companies, because I tend to invest in what I know.

476
01:05:33.520 --> 01:05:36.919
Mike Nelson: So I'm just curious, I mean, how many of you are thinking of

477
01:05:37.370 --> 01:05:42.620
Mike Nelson: Pulling out the profits from your tech investments.

478
01:05:43.430 --> 01:05:46.350
Mike Nelson: That's… An easy question.

479
01:05:46.800 --> 01:05:51.269
Mike Nelson: And if you are, what in the hell do you put the…

480
01:05:52.130 --> 01:05:55.470
Mike Nelson: proceeds into. I mean, I… I just…

481
01:05:56.310 --> 01:06:09.389
Mike Nelson: I have no clue what a good strategy here is. I mean, the reason gold is up by a factor of 3 is a lot of other people are confused, too, when they figure, well, okay, I'll put it in something that will somehow have value someday.

482
01:06:10.540 --> 01:06:23.170
Mike Nelson: We've never… I don't… I don't remember a discussion like this, and maybe that's because people don't like to talk finances, and people have… or because people have very good financial advisors. I have a very good financial advisor, but…

483
01:06:24.380 --> 01:06:31.100
Mike Nelson: you know, he… and he's got, you know, some suggestions, but I'm just curious how many of you are…

484
01:06:31.270 --> 01:06:34.619
Mike Nelson: Thinking of strategies for the upcoming bubble.

485
01:06:36.970 --> 01:06:49.759
Jerry Michalski: Thanks, Mike. And we're not a financially-oriented group, or that necessarily expert in this. There's probably different depths of expertise in the group on that, but it might make a good topic for a call. Like, is there a bubble? What is the bubble? And…

486
01:06:50.280 --> 01:07:04.529
Jerry Michalski: I haven't had a chance to read all the too many articles about, oh my god, there's a bubble, but a couple of them are trying to unpack. There could be several different bubbles going on, and here's how they fit and relate, which I find really fascinating, because

487
01:07:04.730 --> 01:07:23.470
Jerry Michalski: I think there's clearly a bubble in a piece of it. There's, like, this frenzy to do AGI, but there's other parts that are not so bubbly, and I don't know, it's a really good, juicy, complex issue. And I haven't even addressed the, you know, do you divest all tech? The problem with previous bubbles is

488
01:07:24.310 --> 01:07:29.059
Jerry Michalski: If you heeded the first warnings of, oh my god, we're in a bubble, you would miss the actual big run-up.

489
01:07:30.150 --> 01:07:44.749
Jerry Michalski: And you would sell too early, and then if you miss the fall, because usually it's the first couple people who get off the merry-go-round who profit a lot, then you lose everything. So timing is everything in these things, and it's a little nutty, so…

490
01:07:44.750 --> 01:07:48.420
Mike Nelson: Yeah, well, this was a… this was meant to be a two-minute discussion.

491
01:07:48.420 --> 01:07:52.959
Jerry Michalski: Yeah. And it really is kind of, you know, put your hand up if you think there is a bubble.

492
01:07:52.960 --> 01:07:56.050
Mike Nelson: And then put your other hand up if you're gonna try to…

493
01:07:56.700 --> 01:08:04.950
Mike Nelson: Dramatically reduce your exposure in the tech… not dramatically, but significantly reduce your exposure in the tech sector.

494
01:08:04.950 --> 01:08:05.490
Jerry Michalski: Literally.

495
01:08:05.490 --> 01:08:11.050
Mike Nelson: If you have a one-sentence answer to, where would you put your money, you can put that in the chat.

496
01:08:11.050 --> 01:08:13.410
Jerry Michalski: Let's do that. Raise your hand if you think there's a bubble.

497
01:08:14.940 --> 01:08:23.930
Mike Nelson: I think there's 3 bubbles. There's one in cloud computing, one in AI chips, and then one in cryptocurrency, which is… that's a whole other topic.

498
01:08:24.310 --> 01:08:30.399
Jerry Michalski: Cool. And then raise your hand if you're seriously thinking about maybe disinvesting, changing your investment strategy.

499
01:08:33.430 --> 01:08:38.569
Mike Nelson: Cool. Three and a half, that's, that's… thank you, that's very helpful for…

500
01:08:38.770 --> 01:08:41.809
Mike Nelson: Reducing my anxiety a little bit.

501
01:08:41.819 --> 01:08:44.059
Jerry Michalski: And it's a great question, Mike, thank you.

502
01:08:44.060 --> 01:08:46.779
Mike Nelson: And is pork bellies the place I should put all my money?

503
01:08:46.939 --> 01:08:58.739
Jerry Michalski: No, try rye Coins. I put a Wikipedia link in the chat. It's the big stones they used to use, as currency. I think you need to find one of those and, like, roll it around the living room as your asset.

504
01:08:58.740 --> 01:09:00.760
Mike Nelson: I thought that was the island of Yap.

505
01:09:01.220 --> 01:09:02.259
Jerry Michalski: That could well be.

506
01:09:02.260 --> 01:09:03.430
Pete Kaminski: It's the future.

507
01:09:03.630 --> 01:09:05.179
Jerry Michalski: There's a way, there's.

508
01:09:05.189 --> 01:09:07.059
Doug Breitbart: There's also…

509
01:09:07.270 --> 01:09:08.110
Jerry Michalski: Totally, yeah.

510
01:09:08.609 --> 01:09:13.059
Doug Breitbart: There's also the guy, Mike, behind the big short in the real estate crash.

511
01:09:13.309 --> 01:09:19.479
Doug Breitbart: Who has a fund? Who is, set up to short the tech sector?

512
01:09:19.629 --> 01:09:25.759
Doug Breitbart: like, just, if you want to go straight Contra, like, he'd be happy to take your money.

513
01:09:25.760 --> 01:09:27.199
Jerry Michalski: Is it Michael Burry, or…

514
01:09:27.200 --> 01:09:28.909
Mike Nelson: Michael Burry, if you haven't seen…

515
01:09:28.910 --> 01:09:31.220
Jerry Michalski: That is him. Shit, wow. Okay.

516
01:09:31.229 --> 01:09:32.359
Mike Nelson: I've seen the interview.

517
01:09:32.359 --> 01:09:33.459
Doug Breitbart: back.

518
01:09:33.590 --> 01:09:42.859
Mike Nelson: Wow! I haven't seen the interview with the CEO of Palantir. Karp was interviewed about the fact that Burry had shorted Palantir. A lot.

519
01:09:43.339 --> 01:09:44.329
Doug Breitbart: And then Lydia.

520
01:09:44.330 --> 01:09:49.030
Mike Nelson: looked panicked. It's like, oh, this is really bad, oh, this is not fair, oh.

521
01:09:50.529 --> 01:09:53.050
Mike Nelson: And his stock price went down 12%, so…

522
01:09:53.050 --> 01:09:53.540
Doug Breitbart: Dear.

523
01:09:53.540 --> 01:09:53.930
Jerry Michalski: I've got…

524
01:09:53.930 --> 01:09:55.669
Doug Breitbart: You're in the headlights.

525
01:09:55.670 --> 01:10:06.990
Jerry Michalski: So I've got a New Yorker article, that's titled, Michael Burry, Real Life Market Genius from the Big Short, Thinks Another Financial Crisis is Looming, dated, 2016.

526
01:10:08.540 --> 01:10:11.360
Jerry Michalski: So he would have missed… he would have missed a decade of run-up.

527
01:10:11.830 --> 01:10:12.450
Mike Nelson: Yep.

528
01:10:13.010 --> 01:10:17.539
Jerry Michalski: And if you… I don't know when you run out of money from shorting, but you do at some point.

529
01:10:18.810 --> 01:10:24.550
Jerry Michalski: That was part of the problem with The Big Short. I… and by the way, the Big Short movie and book are among my favorite

530
01:10:24.660 --> 01:10:29.389
Jerry Michalski: bits of media like that. They're just so insightful. Gil, Gil, please.

531
01:10:29.600 --> 01:10:37.549
Gil Friend • Sustainability OG • CxO Coach: And the guy who wrote the Big Short has done a series of books of similar depth and insight. Really terrific. Michael somebody?

532
01:10:37.550 --> 01:10:38.200
Mike Nelson: Lewis.

533
01:10:38.200 --> 01:10:40.880
Gil Friend • Sustainability OG • CxO Coach: My brother lives here in Berkeley.

534
01:10:40.880 --> 01:10:42.330
Mike Nelson: He did Moneyball as well.

535
01:10:42.330 --> 01:10:49.520
Gil Friend • Sustainability OG • CxO Coach: Moneyball and a bunch of things. Yeah, brilliant, brilliant guy. I just first want to say, what a fine bunch of humans you all are.

536
01:10:50.020 --> 01:10:55.959
Gil Friend • Sustainability OG • CxO Coach: sitting call, just listening to the quick summaries from everybody. couple of things.

537
01:10:56.360 --> 01:11:01.520
Gil Friend • Sustainability OG • CxO Coach: Mike, I think I heard you say that when to get out is obvious, and obviously it's not obvious.

538
01:11:01.520 --> 01:11:01.969
Mike Nelson: Oh, I didn't.

539
01:11:02.220 --> 01:11:07.050
Gil Friend • Sustainability OG • CxO Coach: For the reason that Jerry said, because bubbles and FOMO are the kind of similar phenomena.

540
01:11:08.160 --> 01:11:17.660
Gil Friend • Sustainability OG • CxO Coach: You know, I mean, how can tech go higher, but you can't miss out on it going higher? Same with gold, same with Bitcoin, there's this weird psychological thing in there.

541
01:11:17.910 --> 01:11:21.930
Gil Friend • Sustainability OG • CxO Coach: golly, a couple of pieces here I want to tie together.

542
01:11:23.970 --> 01:11:32.519
Gil Friend • Sustainability OG • CxO Coach: I've mentioned Ben Hunt before from Epsilon Theory. He's now launched a new site called Panoptica.ai, which I encourage people to take a look at.

543
01:11:32.520 --> 01:11:55.980
Gil Friend • Sustainability OG • CxO Coach: Ben and his partner, Rusty Gwynn, have been tracking narratives globally. They've built AIs that scan, I don't know, 8… Alex, this is to your point about media scans, scanning 8,000 sites around the world on a regular basis and distilling the trends of what's being talked about. So they have multicolored, complex graphs showing the rise and fall of conversations about things like

544
01:11:55.980 --> 01:12:02.050
Gil Friend • Sustainability OG • CxO Coach: bubbles, or bubbles in this or that, recession, interest rates, and so forth. It's a fascinating meta-view.

545
01:12:02.120 --> 01:12:13.720
Gil Friend • Sustainability OG • CxO Coach: on the stories that power the world. Ben also runs an asset management company called Second Foundation Partners. He is a confessed longtime short.

546
01:12:14.220 --> 01:12:21.840
Gil Friend • Sustainability OG • CxO Coach: Short of the real estate bubble back in 2008, and so, you know, can somebody be a short for a long time? Maybe so.

547
01:12:22.030 --> 01:12:36.299
Gil Friend • Sustainability OG • CxO Coach: The Panoptica site also has got a wealth of articles and analysis and so forth, really fascinating stuff, much of it very OGM-y, so have a look at that. On the,

548
01:12:37.330 --> 01:12:55.710
Gil Friend • Sustainability OG • CxO Coach: On the filters and the new scan, just to note that not just Panoptica, but our very own Marshall Kirkpatrick has been building a system that does that, focused around sustainability and regeneration. Doing scans of trends, building products for companies around that, some very interesting stuff in the workbench there.

549
01:12:55.710 --> 01:12:59.210
Gil Friend • Sustainability OG • CxO Coach: Marshall's rarely on these calls, but he's up to some interesting stuff.

550
01:12:59.240 --> 01:13:02.330
Gil Friend • Sustainability OG • CxO Coach: on that. And,

551
01:13:02.880 --> 01:13:17.660
Gil Friend • Sustainability OG • CxO Coach: The other thing I want to say about AI is that I was, spent Tuesday at Capgemini, the French, consultancy. I've been working with them on their net positive… excuse me… net positive projects, which the company appears to have made a major commitment to.

552
01:13:18.240 --> 01:13:34.770
Gil Friend • Sustainability OG • CxO Coach: We've just gone through the second year of training. Each year, like, a cohort of 100, Hispanic-orient… Hispanic background, managers and executives in sustainability and net positive. Fascinating process,

553
01:13:35.000 --> 01:13:36.850
Gil Friend • Sustainability OG • CxO Coach: Planting seeds in…

554
01:13:36.990 --> 01:13:43.189
Gil Friend • Sustainability OG • CxO Coach: hundreds of companies about that. But Tuesday, we had a presentation from a guy named Andres Sjoborn.

555
01:13:43.270 --> 01:14:02.230
Gil Friend • Sustainability OG • CxO Coach: who was, I think, their lead or one of their leads on AI, and it was a fascinating hour-long overview of the field, and how he sees it, and the kind of integrations that companies are doing in applications of AI. I've asked him for the slides, I don't know if they're publicly available. If I do get them, I'll share them here.

556
01:14:02.230 --> 01:14:06.940
Gil Friend • Sustainability OG • CxO Coach: But might be somebody to track. And,

557
01:14:08.940 --> 01:14:25.500
Gil Friend • Sustainability OG • CxO Coach: One other thing here… two other things here. David, really intrigued to hear your discussion about growth and where… and your evolution around the conversation that you're in. You and I have intersected on a little bit of, and I'd love to talk with you more about that.

558
01:14:26.220 --> 01:14:34.690
Gil Friend • Sustainability OG • CxO Coach: Because I think as we go into… when we look at the biological roots of the world that we live in,

559
01:14:36.500 --> 01:14:42.309
Gil Friend • Sustainability OG • CxO Coach: A lot of Mahima stories don't hold up very well. I'll just leave it at that for now and talk more about that another time.

560
01:14:42.510 --> 01:14:50.279
Gil Friend • Sustainability OG • CxO Coach: Even… well, I'll say one other thing. Even, like, our current widespread irateness about what we call corruption.

561
01:14:51.080 --> 01:15:00.349
Gil Friend • Sustainability OG • CxO Coach: Which maybe is not that different from predator-prey relationships and parasite relationships and the things that living systems do, just as kind of part of the game.

562
01:15:00.740 --> 01:15:17.160
Gil Friend • Sustainability OG • CxO Coach: You know, there's a striving for advantage, and there's a countervailing opposition to the advantage, and that may just be what it is. And so, not to say we shouldn't resist corruption, but we maybe should not be quite as surprised and outraged and hopeful of a world without it.

563
01:15:17.750 --> 01:15:31.759
Gil Friend • Sustainability OG • CxO Coach: Given that. And the other thing I just wanted to comment on from the top of the call, Scott, I was fascinated by what you talked about, about the, you know, Kevin McCloud and being the most widespread. Did we lose Scott?

564
01:15:31.880 --> 01:15:33.110
Gil Friend • Sustainability OG • CxO Coach: Oh gosh, too bad.

565
01:15:33.110 --> 01:15:34.540
Jerry Michalski: No, Scott's still on the call.

566
01:15:34.540 --> 01:15:40.700
Gil Friend • Sustainability OG • CxO Coach: Okay. He's muted and no video, but… He's hidden, I'm not seeing the video, okay. Yep. So,

567
01:15:41.580 --> 01:15:48.599
Gil Friend • Sustainability OG • CxO Coach: It just echoed for me the line from whoever it was, somebody here knows about it. It's amazing what you can get done if you're not concerned with getting credit for it.

568
01:15:48.790 --> 01:15:49.420
Jerry Michalski: Yep.

569
01:15:49.450 --> 01:16:07.109
Gil Friend • Sustainability OG • CxO Coach: And so I was thinking about… about… Kevin McCloud and Scott and Scott's deck and other decks that I've seen other people do, and the thought about, about, you know, revolutionary Legos. Like, building… building the pieces of the puzzle and just…

570
01:16:07.200 --> 01:16:26.149
Gil Friend • Sustainability OG • CxO Coach: throwing them out there in a way where they self-propagate like apparently Kevin McCloud's been able to do, rather than trying to build complete strategies, but to feed the development of multiple strategies around the world. Just an intriguing idea, that resonated very powerfully from what Scott said. So, Scott, thank you for that.

571
01:16:26.500 --> 01:16:28.949
Gil Friend • Sustainability OG • CxO Coach: And I'll be chewing on that for a while, I'm complete.

572
01:16:30.170 --> 01:16:32.379
Jerry Michalski: Thank you. There's a bunch of stuff. Poss, please.

573
01:16:35.330 --> 01:16:50.959
Klaus Mager: Yeah, I mean, it's also AI-related, but I've been running into a really interesting phenomenon where every part of the economy is working to integrate AI into their flow.

574
01:16:51.040 --> 01:16:56.429
Klaus Mager: But when you are talking with NGOs and, and,

575
01:16:56.660 --> 01:17:01.300
Klaus Mager: You know, particularly in my food and agriculture space.

576
01:17:01.590 --> 01:17:07.639
Klaus Mager: there is almost an emotional rejection of wanting to deal with AI.

577
01:17:07.670 --> 01:17:26.790
Klaus Mager: And the reasons given is it uses too much electricity, is the first thing you hear, it uses water. But there is that… there has been… and I'm now thinking, you know, how can something like this penetrate so effectively into an entire sector?

578
01:17:26.790 --> 01:17:33.620
Klaus Mager: Yeah, because the, the, the, these opinions are being held pretty universally.

579
01:17:33.630 --> 01:17:37.010
Klaus Mager: And it disempowers non-profit groups

580
01:17:37.140 --> 01:17:52.200
Klaus Mager: from engaging with this technology, which is so powerful. Now, I mean, I can do a search, I mean, David just asked a question about the Bill Gates Foundation. It took me, like, 5 seconds to find this.

581
01:17:52.200 --> 01:17:58.380
Klaus Mager: Not because my AI is customized into these particular topics.

582
01:17:58.870 --> 01:18:07.220
Klaus Mager: So we now have a relationship with the Schumacher Institute, and George Paul is taking it on.

583
01:18:07.300 --> 01:18:22.419
Klaus Mager: to work with the open food, the open global food system. It's a non-profit group, operates in 20 countries, and they're working with food hubs now, and they're in this small CSA, farmers markets.

584
01:18:22.420 --> 01:18:27.930
Klaus Mager: type of environment, and we are going to track them into the AI world.

585
01:18:27.970 --> 01:18:38.129
Klaus Mager: And with two objectives. One is to get them into wholesale markets, and the other one is to get farmers paid for environmental services.

586
01:18:38.200 --> 01:18:48.449
Klaus Mager: You know, and I already know it can be done. I mean, I've done the basic research with the AI to figure this out. And here in Oregon, the Oregon…

587
01:18:48.450 --> 01:19:02.279
Klaus Mager: Community Foundation, which is a $20 billion foundation, has also agreed to work with me. And so I have a meeting next week, where the director, you know, wants to

588
01:19:02.280 --> 01:19:12.370
Klaus Mager: set up a conversation with a catering organization, the hospital co-catering director, and see how could we do this. And so, just to

589
01:19:12.470 --> 01:19:29.140
Klaus Mager: to make this incredible technology available, you know, to support these industries, and to build a bridge. I mean, you really have to see yourself as a translation service, you know, because all the knowledge is already out there.

590
01:19:29.270 --> 01:19:42.910
Klaus Mager: But to track these groups in, and I must say, the effectiveness with which NGOs have been neutralized from using AI and working with it is just stunning.

591
01:19:42.970 --> 01:20:01.959
Klaus Mager: You know, I don't know who came up with this idea of emphasizing this discussion around, don't use it, it's bad for the environment, but it sure worked, and it's taking, you know, a lot of effort to overcome that rejection.

592
01:20:04.650 --> 01:20:06.710
Jerry Michalski: Thanks, Klaus. Rick, please.

593
01:20:07.790 --> 01:20:15.159
Rick Botelho: I'll maybe bounce off of that, because I've had conversations with academics who have this anti-AI mindset.

594
01:20:15.280 --> 01:20:28.309
Rick Botelho: For the environmental, et cetera, et cetera reasons. I just had one with a colleague of mine who's a professor of global studies at UNC about this very issue. He talked about that last breakfast coffee meeting.

595
01:20:28.510 --> 01:20:35.200
Rick Botelho: And one of the things I'm absolutely fascinated with, and I'm trying to develop my own skills.

596
01:20:35.370 --> 01:20:37.920
Rick Botelho: Is the whole notion of co-intelligence.

597
01:20:38.050 --> 01:20:44.350
Rick Botelho: not just from an individual level, but from a collective perspective, which, you know, touches on Klaus's point.

598
01:20:44.660 --> 01:20:47.520
Rick Botelho: Because what you're able to do and achieve

599
01:20:47.660 --> 01:20:51.990
Rick Botelho: is mind-blowing. I'm just blown away by what you can do.

600
01:20:52.170 --> 01:20:58.450
Rick Botelho: But what I've been working on for some time, and I've dropped it here and there in different places.

601
01:20:58.610 --> 01:21:08.520
Rick Botelho: is trying… and I mentioned this phrase before, and I remember Mike saying, keep it simple, about, HAN, which is, you know, humanist guided.

602
01:21:09.010 --> 01:21:10.460
Rick Botelho: AI enabled.

603
01:21:11.300 --> 01:21:18.120
Rick Botelho: Emancipatory neoloney. And I've made some little vignettes explaining that, what Hen means.

604
01:21:18.220 --> 01:21:23.049
Rick Botelho: And, you know, we live in this incredible culture of indoctrination.

605
01:21:23.370 --> 01:21:25.310
Rick Botelho: And it's so pervasive.

606
01:21:25.630 --> 01:21:41.540
Rick Botelho: that, the point that you put in your little chat there, Mike, about curiosity, I mean, to me, curiosity is the most fundamental skill and inquiry skills that we need to have to be able to, mitigate against

607
01:21:41.560 --> 01:21:47.089
Rick Botelho: The pervasive forces of neoliberalism, reductionist, indoctrination, yada.

608
01:21:47.430 --> 01:21:53.659
Rick Botelho: And I'll just share something which is still work in progress, and it's about how to create

609
01:21:53.820 --> 01:22:06.910
Rick Botelho: emancipatory learning communities, and developing learning assets that stimulates people's curiosities and inquiries to ask even better questions and more complex questions, or compound questions, I should say.

610
01:22:06.910 --> 01:22:26.759
Rick Botelho: And that's the beauty about AI, is that you can… I mean, it… you know, we have a reductionist mindset to questioning, and we frame it, and we… whatever. Well, in a compound philosophical question, you can put everything in the kitchen sink in there and see what comes out, and then you can go back and see patterns that you would, or even come across a new concept you've never heard of before, and we've all done that sort of stuff.

611
01:22:26.990 --> 01:22:39.550
Rick Botelho: And the question is, how can we create, you know, an environment where people can learn how to become open-minded, truth-seeking, virtuous freethinkers, so that we can mitigate against all the propaganda that we're

612
01:22:39.600 --> 01:22:43.540
Rick Botelho: bombarded with on a regular basis. Anyway, I'll share something in the chat.

613
01:22:43.580 --> 01:23:03.249
Rick Botelho: for those who are curious about it. It's still a work in progress, and I still feel like I've got some ways to go in terms of how to create learning communities around this, which are, designed to be more, group-centered and based on principles of sociocracy and whatever. So I'll put it in the chat, so…

614
01:23:03.710 --> 01:23:08.390
Rick Botelho: I just… I think the opportunities for co-intelligence is just…

615
01:23:08.700 --> 01:23:11.009
Rick Botelho: You know, Klaus, I concur with you.

616
01:23:12.040 --> 01:23:17.189
Rick Botelho: let's up our skills. How can we… how can we learn from each other more effectively about how to do that? So…

617
01:23:17.730 --> 01:23:18.580
Rick Botelho: Thanks.

618
01:23:19.550 --> 01:23:20.350
Jerry Michalski: Thanks, Rick.

619
01:23:21.040 --> 01:23:22.050
Jerry Michalski: Alex.

620
01:23:24.750 --> 01:23:28.639
Alex Kladitis: Right, Mike, I'll make it easy for you, for your investment conundrum.

621
01:23:29.140 --> 01:23:34.129
Alex Kladitis: And it goes like this. I'm the world's worst possible investor.

622
01:23:34.450 --> 01:23:40.140
Alex Kladitis: I have always sold when I should have bought, and bought when I should have sold.

623
01:23:40.410 --> 01:23:48.329
Alex Kladitis: So, a few years back, and quite a few years back, I made the really great decision to keep 40% of my money in cash.

624
01:23:48.530 --> 01:23:49.580
Alex Kladitis: Pure cash.

625
01:23:49.700 --> 01:23:51.969
Alex Kladitis: Because, obviously, the market's gonna crash.

626
01:23:53.120 --> 01:23:54.329
Alex Kladitis: And it's still there.

627
01:23:54.680 --> 01:23:57.430
Alex Kladitis: And I haven't yet bought into the market.

628
01:23:57.870 --> 01:24:00.829
Alex Kladitis: But as soon as I'm tempted and I buy the market.

629
01:24:01.170 --> 01:24:05.889
Alex Kladitis: I will actually let all of you know that I've done it, because that is a time to get out of it.

630
01:24:06.520 --> 01:24:09.450
Alex Kladitis: But so far, I have not done that.

631
01:24:09.450 --> 01:24:15.520
Mike Nelson: The critical question is, is it cash in dollars, or cash in Swiss francs?

632
01:24:17.150 --> 01:24:22.550
Alex Kladitis: It's cash. It's cash that's losing its value wherever you are.

633
01:24:22.550 --> 01:24:25.840
Mike Nelson: Except if you'd put it in Swiss francs, we were just…

634
01:24:25.840 --> 01:24:27.950
Alex Kladitis: Yes, no, I didn't put it there, I didn't put it there.

635
01:24:27.950 --> 01:24:37.210
Mike Nelson: Thank you, Alex. This is what… the reason I ask is I am in the same category you are. I have, you know, I have sold… sold shares that, you know, went on to.

636
01:24:37.210 --> 01:24:37.620
Alex Kladitis: Yeah.

637
01:24:37.620 --> 01:24:39.560
Mike Nelson: Value by a factor of 6.

638
01:24:40.330 --> 01:24:50.820
Alex Kladitis: There's a few nightmare stories behind that, but so that was… I just wanted to point that out. The other thing is, Pete, I'm interested in the sheaf methodology. I'd love to get in touch with you.

639
01:24:51.880 --> 01:24:58.859
Alex Kladitis: And one more, someone said something… oh, and Doug, a long time ago, a year ago, we spoke about a game you wanted?

640
01:24:59.680 --> 01:25:01.299
Alex Kladitis: Do you remember that conversation?

641
01:25:01.600 --> 01:25:04.369
Alex Kladitis: Let's touch… let's get together, because…

642
01:25:04.600 --> 01:25:10.069
Alex Kladitis: I can show you how to do it yourself, given all these AI things that we've spoken about.

643
01:25:10.560 --> 01:25:13.959
Alex Kladitis: You won't even have to… you don't even need me, just… I just need to show you.

644
01:25:14.230 --> 01:25:16.560
Alex Kladitis: But Pete can, and everybody can, really.

645
01:25:17.380 --> 01:25:18.110
Alex Kladitis: That's it.

646
01:25:18.110 --> 01:25:20.529
Doug Breitbart: Would love, would love that, thank you.

647
01:25:21.340 --> 01:25:22.170
Jerry Michalski: Cool, cool, cool.

648
01:25:22.840 --> 01:25:25.039
Jerry Michalski: Thank you much, Pete!

649
01:25:26.330 --> 01:25:49.109
Pete Kaminski: Real quick, I put some of it in the chat, and I feel like I didn't express it very well, but Mike, your question is really interesting, about whether we're in a bubble. I think we've been in a bubble since about the PC era, more or less. I don't… it's one big tech bubble inflation thing going on. It goes up and down, but the whole thing is going big, and changing the world, and changing us.

650
01:25:49.110 --> 01:26:00.329
Pete Kaminski: Ai feels like some kind of endgame. Either it's gonna usher in, you know, amazing productivity and social justice and everything's gonna be wonderful, or

651
01:26:00.330 --> 01:26:11.309
Pete Kaminski: the whole thing's gonna crash hard because we've turned all… all matter into GPUs, or something, right? I think what's really gonna happen is some weird thing in the middle.

652
01:26:11.330 --> 01:26:12.950
Pete Kaminski: But it feels…

653
01:26:14.220 --> 01:26:26.559
Pete Kaminski: it's hard to say this out loud to professional people who care, but it feels like we're kind of post, you know, what do I do with the market? Do I do cash? Do I do gold? Do I do Bitcoin? You know, all of that.

654
01:26:26.720 --> 01:26:34.920
Pete Kaminski: all of that seems like the wrong questions at this point. So, when… when we said, are we in a bubble, and I go like this, it's like, we're… we're…

655
01:26:35.020 --> 01:26:35.960
Pete Kaminski: passed…

656
01:26:36.090 --> 01:26:45.959
Pete Kaminski: where we've ever even thought about bubbles, where they go, you know? It's… we're in weird territory here. So I think that's kind of what to… what to discuss.

657
01:26:46.620 --> 01:27:05.340
Jerry Michalski: we're sort of in too-big-to-fail territory, and I've seen a couple articles there, too, which is like, this has eaten so much of the economy that this is called the government put, or something like that, where you get to a scale where you can just keep plowing ahead confidently, because the government's gonna backstop you somehow.

658
01:27:05.540 --> 01:27:06.450
Jerry Michalski: And you'll wind up.

659
01:27:06.450 --> 01:27:07.160
Pete Kaminski: Okay. Now…

660
01:27:07.160 --> 01:27:30.639
Pete Kaminski: And now we put AI instead of, you know, AI is going to backstop you. Will it? Won't it? Maybe that will be great, but we won't be able to eat because, as Klaus has told us, you know, we need to figure out fast how to apply AI to actually growing things and delivering food, and making soil regenerative, and all that stuff. So, it's a weird place.

661
01:27:31.390 --> 01:27:32.330
Jerry Michalski: Absolutely.

662
01:27:32.870 --> 01:27:38.610
Jerry Michalski: Thank you. Gil, you may have the last word? Actually, no, Doug's gonna have the last word, so go ahead, Gil.

663
01:27:38.610 --> 01:27:41.509
Gil Friend • Sustainability OG • CxO Coach: Well, let me try to be fast, so there's room for Doug. So,

664
01:27:41.630 --> 01:27:55.480
Gil Friend • Sustainability OG • CxO Coach: Golly, so I had a thing to say, but Pete, what you just said, it's not like we need AI to figure out how to grow food regeneratively, we actually know how to do that. And many of our problems are not technology problems, they're social organization and ownership problems.

665
01:27:55.650 --> 01:27:59.660
Gil Friend • Sustainability OG • CxO Coach: But that's a conversation for another time.

666
01:28:00.730 --> 01:28:07.679
Gil Friend • Sustainability OG • CxO Coach: Ai and bubbles… I mean, it's not one big bubble, it's a series of bubbles. Some of them have stabilized, some of them have not.

667
01:28:07.740 --> 01:28:23.979
Gil Friend • Sustainability OG • CxO Coach: The backstopping, you know, in 2008, the government bailed out some of the banks and not others. This is the whole too-big-to-fail game, and we're in another one of those now. Jerry, to your point, the numbers I hear from Ben Hunt are that,

668
01:28:24.170 --> 01:28:32.590
Gil Friend • Sustainability OG • CxO Coach: The tech sector, which is about 5% of GDP, is now pouring more money into the economy than the consumer sector, which is about 30% of the GDP.

669
01:28:32.900 --> 01:28:36.030
Gil Friend • Sustainability OG • CxO Coach: And that's the only thing keeping the economy afloat at this point.

670
01:28:36.670 --> 01:28:38.260
Gil Friend • Sustainability OG • CxO Coach: And so, yeah, will something…

671
01:28:38.260 --> 01:28:43.199
Jerry Michalski: masking the effects of tariffs, or whatever else might be happening. It's totally swamping all that other stuff.

672
01:28:43.200 --> 01:28:54.139
Gil Friend • Sustainability OG • CxO Coach: for the moment, and so… and so that's a… that's, you know, somebody's on a pump, pumping that bubble to keep that afloat. There's that. On the data center,

673
01:28:54.400 --> 01:29:05.790
Gil Friend • Sustainability OG • CxO Coach: footprint question, which you've talked about a bunch before, it's not just turning AI to solve the problems of agriculture, we're turning AI to design the data centers of the future, and it turns out that Microsoft has been doing that over the last

674
01:29:05.830 --> 01:29:17.389
Gil Friend • Sustainability OG • CxO Coach: 8 years or so. There's a book out just now by Joanne Garvin called, Innovation at Microsoft. She led the Data Center of the Future project, and they've designed data centers that are net-zero energy and net-zero water.

675
01:29:18.900 --> 01:29:33.279
Gil Friend • Sustainability OG • CxO Coach: Are they building them? Doesn't seem to be, and I don't know if anybody is yet, but at least there's that as a theoretical possibility, not just for data centers, but potentially for everything, if we chose to go that way. But again, it's not, can we do it technically, it's will we do it?

676
01:29:33.350 --> 01:29:43.210
Gil Friend • Sustainability OG • CxO Coach: Politically, economically, financially, and so forth. Last, back to Mike's question about where do I move the money if I'm taking it out of tech and gold and Bitcoin and so forth.

677
01:29:43.210 --> 01:29:54.289
Gil Friend • Sustainability OG • CxO Coach: My financial advisor and just about every smart financial… well, who I'm calling a smart financial advisor that I've spoken to, says, find your asset allocation strategy and stay there.

678
01:29:54.910 --> 01:30:01.340
Gil Friend • Sustainability OG • CxO Coach: And I periodically go back to my guy and say, well, like, shouldn't we be moving some money here? He says, no. No, just…

679
01:30:01.370 --> 01:30:16.629
Gil Friend • Sustainability OG • CxO Coach: allocate and stay there and rebalance periodically, and that's the way that you move tech profits into other sectors. The one place where I'm disagreeing with him, I think, is that I think we should be much more offshore than we are now.

680
01:30:17.050 --> 01:30:24.120
Gil Friend • Sustainability OG • CxO Coach: you know, diversify out of the United States into international much more than we are now, just because of the instability of here.

681
01:30:24.840 --> 01:30:27.390
Gil Friend • Sustainability OG • CxO Coach: So that's, you know, that's how I'm thinking about it.

682
01:30:29.310 --> 01:30:31.339
Jerry Michalski: Too bad the scoop's got nothing to say.

683
01:30:31.340 --> 01:30:35.200
Gil Friend • Sustainability OG • CxO Coach: I just drove Mike off the screen by saying that, oh my god.

684
01:30:35.400 --> 01:30:40.189
Jerry Michalski: No, no, no, Doug, your hand is down, but, that might have been accidental.

685
01:30:41.080 --> 01:30:41.730
Doug Breitbart: Alright.

686
01:30:41.930 --> 01:30:46.590
Doug Breitbart: I just, I just wanted to throw in, and this is empirical.

687
01:30:47.880 --> 01:30:56.730
Doug Breitbart: With the rate at which energy technology, battery technology, quantum computing, and the rest are Progressing.

688
01:30:58.140 --> 01:31:05.050
Doug Breitbart: These massive billion-dollar data center Investments.

689
01:31:05.280 --> 01:31:07.560
Doug Breitbart: Are obsolete in 18 months.

690
01:31:09.650 --> 01:31:11.589
Doug Breitbart: How does it make sense?

691
01:31:12.460 --> 01:31:19.550
Doug Breitbart: Like, just on an intuitive, logical level, It doesn't make sense.

692
01:31:19.890 --> 01:31:21.379
Doug Breitbart: It's gotta blow up.

693
01:31:22.750 --> 01:31:28.169
Doug Breitbart: You know, when it's too good to be true, it just doesn't. It defies reality.

694
01:31:28.740 --> 01:31:38.479
Doug Breitbart: And… when… The biggest players in the world are canning 30, 40, 50,000 people.

695
01:31:39.070 --> 01:31:42.379
Doug Breitbart: To divert resource to constructing data centers.

696
01:31:43.780 --> 01:31:47.490
Doug Breitbart: That's the end. That's not the beginning of the end. That's not a bubble.

697
01:31:47.780 --> 01:31:50.349
Doug Breitbart: That's the end of whatever this has been.

698
01:31:50.620 --> 01:31:55.730
Doug Breitbart: It sort of reached its insane… logical conclusion.

699
01:31:56.190 --> 01:32:00.659
Doug Breitbart: As a… as a capitalist, You know, train run amok.

700
01:32:01.840 --> 01:32:08.229
Jerry Michalski: We only need $7 trillion, and we can push right through that little eye of the needle and get into the singularity, don't you want that?

701
01:32:08.420 --> 01:32:15.080
Doug Breitbart: So, so, yeah, I don't… I don't know what's after that, but I do know what's ending.

702
01:32:16.100 --> 01:32:17.160
Doug Breitbart: Thank God.

703
01:32:17.990 --> 01:32:20.520
Doug Breitbart: Like, it needed to go away.

704
01:32:20.630 --> 01:32:23.849
Doug Breitbart: We need to do us differently and better if we're not gonna go extinct.

705
01:32:25.970 --> 01:32:29.569
Jerry Michalski: This conversation does not want to go extinct either. Go ahead.

706
01:32:29.570 --> 01:32:34.470
Alex Kladitis: Sorry, Doug mentioned a couple of things, it was really interesting. So…

707
01:32:34.770 --> 01:32:40.600
Alex Kladitis: I was in the throes of the, blockchain…

708
01:32:40.800 --> 01:32:52.239
Alex Kladitis: temperature in 2015 or thereabouts. NVIDIA shares skyrocketed because they were making the cars that were being used by the… the people that were mining Bitcoin and all the good things.

709
01:32:53.040 --> 01:32:54.659
Alex Kladitis: And the prices went up.

710
01:32:55.030 --> 01:32:59.570
Alex Kladitis: But what actually happened, Doug, and this is interesting what you said, is that

711
01:32:59.670 --> 01:33:13.770
Alex Kladitis: the technology was moving so quickly that the cards, the graphics cards they were producing, became obsolete, exactly as you said, within 18 months. So what happened, there was a backwash in the market of very good cards.

712
01:33:13.790 --> 01:33:23.500
Alex Kladitis: which were useful for games. So there was a second-hand market whilst they were building the latest ones. So it just reminded me of this situation where

713
01:33:23.580 --> 01:33:31.610
Alex Kladitis: All these data centers that are going to become obsolete, maybe they can… that will produce cheap, alternative…

714
01:33:31.970 --> 01:33:33.900
Alex Kladitis: Computing power for something.

715
01:33:34.060 --> 01:33:38.300
Alex Kladitis: What that something is, I have no idea, but I just thought I'd… mention it.

716
01:33:40.760 --> 01:33:41.910
Jerry Michalski: Thank you very much.

717
01:33:43.740 --> 01:33:45.609
Jerry Michalski: Okay, let's take a deep breath.

718
01:33:46.170 --> 01:33:53.399
Jerry Michalski: And then, we can close out this call. We have several different topics proposed for future calls, really juicy ones, so…

719
01:33:53.790 --> 01:33:56.199
Gil Friend • Sustainability OG • CxO Coach: I'll pick from among those, and .

720
01:33:57.300 --> 01:34:01.450
Jerry Michalski: I thank you very much, this has been… Delightful.

721
01:34:04.380 --> 01:34:05.900
Jerry Michalski: Let's be careful out there.

