WEBVTT

00:00:01.000 --> 00:00:03.000
It's just… it's just…

00:00:03.000 --> 00:00:08.000
This is the OGM weekly call for Thursday, November 6th, 2025.

00:00:08.000 --> 00:00:13.000
Uh, and we're talking about switching from left-hand drive to right-hand drive.

00:00:13.000 --> 00:00:19.000
whoops.

00:00:19.000 --> 00:00:23.000
And inconveniently, I was just receiving a call, which I declined.

00:00:23.000 --> 00:00:25.000
Um,

00:00:25.000 --> 00:00:28.000
Actually, it's about driving on the left side of the road to driving on the right side of the road.

00:00:28.000 --> 00:00:29.000
Thank you, yeah, I was trying to…

00:00:29.000 --> 00:00:33.000
the right-hand drive and left-hand drive is the… is the reverse thing, it's where the wheel is.

00:00:33.000 --> 00:00:36.000
And left-hand drive is… right.

00:00:36.000 --> 00:00:40.000
Exactly.

00:00:40.000 --> 00:00:41.000
And I… I… can anybody Google whether other countries have done this besides Sweden?

00:00:41.000 --> 00:00:45.000
So…

00:00:45.000 --> 00:00:46.000
or even Claude, or somebody else?

00:00:46.000 --> 00:00:47.000
Uh, there's a couple. Iceland…

00:00:47.000 --> 00:00:49.000
Nigeria, Ghana, Saudi Arabia, Yemen.

00:00:49.000 --> 00:00:53.000
Yeah. I mean, there were so many…

00:00:53.000 --> 00:00:56.000
Peter does all do it overnight?

00:00:56.000 --> 00:00:57.000
Um…

00:00:57.000 --> 00:00:58.000
What's fascinating… What's fascinating is that all…

00:00:58.000 --> 00:00:59.000
Pretty close, yeah.

00:00:59.000 --> 00:01:04.000
implies a level of coordination that's probably rare.

00:01:04.000 --> 00:01:11.000
Uh, it's… I haven't quite got to how fast it was, but it was pretty fast, too.

00:01:11.000 --> 00:01:14.000
You kind of have to do it… you have to do it all at once. Go ahead, Alex.

00:01:14.000 --> 00:01:15.000
Yeah, yeah.

00:01:15.000 --> 00:01:20.000
It's fascinating, though, that. The main reason was… it was the British Empire, and they drove on the left.

00:01:20.000 --> 00:01:21.000
Yeah.

00:01:21.000 --> 00:01:26.000
So why did the U.S. go to the right from the start.

00:01:26.000 --> 00:01:27.000
It's… it's… it's whether you descend from England or Germany.

00:01:27.000 --> 00:01:30.000
There were supposedly colony ones, sort of.

00:01:30.000 --> 00:01:34.000
So, Germans were on the right.

00:01:34.000 --> 00:01:40.000
So, was there a time where the. different states in the U.S.

00:01:40.000 --> 00:01:42.000
the carriages were driving in the wrong… the different side.

00:01:42.000 --> 00:01:44.000
That's a good question.

00:01:44.000 --> 00:01:54.000
A good question. We, you know, we could look this up. And I seem to remember that at one point, left or right-hand drive had to do with which hand was free when you were in the carriage.

00:01:54.000 --> 00:02:01.000
with horses. And so if you… most people are right-handed, so you wanted to be able to use your right hand

00:02:01.000 --> 00:02:07.000
Defensively, I have no idea, but I think that had something to do with…

00:02:07.000 --> 00:02:08.000
From jousting?

00:02:08.000 --> 00:02:14.000
I heard that it's… I heard that it stems from tournaments, from Knights tournaments, where you have the right-hand free for your jousting tournaments, where your right hand free for your weapon, but that may be totally crap, I don't know.

00:02:14.000 --> 00:02:18.000
Yeah. So here we go. We have our amateur historical theories.

00:02:18.000 --> 00:02:19.000
Uh, to warm up our space.

00:02:19.000 --> 00:02:25.000
I think another one is, what side of the ship you unload.

00:02:25.000 --> 00:02:26.000
Well, that's port and starboard.

00:02:26.000 --> 00:02:27.000
And…

00:02:27.000 --> 00:02:30.000
Hmm.

00:02:30.000 --> 00:02:31.000
Do you think is connected?

00:02:31.000 --> 00:02:32.000
But I think that descended, or it went into… yeah.

00:02:32.000 --> 00:02:33.000
Hmm.

00:02:33.000 --> 00:02:35.000
Oh, now I'm… Latigo is a whip.

00:02:35.000 --> 00:02:37.000
Victoria. Um…

00:02:37.000 --> 00:02:40.000
The, um…

00:02:40.000 --> 00:02:45.000
briefly, we… before the invented the rudder at the stern of the ship,

00:02:45.000 --> 00:02:47.000
Uh, there were steer boards.

00:02:47.000 --> 00:02:50.000
And this is the origin of the words, uh, cyborg.

00:02:50.000 --> 00:02:55.000
Uh, because it was actually the… well, more interestingly, perhaps,

00:02:55.000 --> 00:03:00.000
The steer board, because most people were right-handed, was usually on the right side of your ship,

00:03:00.000 --> 00:03:07.000
And you didn't want to damage the steer board when you docked, so you would dock on… you would put the port on the port side.

00:03:07.000 --> 00:03:09.000
And that's why we have steerboard and port.

00:03:09.000 --> 00:03:12.000
Which became important starboard.

00:03:12.000 --> 00:03:13.000
Oh, right. I like that.

00:03:13.000 --> 00:03:16.000
Yeah, so that's why it's the port. The port was always on your left.

00:03:16.000 --> 00:03:18.000
I'm so glad I come to OGM.

00:03:18.000 --> 00:03:19.000
You know…

00:03:19.000 --> 00:03:21.000
I work over all kinds of things.

00:03:21.000 --> 00:03:25.000
Alright, some days I feel like cliff clavering. I'm full of shit like that, so…

00:03:25.000 --> 00:03:26.000
Um, off we go.

00:03:26.000 --> 00:03:34.000
The different countries switched in one day. The difference is actually the lead time. Sweden took 3 or 4 years, and other countries took 1 or 2.

00:03:34.000 --> 00:03:36.000
Mm-hmm.

00:03:36.000 --> 00:03:40.000
Thank you. Um…

00:03:40.000 --> 00:03:45.000
Fabulous. Today, we are on the first Thursday of November, which

00:03:45.000 --> 00:03:49.000
typically means we do a check-in routine,

00:03:49.000 --> 00:03:54.000
Um, Victoria, I don't think you've been through one of our check-in runs, so I'll explain the protocol.

00:03:54.000 --> 00:03:58.000
I am going to not moderate until everybody has checked in only once.

00:03:58.000 --> 00:04:06.000
We don't want any conversation between the check-ins. Each of us is just going to check in and say, what OGME thing is on our minds, or have we done?

00:04:06.000 --> 00:04:08.000
is, like, active for us.

00:04:08.000 --> 00:04:18.000
It's not… the idea is not to, sort of, dump the whole laundry list of everything you're doing into the call, but rather to pick something that feels…

00:04:18.000 --> 00:04:21.000
exemplifies what you've been up to in some sense.

00:04:21.000 --> 00:04:29.000
And, uh, we don't use the chat, and this is difficult, I know, but especially for Pete and for me, who adore…

00:04:29.000 --> 00:04:33.000
Um, following things up with, oh, I found it, I found it, here's… look, look what the link is.

00:04:33.000 --> 00:04:42.000
Uh, but Pina and I will probably do that privately, and then once everybody has checked in once, we switch back into normal conversational mode,

00:04:42.000 --> 00:04:49.000
But I… during the check-in, whoever wants to talk next, raise your hand, or just, if nobody has their hand raised, just step in.

00:04:49.000 --> 00:04:53.000
step into the call. Whenever you're moved to, uh,

00:04:53.000 --> 00:04:56.000
check in. And, um…

00:04:56.000 --> 00:05:00.000
What am I forgetting about the protocol?

00:05:00.000 --> 00:05:07.000
I think that's most of it. So, try not to engage in conversation during the check-in route. It's just one person at a time.

00:05:07.000 --> 00:05:13.000
And then just make notes on a piece of paper or in the chat without hitting enter, without hitting return, so it doesn't go to the chat.

00:05:13.000 --> 00:05:19.000
But make notes about stuff you'd like to talk about when we go back into conversation mode. That's all.

00:05:19.000 --> 00:05:23.000
Uh, there you go. Excellent.

00:05:23.000 --> 00:05:28.000
So, with that, I am going to mute myself, and uh…

00:05:28.000 --> 00:05:30.000
Oh, oh, I know what I forgot.

00:05:30.000 --> 00:05:36.000
We… during check-in rounds, we like, um, empty space. We like quiet.

00:05:36.000 --> 00:05:39.000
So, it's perfectly fine for us to be still,

00:05:39.000 --> 00:05:42.000
And just hang out with each other, kind of quicker meeting style.

00:05:42.000 --> 00:05:45.000
That is a big plus, because

00:05:45.000 --> 00:05:49.000
This is a bit of an antidote to our normally frenzied pace.

00:05:49.000 --> 00:05:51.000
Uh, through conversations.

00:05:51.000 --> 00:05:52.000
That's the thing I forgot.

00:05:52.000 --> 00:05:59.000
Jerry, could I just suggest, before we go into formal check-in round, that, uh, silence is not a terrible thing even during a normal call?

00:05:59.000 --> 00:06:00.000
We might have a little more breath.

00:06:00.000 --> 00:06:02.000
Oh, and…

00:06:02.000 --> 00:06:06.000
Anytime.

00:06:06.000 --> 00:06:07.000
Yep.

00:06:07.000 --> 00:06:09.000
Agreed, and I occasionally… I occasionally take us into silence during a normal call, but I often… I often forget for too long, so…

00:06:09.000 --> 00:06:10.000
I welcome it when you do it.

00:06:10.000 --> 00:06:17.000
Thank you. And I love taking groups into silence, and they always welcome it. Everybody's like, oh, okay, good.

00:06:17.000 --> 00:06:19.000
And you can just see people.

00:06:19.000 --> 00:06:23.000
tune in differently, so it's lovely. Thank you for the reminder again.

00:06:23.000 --> 00:06:36.000
Uh, with that, I'm going to mute myself and see who wants to be our first, uh, check-in.

00:06:36.000 --> 00:06:42.000
I think I'm gonna go first.

00:06:42.000 --> 00:06:49.000
Um, so… thinking about my personal projects and stuff.

00:06:49.000 --> 00:06:58.000
I stumbled onto a guy named Kevin McLeod. through a documentary that I haven't watched yet. So, Kevin McLeod.

00:06:58.000 --> 00:07:07.000
has made 2,000 pieces of royalty-free music. And made them available under the Creative Commons.

00:07:07.000 --> 00:07:13.000
he did this just… He was a composer and decided to start doing this.

00:07:13.000 --> 00:07:18.000
And as a result, his music has been picked up and used.

00:07:18.000 --> 00:07:26.000
In the backgrounds of… countless TikTok videos and… Facebook Reels and all that kind of stuff.

00:07:26.000 --> 00:07:33.000
Um, one of his songs has been played 31.6 billion times.

00:07:33.000 --> 00:07:38.000
He's the most prolific composer you've never heard of, according to the New York Times.

00:07:38.000 --> 00:07:47.000
And what it made me think about… was the idea of… Remixability.

00:07:47.000 --> 00:07:54.000
of what you are producing. And instead of making meals, making.

00:07:54.000 --> 00:08:04.000
Perhaps ingredients or recipes. And things that other people can and want to use to create things that are theirs.

00:08:04.000 --> 00:08:16.000
as opposed to… spending most of my time creating something that's… perfect and self-contained, and… is its own thing.

00:08:16.000 --> 00:08:23.000
never to be taken apart and…

00:08:23.000 --> 00:08:28.000
given a life of its own. through interactions with other people.

00:08:28.000 --> 00:08:35.000
So, that relates to the projects that I've been working on, and it's kind of given me a different sort of perspective on.

00:08:35.000 --> 00:08:40.000
that idea? How can I make the things that I'm doing.

00:08:40.000 --> 00:08:47.000
more… remixable by the people who might want to use them.

00:08:47.000 --> 00:08:58.000
I've talked about my universals. uh, set of… frameworks, and I've… turned them into a little deck that I've been playing around with.

00:08:58.000 --> 00:09:07.000
And the ability to take. several, and move them around, and recombine them, and… all of that has changed it for me.

00:09:07.000 --> 00:09:11.000
And I hope that it's going to make it actually more usable.

00:09:11.000 --> 00:09:41.000
for other people, should I publish this?

00:09:45.000 --> 00:09:47.000
Well, I'll go.

00:09:47.000 --> 00:09:56.000
And I'll confess I'm finding it very hard not to comment on what Scott just said, but I won't.

00:09:56.000 --> 00:10:07.000
Most OGM-y thing for me and Alex will like hearing about this. I've been working on vibe coding, um, a technique for scrubbing, uh, for automatically de-identifying.

00:10:07.000 --> 00:10:12.000
transcripts of coaching conversation that I'm having with my clients. As most of you know, I'm

00:10:12.000 --> 00:10:16.000
Primarily working these days as an executive coach.

00:10:16.000 --> 00:10:18.000
Excuse me.

00:10:18.000 --> 00:10:24.000
Um, for CXOs and founders, uh, who are purpose-driven. Um, I, um,

00:10:24.000 --> 00:10:35.000
I de-identify the conversations, I feed them into one of two bots that I've got running, one on Delphi and one that Alex and I have been building custom in the lab, which is more cool and sophisticated.

00:10:35.000 --> 00:10:48.000
And of course, to use that, I have to get any personally identifying information out of the scheme. I've done that with manual global search and replace for a long time, and now I've got… Claude has built me a drag-and-drop,

00:10:48.000 --> 00:10:56.000
finished configuring it yesterday, and today I'll test it on a handful of clients and see how well it does, and if it works well, then we're off to the races.

00:10:56.000 --> 00:10:59.000
Um, and, um…

00:10:59.000 --> 00:11:07.000
Aside from the specifics of that, it's cool for me to be diving in… this is my first re-entry into the world of coding in, like, 100 years.

00:11:07.000 --> 00:11:12.000
And so, kind of remarkable what, uh, what's possible, um,

00:11:12.000 --> 00:11:18.000
Both out of the box and with the kind of fine-tuning that becomes available. I know that a number of you have done way more than me.

00:11:18.000 --> 00:11:26.000
Um, but that's the OGME report from me for today.

00:11:26.000 --> 00:11:56.000
And I'm still here, but I'm gonna hide, because I'm preparing breakfast, and you don't want to watch that.

00:12:50.000 --> 00:12:57.000
I'll go, um… If I may. Um… Something that surprised me, uh, happened.

00:12:57.000 --> 00:13:05.000
This week, I came across this week, like, sadly it's going to be an AI thing, but slightly different, I hope.

00:13:05.000 --> 00:13:11.000
So, ever since LLMs have become so powerful, I have set up a number of them where.

00:13:11.000 --> 00:13:19.000
the scandal finance pages, and they give me a report every morning, they scan lots of medical things, give me reports, etc.

00:13:19.000 --> 00:13:25.000
So, won't go into it, but I just wanted to know what China was saying.

00:13:25.000 --> 00:13:32.000
And I repurposed. I'll use the usual, the same format, more or less. I repurposed it so that.

00:13:32.000 --> 00:13:40.000
it's task, as I asked it. was to… scan all the Chinese, or as many Chinese.

00:13:40.000 --> 00:13:45.000
big daily website papers, whatever. and tell me what the Chinese are talking about.

00:13:45.000 --> 00:13:58.000
Just that. And what I've found, cut a long story short, is that… I'm using ChatGPT. It wouldn't… not give me what the Chinese papers are saying without telling me.

00:13:58.000 --> 00:14:03.000
why they're either wrong, or… this is a true situation, or whatever. It was as if.

00:14:03.000 --> 00:14:09.000
inbuilt into the AI. is this bit that says.

00:14:09.000 --> 00:14:15.000
what have the Chinese say is not true? this is the interpretation that we're giving. And I've tried really hard to say to it, do not.

00:14:15.000 --> 00:14:21.000
comment on it. Do not do anything. All I want is a strap line of what they're saying. That is it.

00:14:21.000 --> 00:14:25.000
And the reason I found it interesting was because, in the end, I gave up, because I just wouldn't do it.

00:14:25.000 --> 00:14:31.000
Um, the reason I find it interesting is because when DeepSeek came up, which is the Chinese thing.

00:14:31.000 --> 00:14:37.000
the whole press was saying it's biased towards. pro-China stuff.

00:14:37.000 --> 00:14:43.000
And it was, if you… if you tried to do anything, anything politics with it, it would, and particularly Tiananmen Square.

00:14:43.000 --> 00:14:48.000
it would actually not… not behave in our bios standards.

00:14:48.000 --> 00:14:56.000
But I was surprised to see how… Obviously, in my opinion, it's inbuilt into the training data.

00:14:56.000 --> 00:15:01.000
But we have our biases as well. And more to the point.

00:15:01.000 --> 00:15:07.000
it's, you know, we talk about AI being safe and whatever, you know, all this.

00:15:07.000 --> 00:15:13.000
And I'm surprised that it's so in-built. I could talk about this for a lot further, but basically, it just surprised me.

00:15:13.000 --> 00:15:23.000
that our AIs are also quite. Opinionated in certain directions, particularly that one.

00:15:23.000 --> 00:15:38.000
Um, so…

00:15:38.000 --> 00:15:40.000
I'll go.

00:15:40.000 --> 00:15:47.000
Um… as you may know, I've been using Agentic AI

00:15:47.000 --> 00:15:58.000
assistance for software development for, I don't know, 6-8 months, probably longer than that. Um, so I'm using… I've been using Cloud Code, um, and it…

00:15:58.000 --> 00:16:04.000
is… it works with me about at the level of a human software developer.

00:16:04.000 --> 00:16:12.000
Um, agentic means lots of crazy things, and AI means lots of crazy things, but when I say agentic AI, it's a coding assistant.

00:16:12.000 --> 00:16:14.000
What I mean is…

00:16:14.000 --> 00:16:24.000
Something like an LLM, but set up so that it can read and write many files, source code files or text files, or whatever. And also,

00:16:24.000 --> 00:16:28.000
Um, one of the things it does is, uh, you can tell it

00:16:28.000 --> 00:16:37.000
to do something, and it will make a little list for itself. Here's a bunch of to-dos that I need to get Pete's bug fixed or feature implemented, or something like that.

00:16:37.000 --> 00:16:47.000
And then it goes down that list and checks them off, and does another thing, checks it off, does another thing. So it takes the LLM experience and kind of up-levels it towards

00:16:47.000 --> 00:16:53.000
An autonomous human kind of level, and I don't mean that it's smart like a human, or…

00:16:53.000 --> 00:16:55.000
thoughtful like a human, or creative like a human.

00:16:55.000 --> 00:17:00.000
Although it kind of is, all of those. Um…

00:17:00.000 --> 00:17:09.000
Uh, so, a thing that I've been tracking for a long time is doing the same thing with, uh, text, either non-fiction or fiction.

00:17:09.000 --> 00:17:12.000
Um, having a bot understand

00:17:12.000 --> 00:17:22.000
Like, a big chunk of, um, uh, serial fiction, for instance, and being able to write new episodes, or going over the whole thing and making a plan for

00:17:22.000 --> 00:17:27.000
you know, new, uh, volume in a series, something like that.

00:17:27.000 --> 00:17:34.000
does that really well. Um, I think it would also work great for a legal team.

00:17:34.000 --> 00:17:42.000
Um, doing a bunch of work with lots of documents, and they kind of all need to fit together and change, you know, together, and…

00:17:42.000 --> 00:17:44.000
Uh, tell me, you know, tell me the…

00:17:44.000 --> 00:17:51.000
Um, overarching meaning of these documents together. Tell me which ones don't quite fit right. How would you rewrite it?

00:17:51.000 --> 00:17:58.000
Go ahead and try rewriting that and see if I like the draft that you do. No, I don't like that one, throw it away, do another one. So…

00:17:58.000 --> 00:18:10.000
non-fiction, big research studies, fiction, uh, legal stuff, um, anything where you're using a corpus of documents and multiple people.

00:18:10.000 --> 00:18:22.000
So I've been super excited about this for a while, and have a hard time explaining to people why it's cool, and why they should be doing it as the same… same like me. I finally got the first taste of… I've worked with a couple people,

00:18:22.000 --> 00:18:26.000
adopting these tools, and…

00:18:26.000 --> 00:18:33.000
It's kind of a mix of, like, terror because, you know, there's a lot of overhead to get to be able to work with the tool.

00:18:33.000 --> 00:18:39.000
Um, it doesn't feel like a lot of overhead to me, because software developers built these things, so they built it so it…

00:18:39.000 --> 00:18:42.000
just fits… slots perfectly into a software development thing.

00:18:42.000 --> 00:18:46.000
And for me, it feels like there's a little bit of a difference between

00:18:46.000 --> 00:18:53.000
building software and building Obsidian vault, and yada yada, and using Git, and it gets more and more complicated, kind of.

00:18:53.000 --> 00:18:59.000
So, um… so the newest one, Cloud Code, now has a web version. You don't have to use your terminal.

00:18:59.000 --> 00:19:04.000
Um, there's another one called Droid from a company named Factory Eye, which also has a web.

00:19:04.000 --> 00:19:15.000
The web versions also work on mobile, kind of, so we're right at the cusp of these things not being where you need a geek to… you don't need to be a geek to use it.

00:19:15.000 --> 00:19:19.000
Um, however, I've also… I've also kind of had a…

00:19:19.000 --> 00:19:23.000
I was really disappointed in the…

00:19:23.000 --> 00:19:28.000
And the difficulty with which a non-technical person had to kind of, like, muscle their way through it.

00:19:28.000 --> 00:19:31.000
I was watching somebody use it, and they were happy.

00:19:31.000 --> 00:19:38.000
But I could see it was kind of like they had an oven mitts on, and they were trying to do, you know, fine-grained tasks.

00:19:38.000 --> 00:19:40.000
Um, so…

00:19:40.000 --> 00:19:46.000
Anyway, long story short, um, I think… I think Cloud Code Web…

00:19:46.000 --> 00:19:52.000
I… at least some people are getting a $250 credit. If you go to your Cloud Pro account,

00:19:52.000 --> 00:20:02.000
it might say, you should try Cloud Code Web. It's… and here's $250 for you to spend by November 18th. Um, Droid has another big promotion right now.

00:20:02.000 --> 00:20:09.000
Um, more people should be doing this, and I'm kind of pulling back from saying everybody should do it right away, because

00:20:09.000 --> 00:20:11.000
It's a little hard, um…

00:20:11.000 --> 00:20:16.000
And using the web version is actually even a little bit more hard than the terminal version, I think.

00:20:16.000 --> 00:20:20.000
Um, it's… it's an easier on-ramp, but goes…

00:20:20.000 --> 00:20:37.000
Because it's harder… it's more like molasses or something like that. Um, everybody's gonna be doing this kind of stuff in 18 months, 24 months. Um, you could be doing it today. I would love to show some people how to do that more.

00:20:37.000 --> 00:20:43.000
Um, kind of as a coda, one of the things that I've been doing, a lot of times from an OGM call,

00:20:43.000 --> 00:20:46.000
Uh, somebody will mention something, um…

00:20:46.000 --> 00:20:51.000
Uh, I think it was last week, actually, Stacy mentioned something about, um,

00:20:51.000 --> 00:20:57.000
uh… people in her building working together, and that makes a… a little…

00:20:57.000 --> 00:21:00.000
society that protects itself from outsiders and things like that.

00:21:00.000 --> 00:21:07.000
And I said, huh, that reminds me of the broken windows theory, that reminds me of this, that reminds me of that. And so…

00:21:07.000 --> 00:21:11.000
I think I've been doing for about 6 months is…

00:21:11.000 --> 00:21:14.000
I know there's a bunch of frameworks around a topic,

00:21:14.000 --> 00:21:20.000
Um, hey, AI, help me put together a wiki, uh, you know, a 20-page, kind of,

00:21:20.000 --> 00:21:31.000
paper slash thesis slash framework slash overview of sometimes things that are a little bit different, that people have kind of been looking at the same part of the world.

00:21:31.000 --> 00:21:39.000
and coming up with different ways to do it, and they don't talk to each other. So, the most fun for me is when me and the AI can kind of smash

00:21:39.000 --> 00:21:46.000
frameworks together that ought to go together, but never have before. And so I've been building these 10, 20-page websites like that.

00:21:46.000 --> 00:21:51.000
collective efficacy, social cyclic theories, the intention-action gap.

00:21:51.000 --> 00:21:58.000
strategy and management frameworks, government governance principles, commons stewardship by communities.

00:21:58.000 --> 00:22:07.000
And I… I just do these kind of one-off things, and they're half, like, amazing, and I've… I've pointed out a couple of them in the OGM list.

00:22:07.000 --> 00:22:13.000
Um, and half I'm embarrassed by it, because it's mostly built by AI.

00:22:13.000 --> 00:22:15.000
So, I'm working with an…

00:22:15.000 --> 00:22:21.000
the AIs now to kind of frame that into what is good about these things, and what is bad.

00:22:21.000 --> 00:22:27.000
How do humans work with them? Why is it okay that the, um…

00:22:27.000 --> 00:22:37.000
that the AI has kind of organized things in a way that makes sense. So, I'm working on kind of a meta-collection of all these things together called

00:22:37.000 --> 00:22:42.000
field of sheaths. Each of these things is… I'm thinking of as a sheaf.

00:22:42.000 --> 00:22:58.000
Um, so now I've got, you know, 6 or 8 sheaves of whatever these things knowledge together. And, um, so I'm working on that. I'm too embarrassed to release it, and uh… yet. Um, but I am working on it. If this is something that's like, oh my gosh, Pete,

00:22:58.000 --> 00:23:12.000
Could I help, you know, could I do that? Could you help me do one of those myself? Um, I'd love to… love to hear from you.

00:23:12.000 --> 00:23:16.000
I'll go next quickly. If you guys got the Plex.

00:23:16.000 --> 00:23:46.000
Uh… I think, um… You should, uh… I'm really happy about the two people I asked to write for it, and they wrote this one, and I didn't have anything, and I was very happy with it, so…

00:24:05.000 --> 00:24:08.000
Well, I'll toss in what I've been thinking about.

00:24:08.000 --> 00:24:14.000
Uh, we're playing with the notion of growth. The framework is kind of like, how do I…

00:24:14.000 --> 00:24:16.000
uh, integrate, uh…

00:24:16.000 --> 00:24:24.000
regenerative notions that I've been, you know, developing over the last few years have been, you know, that have been emerging, I think, in many spaces in the last few years.

00:24:24.000 --> 00:24:29.000
into, kind of, the technocratic public policy background that I've… that I've always, uh…

00:24:29.000 --> 00:24:35.000
bottled myself in. And… one of the key underlying things is that in the public policy world, we don't really

00:24:35.000 --> 00:24:44.000
focus on living systems. You know, we… we may talk about systems, we may, you know, and maybe not often even that. You just talk about optimization and

00:24:44.000 --> 00:24:47.000
efficiencies, and kind of, you know, there's, there's, uh…

00:24:47.000 --> 00:24:49.000
these other values that get, um…

00:24:49.000 --> 00:24:52.000
being used as proxies for something good, I think.

00:24:52.000 --> 00:24:57.000
Um, and so we, you know, you tend to get trapped into making things a little bit better.

00:24:57.000 --> 00:25:03.000
Uh, and rarely can you get to kind of a positive sum outcome abundance kind of notions.

00:25:03.000 --> 00:25:08.000
Um, and so I've been like, well, how would I retrofit some of that into, you know, our old…

00:25:08.000 --> 00:25:10.000
borrow, uh, the old framings.

00:25:10.000 --> 00:25:19.000
And, you know, what in another version of that is? What can we learn from traditional economics? Like, a lot of people kind of want to just throw it away and say economists are fools, and…

00:25:19.000 --> 00:25:28.000
you know, they're all… they're funny, I like to make fun of them and stuff. But, you know, there's a lot of wisdom in economics, too, so I wouldn't… I don't think we should toss it away. We should think of it as Indigenous knowledge, probably.

00:25:28.000 --> 00:25:33.000
Um, so what… what is, you know, how do these two things get together? One of the core

00:25:33.000 --> 00:25:35.000
words, anyway, I think, is growth.

00:25:35.000 --> 00:25:38.000
Traditional economics kind of relies on growth.

00:25:38.000 --> 00:25:45.000
As… certainly as one of the answers to, like, do you want to get rid of poverty, how do you get rid of poverty? Well, you grow.

00:25:45.000 --> 00:25:50.000
Um, so there's… there's that kind of component. But the other is just kind of, like, economies tend to be…

00:25:50.000 --> 00:25:54.000
robust while they're growing, they tend to get very fragile while they're not.

00:25:54.000 --> 00:26:04.000
Um, so there's… there's this kind of key brain in there. And living systems are kind of critical. Growth matters a ton to living systems. And so I've just been wondering,

00:26:04.000 --> 00:26:12.000
Like, what are the kinds of growth and the perspectives on growth that would inform, kind of, traditional economics in a more of a living systems

00:26:12.000 --> 00:26:14.000
way.

00:26:14.000 --> 00:26:19.000
And are there, kind of, examples, you know, basically, uh,

00:26:19.000 --> 00:26:27.000
metaphors that we can take from biological systems around growth that would inform traditional economics in new ways.

00:26:27.000 --> 00:26:32.000
And for, you know, the part of what provoked this was somebody talking about a story of how they had

00:26:32.000 --> 00:26:39.000
submitted for their Stanford application decades ago. They had submitted… they had to write a review of the book they'd written.

00:26:39.000 --> 00:26:44.000
And the book that she had written was about the history of sourdough.

00:26:44.000 --> 00:26:53.000
And how people, like, in the, uh, as they were crossing the country, would keep their sourdough alive, and what it would take to keep your sourdough starter alive.

00:26:53.000 --> 00:27:00.000
And how, you know, the women pioneers were keeping the sourdough next to their breasts to keep it warm as they crossed the mountains.

00:27:00.000 --> 00:27:07.000
And I was just thinking about, like, the lively… the sourdough being alive, and the growth of sourdough as being a metaphor for something.

00:27:07.000 --> 00:27:10.000
So that's as far as I've got, and I need to take it to, um…

00:27:10.000 --> 00:27:31.000
pizza, much more… much smarter AI now to try and evaluate it, but growth. What sort of insights can he get from growth?

00:27:31.000 --> 00:27:36.000
Yeah, I got… I got an email yesterday from.

00:27:36.000 --> 00:27:43.000
a mailing list, the Union of Concerned Scientists. Sending out yet another, you know, last.

00:27:43.000 --> 00:27:51.000
urgent warning. Um, and, and… This has been going on for, like, 3, 4 years.

00:27:51.000 --> 00:27:58.000
And it's from Oregon, actually, Portland's… I think Portland State, or Oregon State is, uh.

00:27:58.000 --> 00:28:10.000
is the hub where now they're consolidating the data, and uh… And, um, you know, explain how we are passing one tipping point after the next.

00:28:10.000 --> 00:28:17.000
And it will be ignored, you know, it's just yet another alert that, uh.

00:28:17.000 --> 00:28:23.000
people who pay attention to it, people who know what it is, saying, okay, yeah, I know.

00:28:23.000 --> 00:28:29.000
And others really dismiss it, like, you know, it has been done for, you know, a really long time.

00:28:29.000 --> 00:28:38.000
So the… the… the… what… what is really missing here is a transition to what can you actually do about this?

00:28:38.000 --> 00:28:44.000
And because there are lots of things, you know, we can do if we put our mind to it.

00:28:44.000 --> 00:28:52.000
But, um, it's just… it just doesn't… doesn't seem to want to work. And of course, what we're doing.

00:28:52.000 --> 00:28:58.000
Uh, with our Food with Thought, uh. effort here.

00:28:58.000 --> 00:29:03.000
is to engage, you know, the neglected part of the market.

00:29:03.000 --> 00:29:08.000
You know, so you have small to medium-sized, uh.

00:29:08.000 --> 00:29:12.000
farmers, the… what's called the agriculture of the middle.

00:29:12.000 --> 00:29:16.000
Ooh. who can't get into the market.

00:29:16.000 --> 00:29:22.000
Um, and… The reason that is important is because.

00:29:22.000 --> 00:29:28.000
Food and agriculture is probably the most damaging part of our.

00:29:28.000 --> 00:29:39.000
into action with the natural world. Um, destroying, you know, watersheds and soils and biodiversity and… So it's incredibly damaging.

00:29:39.000 --> 00:29:44.000
But it has the capacity to turn that. It has the capacity.

00:29:44.000 --> 00:29:48.000
Not to recover and to heal, and to regenerate.

00:29:48.000 --> 00:29:57.000
Um, and the… in the large companies, you know, whether that's Monsanto, or Nestle, or, you know, everybody in there.

00:29:57.000 --> 00:30:01.000
They're working to automate this, they're working to reduce.

00:30:01.000 --> 00:30:14.000
Nitrogen by smart technology. climate-smart technology, and uh… hugely capital-intensive.

00:30:14.000 --> 00:30:21.000
even going to exclude more? farmers who cannot get into.

00:30:21.000 --> 00:30:29.000
that, you know, the investment cost required to. to build, you know, these automations, and it's not really necessary.

00:30:29.000 --> 00:30:37.000
You can absolutely, uh, shift. you know, into regenerative practices at scale.

00:30:37.000 --> 00:30:49.000
If farmers are being incentivized and paid for. So the, the, the… what, you know, what we would need to know about it, of course, is a farmer who wants to put in a cover crop.

00:30:49.000 --> 00:30:55.000
And then put his cash crop in, needs different pieces of equipment to make that work.

00:30:55.000 --> 00:31:02.000
Well, they're expensive. Now, so… So how is a small to medium-sized farmer going to shift into.

00:31:02.000 --> 00:31:07.000
environmental farming. You know, without having.

00:31:07.000 --> 00:31:16.000
the support structure for it. And so what we are arguing as a core point here is.

00:31:16.000 --> 00:31:21.000
There are measurements. that, uh, make it possible.

00:31:21.000 --> 00:31:29.000
to… to monitor what a farmer is doing, what a farmer has promised to do, so put a contract out there.

00:31:29.000 --> 00:31:35.000
And then monitor the outputs of that. So there's already some great examples. For example, in the Mississippi.

00:31:35.000 --> 00:31:40.000
But for delta, um… There are groups of farmers.

00:31:40.000 --> 00:31:46.000
who have been combined to reduce nitrogen runoff into the Mississippi River.

00:31:46.000 --> 00:31:49.000
Which then, of course, runs down into the Gulf of Mexico.

00:31:49.000 --> 00:32:00.000
While that's easy to measure, you know, it doesn't take much to… to, uh, to track that. And the incentives have been sufficient enough for those farmers.

00:32:00.000 --> 00:32:06.000
who are, um, very conservative and typically not interested.

00:32:06.000 --> 00:32:11.000
in experimenting a lot. there is just so much money at stake.

00:32:11.000 --> 00:32:19.000
that they are going to do it. And… Uh, and so what's these… what… and there are all kinds of little, like, one-point.

00:32:19.000 --> 00:32:26.000
uh… measurements. that can… that have the capacity to shift the entire system.

00:32:26.000 --> 00:32:33.000
Because in order to reduce the nitrogen one-off. You have to find other ways to fertilize your crop.

00:32:33.000 --> 00:32:38.000
You know, and so, you know, in the most… A basic thing is soil health.

00:32:38.000 --> 00:32:43.000
So if you have to increase your soil health, now we're talking about putting a cover crop on it.

00:32:43.000 --> 00:32:50.000
You know, maybe doing a no-till, maybe rotating your carp into a different type of crop, maybe a perennial or so.

00:32:50.000 --> 00:32:54.000
In order to do this. So all these other things fall into place.

00:32:54.000 --> 00:32:59.000
So by building a secondary revenue stream, the farmer now has two crops.

00:32:59.000 --> 00:33:04.000
He has his one, his food cup, and then he has an environmental benefit crop.

00:33:04.000 --> 00:33:08.000
You know, that comes out that is linked together.

00:33:08.000 --> 00:33:12.000
So we're saying this is one. I mean, in every industry.

00:33:12.000 --> 00:33:21.000
You know, there are ways where we could. insert ourselves and provide the incentives at scale.

00:33:21.000 --> 00:33:28.000
You know, to catch… the environmental deterioration that is, like, so obvious in your face now.

00:33:28.000 --> 00:33:36.000
And in the… what… what we are focusing on is the mostly NGOs, nonprofit.

00:33:36.000 --> 00:33:46.000
Small and medium-sized farms. So, and the reason for that is that if you combine all of the farmers markets and the CSAs.

00:33:46.000 --> 00:33:52.000
You know, and the, uh… the low level, uh.

00:33:52.000 --> 00:34:00.000
Uh, uh, output of… of this sector, it's a single digit of total sales.

00:34:00.000 --> 00:34:06.000
You know, the last time I checked was, like, 3% of total sales is if you combine all CSAs.

00:34:06.000 --> 00:34:19.000
Farmers markets, uh, into, into one… together. So… so the… the… to… to get these farmers into the wholesale market, that's our objective, to assist.

00:34:19.000 --> 00:34:26.000
bundling and aggregating small farmers. So they can participate in wholesale markets.

00:34:26.000 --> 00:34:34.000
So I was just… I was just, uh, wrote another newsletter. I mean, I was just thinking about… how easy it would be.

00:34:34.000 --> 00:34:38.000
You know, to really make an impact, and how stubbornly resistant.

00:34:38.000 --> 00:35:08.000
the market is to… to engage in change.

00:36:13.000 --> 00:36:16.000
One small thing and a bigger thing. The small thing is, um,

00:36:16.000 --> 00:36:20.000
I'm wearing a t-shirt. It's a beautiful day in this neighborhood.

00:36:20.000 --> 00:36:27.000
Because April gave a talk in Pittsburgh, and after the talk, she went and visited the Mr. Rogers Museum.

00:36:27.000 --> 00:36:35.000
In Pittsburgh. And Mr. Rogers has a super interesting history, uh, but Mr. Rogers played a really important role in April's childhood.

00:36:35.000 --> 00:36:44.000
Um… because she had a difficult relationship with her mom, and Mr. Rogers was telling her that she was okay just the way she is.

00:36:44.000 --> 00:36:49.000
and was one of the few entities that was doing that for her back in the day, and I didn't… I didn't realize how special that was.

00:36:49.000 --> 00:36:52.000
Until we had talked about it some years back.

00:36:52.000 --> 00:36:57.000
And uh… so it was lovely that she got to visit the museum, and she came back with a road sign.

00:36:57.000 --> 00:37:02.000
Uh, that says this is a kindness zone, and it looks exactly like a street sign.

00:37:02.000 --> 00:37:04.000
That, you know, you would put in a neighborhood,

00:37:04.000 --> 00:37:08.000
And we're gonna put it up on the wall here, so that's kind of cool. So that's tiny.

00:37:08.000 --> 00:37:10.000
then, uh, separately,

00:37:10.000 --> 00:37:16.000
I've been reading Dan Brown's latest Pock Boiler, The Secret of Secrets.

00:37:16.000 --> 00:37:22.000
And there'll be a tiny plot spoiler in what I say now, so if you really, really, really want to read,

00:37:22.000 --> 00:37:27.000
Um, mayor-written book, because Brown is not Faulkner,

00:37:27.000 --> 00:37:31.000
Uh, but one thing I love about Brown is, from the Da Vinci Code,

00:37:31.000 --> 00:37:38.000
I picked up in, you know, two-thirds of the way through the book, he has this thesis about the marginalization of the Divine Feminine.

00:37:38.000 --> 00:37:43.000
And I think I've told this story here before, but I love that thesis, and I have read

00:37:43.000 --> 00:37:49.000
Leonard Schne's book, The Alphabet vs. the Goddess, within a couple months of The Da Vinci Code, and it has the same thesis,

00:37:49.000 --> 00:37:53.000
In an academic work, from a completely different perspective,

00:37:53.000 --> 00:37:57.000
And I had been chasing the word consumer for already a couple decades then,

00:37:57.000 --> 00:38:04.000
And I had gotten to the same place myself, thinking about yin and yang and a bunch of other stuff, all of which is a longer story.

00:38:04.000 --> 00:38:09.000
But all those three stories kind of clicked in my head in a really wonderful way, and I'm like, oh,

00:38:09.000 --> 00:38:14.000
And now I start reading Dan Brown, and one of the major plot points of The Secret of Secrets

00:38:14.000 --> 00:38:18.000
is, um, non-local consciousness.

00:38:18.000 --> 00:38:26.000
Uh, and that snapped together with what Sunil Malhotra, a friend of OGM's, had said to me, I don't know, a year and a half ago,

00:38:26.000 --> 00:38:29.000
He basically said, hey, and I mentioned it on an OGM call somewhere,

00:38:29.000 --> 00:38:42.000
Uh, he said, hey, Jerry, what if consciousness isn't a thing that is… that materializes out of neurons in our heads when we have some kind of funny critical mass of neurons and activity? But what if consciousness is the background radiation

00:38:42.000 --> 00:38:45.000
of the universe, and we occupy it.

00:38:45.000 --> 00:38:48.000
we've sort of… we snap in… we… we…

00:38:48.000 --> 00:38:50.000
tune into it, like a radio station,

00:38:50.000 --> 00:38:57.000
Uh, and then when we die, we sort of leave it, or meld back into it, something like that. And that really sort of…

00:38:57.000 --> 00:39:05.000
change the way I look at things, and then by reading Dan Brown, I realized I had already put non-local consciousness in my brain from a completely different

00:39:05.000 --> 00:39:15.000
bit of research, and that it was the same… same general idea, that consciousness is, in fact, not a materialistic phenomenon of something that's happening in the chemistry,

00:39:15.000 --> 00:39:18.000
in our heads, but is in fact a much broader thing.

00:39:18.000 --> 00:39:27.000
a thing I like, which gets you into woo-woo noetic science territory, which I'm beginning to respect a lot more.

00:39:27.000 --> 00:39:35.000
That… and so one of the interesting things in the plot point is, so Tuesday night, I stopped someplace and have a meal, and

00:39:35.000 --> 00:39:43.000
keep reading in the book, and I get to the point where they talk about GABA, the basically inhibitor or receptor in our heads,

00:39:43.000 --> 00:39:50.000
GABA is, uh… gamma-aminobutyric acid, GABA, or GABA.

00:39:50.000 --> 00:39:53.000
And it appears to be the primary inhibitor

00:39:53.000 --> 00:39:55.000
in our neural system,

00:39:55.000 --> 00:39:58.000
Which means that high levels of GABA

00:39:58.000 --> 00:40:01.000
keep us from being overwhelmed by the world's stimuli.

00:40:01.000 --> 00:40:08.000
And when you have low GABA, you're suddenly, like, you take in a whole lot more, and apparently,

00:40:08.000 --> 00:40:11.000
Um, one of the mechanisms of epilepsy

00:40:11.000 --> 00:40:17.000
is a precipitous drop in GABA, which then causes your brain to be overwhelmed, and you go into seizure.

00:40:17.000 --> 00:40:25.000
Epileptics, when they come back out of a grand mal seizure, have moments of bliss, of like, you know, connection with the infinite.

00:40:25.000 --> 00:40:32.000
Um, and then they sort of reboot their brains back into being online with the usual constraints and filters on and everything else.

00:40:32.000 --> 00:40:34.000
Uh, and then she's, uh, uh…

00:40:34.000 --> 00:40:41.000
the protagonist's other half, who's part of the plot, says, you know, have you ever done, like, hallucinogens?

00:40:41.000 --> 00:40:44.000
They seem to reduce your GABA.

00:40:44.000 --> 00:40:55.000
levels, so that you can perceive what's always happening around you. It's not… it's not that they generate a hallucination in your head, it's that they lower your filters so that you can see what's going on around you.

00:40:55.000 --> 00:40:57.000
Which was all very funny because

00:40:57.000 --> 00:41:06.000
yesterday afternoon, um, I had my first psilocybin journey ever, which, in fact, for me, is my first trip ever. I've never done drugs.

00:41:06.000 --> 00:41:15.000
And a friend of mine in the neighborhood posted me at his place. It was incredibly safe and lovely. He's completely trustworthy, just it was great.

00:41:15.000 --> 00:41:21.000
And I had a phenomenal experience. I… I felt connected to, like, the mycelial

00:41:21.000 --> 00:41:31.000
Networks of the world. He had, uh, grown the shrooms himself. You can order kits, et cetera, et cetera. It works fine. He had tested it on himself.

00:41:31.000 --> 00:41:34.000
So he knew it was going to be good. Uh, and…

00:41:34.000 --> 00:41:39.000
it was really transformative. I had an incredible experience. And I'm still processing it,

00:41:39.000 --> 00:41:50.000
I tried during the experience to leave myself little dorms or candles or gateways to step back into the experience without doing the whole shroom thing. I don't know if that's going to work.

00:41:50.000 --> 00:41:53.000
Um, but I had… I had a…

00:41:53.000 --> 00:41:59.000
I have many different things happened. I felt like I was everything all at once-ish.

00:41:59.000 --> 00:42:08.000
Um, I now understand special effects, uh, you know, in movies a lot better, because they're basically just, like, copying what happens to you when you see everything.

00:42:08.000 --> 00:42:10.000
And, uh, trying to, you know,

00:42:10.000 --> 00:42:12.000
map that into graphics and visuals.

00:42:12.000 --> 00:42:17.000
Uh, but, uh, you know, the overwhelm you feel

00:42:17.000 --> 00:42:25.000
in the moment is not easily replicable in a movie theater when you're sitting, staring at a screen far away, and you know that there's no immediate consequences to you.

00:42:25.000 --> 00:42:31.000
Anyway, I feel like I'm transformed a bit, I don't know how.

00:42:31.000 --> 00:42:34.000
But, uh, it was thrilling.

00:42:34.000 --> 00:42:43.000
that, and complete.

00:42:43.000 --> 00:42:53.000
Well, Jerry, I'll step up and, uh… I had 1,700 different ways to go this week, but I'll start with where you left off.

00:42:53.000 --> 00:42:58.000
It is gratifying to know that I am not the only person.

00:42:58.000 --> 00:43:04.000
from the West Coast, who has never done a trip of any type.

00:43:04.000 --> 00:43:15.000
I, uh, don't even… haven't even smoked a cigarette, which is… goes to show you how I… I did a total bypass past adolescence.

00:43:15.000 --> 00:43:22.000
Uh, when I joined the Clinton administration in January of 20… uh, January 1993.

00:43:22.000 --> 00:43:26.000
We all had to get security clearances, and in my case, at a pretty high level.

00:43:26.000 --> 00:43:43.000
And, of course, they have the… people interviewing you, and because it was the Clinton administration, and we had a whole bunch of people in their late 20s and early 30s, the first question was, okay, how often have you done drugs, and are you still doing them?

00:43:43.000 --> 00:43:47.000
And I said, well, no, I've never done drugs.

00:43:47.000 --> 00:43:58.000
And they just refuse to believe this, because it not only were, you know, every other person they'd interviewed had done drugs in college, and several of them were doing drugs.

00:43:58.000 --> 00:44:04.000
Currently, and actually had to go on some kind of, you know, withdrawal plan, and had to be.

00:44:04.000 --> 00:44:09.000
tested every couple weeks to make sure they were on it.

00:44:09.000 --> 00:44:19.000
I didn't know how much they didn't believe me until I had two random drug tests in the next 10 days.

00:44:19.000 --> 00:44:28.000
But apparently, you know, they've chose to believe me, or maybe they just talked to somebody else who had worked with me when I was working with Gore.

00:44:28.000 --> 00:44:37.000
in the Senate, and they were assured that, yeah, Mike Nelson, if there's anybody in the Clinton administration who hasn't done drugs, it's Mike Nelson.

00:44:37.000 --> 00:44:46.000
But anyway, I am… gonna riff on what you said about avoiding overstimulation, because the topic I was going to lay out there.

00:44:46.000 --> 00:44:54.000
is how I am feeling completely. Absolutely. Intellectually, politically.

00:44:54.000 --> 00:45:02.000
overstimulated. Um, this is partly because I spent Saturday at TEDx Mid-Atlantic.

00:45:02.000 --> 00:45:11.000
And we had some very powerful talks by everyone from Joe Trippi to Michael McFaul, our former ambassador to Russia.

00:45:11.000 --> 00:45:19.000
to a rapper in DC who has sort of reinvented the business model.

00:45:19.000 --> 00:45:25.000
he… he signed up with a major studio, he was doing cool stuff with other cool rappers.

00:45:25.000 --> 00:45:39.000
And he was going all over the country. And he said, forget this. And he started front porch. And he just… goes out on his front porch, and he gets guests to come by, and they do things, and the neighborhood loves it.

00:45:39.000 --> 00:45:44.000
And, uh, not as lucrative. But a lot less exhausting.

00:45:44.000 --> 00:45:55.000
So, just one example of some of the wild ideas that came out. But I've also just been overstimulated by the politics of what's happened in the last 3 days.

00:45:55.000 --> 00:46:00.000
Uh, here in Virginia, of course, we've gotten a lot more political advertising than most.

00:46:00.000 --> 00:46:03.000
Uh, you've all seen that one of my favorite.

00:46:03.000 --> 00:46:18.000
former congresspeople. Abigail Spanberger got elected, and we've known her personally for about 6 years, and she's very close with one of our best friends. Uh, so we're just delighted that a moral.

00:46:18.000 --> 00:46:26.000
decent, smart. kind person could… and a young person.

00:46:26.000 --> 00:46:35.000
become our new governor. Virginia's blessed. We have a… Uh, term limit system, you only get to be governor for one.

00:46:35.000 --> 00:46:40.000
4-year term, and as a result, there's a lot of focus on building.

00:46:40.000 --> 00:46:52.000
talent, and being sure there's several choices. Um… But we won the governorship, lieutenant governor, and Attorney General, which is going to be incredibly important.

00:46:52.000 --> 00:46:54.000
Given all the lawsuits that we're going to have to file.

00:46:54.000 --> 00:47:04.000
against the Trump administration. But most importantly, we went from having a one-person majority for the Democrats in the legislature.

00:47:04.000 --> 00:47:08.000
to having a 14. seat.

00:47:08.000 --> 00:47:15.000
Majority. So it's, you know, like. 52% to… 61%, something like that?

00:47:15.000 --> 00:47:26.000
Uh, and around the country, we had other. very impressive landslide victories. So, it's clear that people are fed up with what Mr. Trump has been doing.

00:47:26.000 --> 00:47:32.000
In some cases, they're also saying they're fed up with what the Democrats are doing, but they know that the Democrats are.

00:47:32.000 --> 00:47:38.000
unable to get organized, and therefore unable to do as much damage as Trump and his minions.

00:47:38.000 --> 00:47:46.000
Um. I'm also tracking a lot of technology developments around the world.

00:47:46.000 --> 00:47:50.000
And that's, again, overstimulating. There's just too much going on.

00:47:50.000 --> 00:47:56.000
The Chinese are… pushing forward a digital identity system.

00:47:56.000 --> 00:48:00.000
Which they're going to try to get other… countries, or at least.

00:48:00.000 --> 00:48:12.000
multinationals to use. Um… Clearly, that's going to have surveillance built in, so you're not going to be able to go anonymously from service to service on the internet.

00:48:12.000 --> 00:48:21.000
Uh, the Internet Engineering Task Force was meeting. This week, and one of the most lively debates was about.

00:48:21.000 --> 00:48:30.000
a new authentication system for. um, digital transactions. And also.

00:48:30.000 --> 00:48:39.000
a tool that would allow a website. to prevent people from coming and scraping all the content on the website.

00:48:39.000 --> 00:48:42.000
And this led to a lot of interest and discussions about, you know, well.

00:48:42.000 --> 00:48:57.000
Can McDonald's use this to make sure that Burger King can't go on its website and find out what its prices are, or what its sales are by… by store? I mean, it's really… it's… it's a good thing.

00:48:57.000 --> 00:49:03.000
Because you do have these companies, these bots, that are scraping content and looking for.

00:49:03.000 --> 00:49:11.000
a stray social security number, then they're just putting a huge demand on all these websites, including non-profit ones.

00:49:11.000 --> 00:49:17.000
So, but how do you stop a bad person from abusing the openness of the internet?

00:49:17.000 --> 00:49:22.000
While allowing a researcher. to span, uh.

00:49:22.000 --> 00:49:30.000
100,000 websites, and go search through. terabytes of data.

00:49:30.000 --> 00:49:40.000
Tough, tough, tough issues, and we're… we're having all these… discussions, but they're all kind of below the radar, because we're all fixated on whether AI will.

00:49:40.000 --> 00:49:46.000
destroy world civilization. So, I've been overstimulated, I don't know if I've stimulated you.

00:49:46.000 --> 00:49:57.000
But, um, uh, I am intrigued by your… experience, and uh… this promise of… finding a way to turn off the brain a little bit.

00:49:57.000 --> 00:50:02.000
Um, I usually do that by going for long bike rides, which is my next, uh.

00:50:02.000 --> 00:50:05.000
my next thing, and I… I have to say.

00:50:05.000 --> 00:50:12.000
We have had more than a week of spectacular weather. It is peak, peak, peak fall foliage here in Washington.

00:50:12.000 --> 00:50:18.000
Uh, and I'm trying to enjoy that. being semi-retired, I can enjoy that.

00:50:18.000 --> 00:50:21.000
But, yeah, there's just… there's so much going on.

00:50:21.000 --> 00:50:27.000
And I'd, uh, urge you to check out the Carnegie Endowment website.

00:50:27.000 --> 00:50:32.000
the… some of the sessions that we've had over the last two weeks on everything from.

00:50:32.000 --> 00:50:37.000
information ecology. A book by one of my new co- by one of my colleagues.

00:50:37.000 --> 00:50:41.000
to a session I just listened to on the radical right.

00:50:41.000 --> 00:50:49.000
Um, and the New World Order. I mean, it's just… A lot of big thinking going on, a lot of change happening, which, of course.

00:50:49.000 --> 00:50:54.000
could mean that we could have. an improvement!

00:50:54.000 --> 00:51:00.000
Nelson's Law of Life is that nothing. improves unless it changes.

00:51:00.000 --> 00:51:05.000
Murphy's corollary to Nelson's Law is that not every change is an improvement.

00:51:05.000 --> 00:51:09.000
But I'm gonna try to help push people in the direction of improvement.

00:51:09.000 --> 00:51:28.000
Thank you. Long, long, uh, somewhat optimistic. Um, screed.

00:51:28.000 --> 00:51:33.000
I really don't know what to say after all your tickings.

00:51:33.000 --> 00:51:43.000
Because I have just been writing my novel. researching about how people seek certainty.

00:51:43.000 --> 00:51:50.000
Um, attending a workshop? to which now I have a session, so I will have to leave early.

00:51:50.000 --> 00:51:59.000
In which I am observing Akira Sync. If people doesn't have curiosity, even if they are adults.

00:51:59.000 --> 00:52:05.000
willingly, uh… engaged in a workshop.

00:52:05.000 --> 00:52:20.000
they say they are interested in learning something. But in the end, when… in the community side, I post, uh… Some questions, or some things to trigger conversations?

00:52:20.000 --> 00:52:29.000
Nobody responds. Anas… A former, uh.

00:52:29.000 --> 00:52:36.000
what's the name? Because sometimes. I… I have been designing courses for.

00:52:36.000 --> 00:52:41.000
companies and everything, so… For me, it's incredible how.

00:52:41.000 --> 00:52:52.000
Curiosity is dropping in our gold. So this is one of the things I'm also, like, taking notes about and trying to understand.

00:52:52.000 --> 00:53:00.000
But I'm not as engaged as you are in all those great things and experiences.

00:53:00.000 --> 00:53:07.000
So that's my little check-in.

00:53:07.000 --> 00:53:11.000
Can I… can I use my check-in to answer Victoria?

00:53:11.000 --> 00:53:12.000
Since I didn't check in. I can't… okay.

00:53:12.000 --> 00:53:22.000
No. Although Victoria's gonna drop off the call at the top of the hour, so she won't hear you if you do it later, so yes.

00:53:22.000 --> 00:53:23.000
Yeah.

00:53:23.000 --> 00:53:29.000
Okay, thank you for rethinking. I would love to answer your question about how I approach certainty.

00:53:29.000 --> 00:53:35.000
If you're interested. Because I've been… I'm really clear on how I do it.

00:53:35.000 --> 00:53:42.000
I think of every story. as being… there's two truths and a lie in it.

00:53:42.000 --> 00:53:49.000
So I'm always… that helps me to keep an open mind, and I'm always trying to figure out which is the lie.

00:53:49.000 --> 00:53:55.000
So it's a constant process of, well, it could be that, or it could be this.

00:53:55.000 --> 00:54:01.000
And that's how I keep going every step of the way.

00:54:01.000 --> 00:54:02.000
Thanks, Stacey.

00:54:02.000 --> 00:54:04.000
So I just wanted to share that.

00:54:04.000 --> 00:54:08.000
We now return to our regular program, which is already in progress.

00:54:08.000 --> 00:54:21.000
And we have a couple people who haven't checked in yet.

00:54:21.000 --> 00:54:31.000
So I'll go next. Um… So this is sort of… Zooming way, way, way, way, way in.

00:54:31.000 --> 00:54:35.000
and zooming way, way, way out at the same time.

00:54:35.000 --> 00:54:42.000
So, at the tender age of 69, as an ADD person, my executive function is finally fully.

00:54:42.000 --> 00:54:52.000
matured. And… With that, um, I have found a… I mean, huge expansion of capacity capability.

00:54:52.000 --> 00:54:58.000
to grapple with things and play with things, and do things, and use things.

00:54:58.000 --> 00:55:03.000
Um, that, you know. were opaque and impenetrable and unusable before.

00:55:03.000 --> 00:55:17.000
Um. So, um, one of the phenomena of my daily reality up until about a week ago was… Um, 4 to 5 browser windows with 3 monitors.

00:55:17.000 --> 00:55:26.000
Uh, each one with 45 to 50 tabs. And, um, because on one hand.

00:55:26.000 --> 00:55:32.000
Um, that was my way of… having things up.

00:55:32.000 --> 00:55:37.000
That I didn't want to lose. Of course, when you have 250 tabs.

00:55:37.000 --> 00:55:46.000
that you can't see because they're Microsoft, probably small, you can't differentiate. It's not as if you can find anything that way either, but emotionally.

00:55:46.000 --> 00:55:56.000
Um, they're there. They exist. And… So, approaching your computer first thing in the morning with that.

00:55:56.000 --> 00:56:02.000
was always not fun. It was always like this overwhelming crap.

00:56:02.000 --> 00:56:11.000
Like, what do I do now? And, um… And I was thinking about, you know.

00:56:11.000 --> 00:56:17.000
browser functionalities. And I saw something that sort of flashed that I hadn't really.

00:56:17.000 --> 00:56:21.000
I knew the capability had to be in there, but I didn't know.

00:56:21.000 --> 00:56:26.000
what it was called, or where to find it, and… Then, I noticed in a menu.

00:56:26.000 --> 00:56:32.000
Something called… Tab Groups.

00:56:32.000 --> 00:56:43.000
And it was like… Okay, I think… I think, um, this is my path to… emancipation, to tab emancipation.

00:56:43.000 --> 00:56:49.000
And, um, and because of newly vested executive function, I was able to, like.

00:56:49.000 --> 00:56:56.000
open it up and learn it and master it very, very quickly, and I was able to.

00:56:56.000 --> 00:57:00.000
take those 250 tabs and… sort and bucket them into groups, and I was able to.

00:57:00.000 --> 00:57:05.000
Then close those groups, and then I was able to close the browser windows of those groups.

00:57:05.000 --> 00:57:10.000
And freed up my machine's resources, and all of a sudden, like, my computer.

00:57:10.000 --> 00:57:13.000
My poor little laptop that could, that I was asking.

00:57:13.000 --> 00:57:25.000
you know, mini computer demands on. was able to function again. And, like, things started to flow, and I could close all that stuff, and only have the one or two tab groups.

00:57:25.000 --> 00:57:34.000
up that were relevant in that moment to actually do work and not be in overwhelm.

00:57:34.000 --> 00:57:44.000
This is life-changing. for someone like me…

00:57:44.000 --> 00:57:52.000
And… pulling way, way, way, way, way out.

00:57:52.000 --> 00:58:08.000
Um. It goes under the heading in my… you know, working reality of… the… the most meaningful questions.

00:58:08.000 --> 00:58:16.000
for me, in terms of what am I… devoting my attention to what am I engaging with and why?

00:58:16.000 --> 00:58:25.000
is, uh… Um… Based on the belief that every human being.

00:58:25.000 --> 00:58:32.000
has value to add and contribute.

00:58:32.000 --> 00:58:38.000
And… the only meaningful operative question, inquiry.

00:58:38.000 --> 00:58:48.000
If you buy that. is… What do each one of those individuals need in order to.

00:58:48.000 --> 00:58:53.000
be enabled to contribute.

00:58:53.000 --> 00:59:06.000
And then the third and closing inquiry is. Based on that… how do we meet everyone's needs?

00:59:06.000 --> 00:59:11.000
And pretty much. everything else.

00:59:11.000 --> 00:59:22.000
If it is not germane to… those inquiries is… substantially irrelevant.

00:59:22.000 --> 00:59:29.000
So that's the zooming out. So, I found what I needed.

00:59:29.000 --> 00:59:35.000
in tab groups? It unleashed a flood tide of capacity.

00:59:35.000 --> 00:59:40.000
of organizing, of… moving into action.

00:59:40.000 --> 00:59:45.000
of unleashing the hounds in all sorts of ways, and now I'm going to close with.

00:59:45.000 --> 00:59:51.000
the immediate nexus that OGM represents in my world.

00:59:51.000 --> 00:59:56.000
So, what's starting to happen is I'm touching a bunch of projects that.

00:59:56.000 --> 00:59:59.000
A lot of you are familiar with, because they're.

00:59:59.000 --> 01:00:04.000
projects of OGM people, or people that have dipped in and out of here.

01:00:04.000 --> 01:00:14.000
Um, and… Um, they're starting… to cross-connect.

01:00:14.000 --> 01:00:20.000
Um, via individuals and conversations. They're starting to coalesce, where.

01:00:20.000 --> 01:00:27.000
you know, two completely independent projects have managed to connect, and all of a sudden, they're talking.

01:00:27.000 --> 01:00:33.000
And I have awareness of the one, and I have awareness of the other, and I have awareness of how they complement.

01:00:33.000 --> 01:00:44.000
And fit into a larger context. And, um… I'd say 60% of them… there's a list of about 12.

01:00:44.000 --> 01:00:53.000
60% of them all have OGM in common. So, Jerry, a half tip.

01:00:53.000 --> 01:00:59.000
Um, for… being, you know, the energetic center of gravity.

01:00:59.000 --> 01:01:05.000
out of which a lot of these things, for me.

01:01:05.000 --> 01:01:12.000
Uh, we're connected with and met, and… now coalesceable.

01:01:12.000 --> 01:01:42.000
in some fashion that I have yet to… No completely, but that I'm working on. So, anyway, with that, I'm complete.

01:01:45.000 --> 01:01:49.000
Rick, I don't know if you're just listening in, or if you want to step in.

01:01:49.000 --> 01:01:51.000
Um…

01:01:51.000 --> 01:01:55.000
But I don't want to hold up our transition to conversation.

01:01:55.000 --> 01:01:58.000
So I'm gonna pause a moment.

01:01:58.000 --> 01:02:03.000
Am I missing anybody else? Did somebody else not throw?

01:02:03.000 --> 01:02:04.000
We're all set, good. Um, there's…

01:02:04.000 --> 01:02:07.000
Yeah, I'll pass, Jerry. Go ahead.

01:02:07.000 --> 01:02:10.000
Okay, thanks, Rick.

01:02:10.000 --> 01:02:13.000
We are now officially in conversation mode.

01:02:13.000 --> 01:02:17.000
Excellent. Mike took that cue instantaneously.

01:02:17.000 --> 01:02:25.000
And put the Carnegie calendar onto the chat, which I was going to ask you to do, so fabulous, thank you so much.

01:02:25.000 --> 01:02:32.000
And, uh, anybody… we've got, like, five, uh, four hands up immediately. Okay, so Mike, your first.

01:02:32.000 --> 01:02:36.000
Well, I not only acted immediately, I jumped the gun.

01:02:36.000 --> 01:02:42.000
with my hand, but uh… one of the issues I didn't raise, and this is.

01:02:42.000 --> 01:02:51.000
Weighing on… weighing on me a great deal. Um… our library at Carnegie gets the Wall Street Journal.

01:02:51.000 --> 01:03:05.000
I used to subscribe to it. Um… but the fact that it's owned by Murdoch makes it morally ambiguous to… pay, uh, support his profits.

01:03:05.000 --> 01:03:11.000
But I do like to support the journalists, and I do try to publicize the great articles that I read.

01:03:11.000 --> 01:03:17.000
Uh, from the hard copy that we have at our library.

01:03:17.000 --> 01:03:22.000
every day for the last 3 weeks, there's been at least one article along the lines of.

01:03:22.000 --> 01:03:29.000
Oh my god, it's an AI bubble! And different versions on why it is.

01:03:29.000 --> 01:03:35.000
Uh, it's been clear to me for at least 2 years that we're on this track. It's so reminiscent of.

01:03:35.000 --> 01:03:40.000
what happened with, uh, Time Warner being bought by AOL.

01:03:40.000 --> 01:03:44.000
I mean, OpenAI is the AOL of our day.

01:03:44.000 --> 01:03:53.000
Has a completely overinflated. stock price, and… Altman's doing exactly the right thing and using it to buy all sorts of.

01:03:53.000 --> 01:04:00.000
do all sorts of deals. Which are going to not look very good in a year or two.

01:04:00.000 --> 01:04:09.000
But… this bubble is… going to deflate, either.

01:04:09.000 --> 01:04:16.000
In a few weeks, or in a few years, it's clear we're not going to maintain the kind of growth we've seen from NVIDIA.

01:04:16.000 --> 01:04:24.000
And some of these other companies. Um, and the questions I have for anybody here who has.

01:04:24.000 --> 01:04:32.000
the courage to give financial advice. Is it going to drag down just the AI-related companies?

01:04:32.000 --> 01:04:36.000
Or is it going to drag down the entire tech sector?

01:04:36.000 --> 01:04:43.000
Or is it gonna drag down the entire market? In one of the articles in the Wall Street Journal was.

01:04:43.000 --> 01:04:51.000
the U.S. Economy is now AI. made the point that all of the growth that we've seen in more than.

01:04:51.000 --> 01:05:00.000
12 months. is because of all the investment that's going into these data centers. I mean, if you just take that out.

01:05:00.000 --> 01:05:12.000
the economy's been flat. And Trump is doing everything he can to make it go down. Um, so my question… Because I was very lucky, I.

01:05:12.000 --> 01:05:19.000
was employee number 130. At Cloudflare, before it went IPO.

01:05:19.000 --> 01:05:23.000
a good chunk of my retirement portfolio is still in Cloudflare stock.

01:05:23.000 --> 01:05:31.000
And other parts of my portfolio are in. IBM stock. And other tech companies.

01:05:31.000 --> 01:05:35.000
Because I tend to invest in what I know.

01:05:35.000 --> 01:05:38.000
So I'm just curious, I mean, how many of you are thinking of.

01:05:38.000 --> 01:05:44.000
pulling out the profits. From your tech investments.

01:05:44.000 --> 01:06:01.000
That's an easy question. And if you are, what in the hell do you put the… Proceeds into. I mean, I… I just… I have no clue what a good strategy here is. I mean, the reason…

01:06:01.000 --> 01:06:11.000
Gold is up by a factor of 3, as a lot of other people are confused, too, when they figure, well, okay, I'll put it in something that will somehow have value someday.

01:06:11.000 --> 01:06:21.000
We've never… I don't… I don't remember a discussion like this, and maybe that's because people don't like to talk finances, and… people have… or because people have very good financial advisors.

01:06:21.000 --> 01:06:36.000
I have a very good financial advisor, but… you know, he… and he's got, you know, some suggestions, but I'm just curious how many of you are… thinking of strategies for the upcoming bubble.

01:06:36.000 --> 01:06:45.000
Thanks, Mike. And we're not a financially-oriented group, or that necessarily expert in this. There's probably different depths of expertise in the group on that.

01:06:45.000 --> 01:06:49.000
But it might make a good topic for a call, like, is there a bubble? What is the bubble?

01:06:49.000 --> 01:06:55.000
And… I haven't had a chance to read all the too many articles about, oh my god, there's a bubble.

01:06:55.000 --> 01:06:57.000
A couple of them are trying to unpack

01:06:57.000 --> 01:07:01.000
There could be several different bubbles going on, and here's how they

01:07:01.000 --> 01:07:04.000
fit and relate, which I find really fascinating, because

01:07:04.000 --> 01:07:09.000
I think there's clearly a bubble in a piece of it. There's, like, this frenzy to do AGI.

01:07:09.000 --> 01:07:15.000
But there's other parts that are not so bubbly, and I don't know, it's a really good, juicy, complex issue.

01:07:15.000 --> 01:07:20.000
And I haven't even addressed the, you know, do you divest all tech

01:07:20.000 --> 01:07:24.000
The problem with previous bubbles is

01:07:24.000 --> 01:07:30.000
If you heated the first warnings of, oh my god, we're in a bubble, you would miss the actual big run-up.

01:07:30.000 --> 01:07:43.000
Um, and you would sell too early, and then if you missed the fall, because usually it's the first couple people who get off the merry-go-round who profit a lot, then you lose everything. So timing is everything in these things.

01:07:43.000 --> 01:07:45.000
And it's a little nutty, so…

01:07:45.000 --> 01:07:48.000
Yeah, well, this was a… this was meant to be a two-minute discussion.

01:07:48.000 --> 01:07:49.000
Yeah.

01:07:49.000 --> 01:07:53.000
And it really is kind of, you know, put your hand up if you think there is a bubble.

01:07:53.000 --> 01:08:01.000
And then put your other hand up if you're gonna try to.

01:08:01.000 --> 01:08:02.000
Yeah.

01:08:02.000 --> 01:08:04.000
dramatically reduce your exposure in the tech… not dramatically, but significantly reduce your exposure in the tech sector.

01:08:04.000 --> 01:08:05.000
Let's do that.

01:08:05.000 --> 01:08:10.000
And if you have a one-sentence answer to where would you put your money, you can put that in the chat.

01:08:10.000 --> 01:08:15.000
Let's do that. Raise your hand if you think there's a bubble.

01:08:15.000 --> 01:08:21.000
I think there's 3 bubbles. There's one in cloud computing, one in AI chips, and then.

01:08:21.000 --> 01:08:24.000
one in cryptocurrency, which is… That's a whole other topic.

01:08:24.000 --> 01:08:33.000
cool. And then raise your hand if you're seriously thinking about disinvesting, changing your investment strategy.

01:08:33.000 --> 01:08:35.000
Cool, that's… that's right.

01:08:35.000 --> 01:08:36.000
That's…

01:08:36.000 --> 01:08:41.000
Oh, okay, three and a half. That's… that's… thank you, that's very helpful for… reducing my anxiety a little bit.

01:08:41.000 --> 01:08:44.000
And it's a great question, Mike, thank you.

01:08:44.000 --> 01:08:45.000
And is pork bellies the place I should put all my money?

01:08:45.000 --> 01:08:46.000
Thanks.

01:08:46.000 --> 01:08:52.000
No, try rye Coins. I put a Wikipedia link in the chat, it's the…

01:08:52.000 --> 01:09:01.000
The big stones they used to use, uh, as currency. I think you need to find one of those and, like, roll it around the living room as your asset.

01:09:01.000 --> 01:09:02.000
That could well be.

01:09:02.000 --> 01:09:03.000
plastics, it's the future.

01:09:03.000 --> 01:09:04.000
I thought that was the island of Yap.

01:09:04.000 --> 01:09:05.000
Yeah.

01:09:05.000 --> 01:09:06.000
Oh, wait.

01:09:06.000 --> 01:09:07.000
That… that is the island of the yacht.

01:09:07.000 --> 01:09:14.000
There's… there's also… There's also the guy, Mike, behind the big short in the real estate crash.

01:09:14.000 --> 01:09:18.000
who has a fund, who is, uh, set up to short.

01:09:18.000 --> 01:09:22.000
The tech sector. Like, just, if you want to go straight Contra.

01:09:22.000 --> 01:09:23.000
Yep.

01:09:23.000 --> 01:09:25.000
Um, like, he'd be happy to take your money.

01:09:25.000 --> 01:09:28.000
Is it Michael Burry, or…? Yeah.

01:09:28.000 --> 01:09:29.000
That is some shit, wow. Okay, so he's…

01:09:29.000 --> 01:09:30.000
Okay. Yeah. Michael Burry, and if you haven't seen… If you haven't seen the interview.

01:09:30.000 --> 01:09:33.000
Yeah, yeah. Yeah. He's back!

01:09:33.000 --> 01:09:34.000
Wow!

01:09:34.000 --> 01:09:38.000
If you haven't seen the interview with the CEO of Palantir.

01:09:38.000 --> 01:09:43.000
Karp was interviewed about the fact that Burry had shorted Palantir.

01:09:43.000 --> 01:09:44.000
A lot. he looked panicked. He's like, oh, this is really bad. Oh, this is not fair, oh!

01:09:44.000 --> 01:09:53.000
and NVIDIA.

01:09:53.000 --> 01:09:54.000
I've got…

01:09:54.000 --> 01:09:55.000
And his stock price went down 12%, so… Sorry.

01:09:55.000 --> 01:09:56.000
Deer… deer in the headlights.

01:09:56.000 --> 01:10:03.000
So I… so… so I've got a New Yorker article, uh, that's titled, Michael Burry, Real-Life Market Genius from the Big Short.

01:10:03.000 --> 01:10:05.000
thinks another financial crisis is looming, dated,

01:10:05.000 --> 01:10:08.000
2016.

01:10:08.000 --> 01:10:13.000
So he would have missed… he would have missed a decade of run-up.

01:10:13.000 --> 01:10:14.000
And I don't know when you run out of money from shorting, but you do at some point.

01:10:14.000 --> 01:10:19.000
Yep.

01:10:19.000 --> 01:10:24.000
Um, that was part of the problem with the big short. And by the way, the Big Short movie and book are among my favorite

01:10:24.000 --> 01:10:28.000
bits of media like that. They're just so insightful.

01:10:28.000 --> 01:10:29.000
Uh, Gil… Gil, please.

01:10:29.000 --> 01:10:30.000
Yep.

01:10:30.000 --> 01:10:37.000
And the guy who wrote the big short has done a series of books of similar depth and insight, really terrific. Michael somebody?

01:10:37.000 --> 01:10:38.000
Michael Lewis.

01:10:38.000 --> 01:10:39.000
Listen, Michael, this lives here in Berkeley.

01:10:39.000 --> 01:10:40.000
Michael Lewis is brilliant. He did Moneyball as well.

01:10:40.000 --> 01:10:41.000
Nicholous?

01:10:41.000 --> 01:10:49.000
Um, Moneyball and a bunch of things, yeah, brilliant, brilliant guy.

01:10:49.000 --> 01:10:50.000
Aww.

01:10:50.000 --> 01:10:53.000
But I just first want to say, what a fine bunch of humans you all are. It's been a fascinating call just listening to the quick summaries from everybody.

01:10:53.000 --> 01:10:56.000
Um, couple things, um…

01:10:56.000 --> 01:11:01.000
Mike, I think I heard you say that when to get out is obvious, and obviously it's not obvious.

01:11:01.000 --> 01:11:02.000
Uh, for the reason that Jerry said, because bubbles and FOMO are the kind of similar phenomenon.

01:11:02.000 --> 01:11:08.000
Oh, I didn't say that.

01:11:08.000 --> 01:11:15.000
You know, I mean, how can tech go higher, but you can't miss out on it going higher? Same with gold, same with Bitcoin. There's this weird psychological thing in there.

01:11:15.000 --> 01:11:17.000
Um, um…

01:11:17.000 --> 01:11:20.000
Golly, a couple of pieces here I want to tie together.

01:11:20.000 --> 01:11:23.000
Um,

01:11:23.000 --> 01:11:31.000
I've mentioned Ben Hunt before from Epsilon Theory. He's now launched a new site called Panoptica.ai, which I encourage people to take a look at.

01:11:31.000 --> 01:11:41.000
Um, Ben and his partner, Rusty Gwynn have been tracking narratives globally. They've built AIs that scan, I don't know, 8… Alex, this is to your point, about media scans.

01:11:41.000 --> 01:11:53.000
Scanning 8,000 sites around the world on a regular basis, and distilling the trends of what's being talked about. So they have multicolored, complex graphs showing the rise and fall.

01:11:53.000 --> 01:12:02.000
of conversations about things like bubbles, or bubbles in this or that, recession, interest rates, and so forth. It's a fascinating meta-view.

01:12:02.000 --> 01:12:14.000
on the stories to power the world. Ben also runs an asset management company called Second Foundation Partners. He is a confessed longtime short.

01:12:14.000 --> 01:12:21.000
shorted the real estate bubble back in 2008, and uh… so, you know, can somebody be a short for a long time? Maybe so.

01:12:21.000 --> 01:12:32.000
The Panoptica site also has got a wealth of articles and analysis and so forth, really fascinating stuff, much of it very OGM-y, so…

01:12:32.000 --> 01:12:37.000
Have a look at that. Um, on the, um…

01:12:37.000 --> 01:12:47.000
On the filters in the new scan, just to note that not just Panoptica, but our very own Marshall Kirkpatrick has been building a system that does that, focused around sustainability and regeneration.

01:12:47.000 --> 01:12:54.000
Uh, doing scans of trends, building products for companies around that, some very interesting stuff in the workbench there.

01:12:54.000 --> 01:12:59.000
Um, Marshall's rarely on these calls, but he's up to some interesting stuff.

01:12:59.000 --> 01:13:02.000
on that. And, um…

01:13:02.000 --> 01:13:08.000
The other thing I want to say about AI is that I was, uh, spent Tuesday at Capgemini, the French, uh,

01:13:08.000 --> 01:13:11.000
consultancy have been working with them on their net positive.

01:13:11.000 --> 01:13:18.000
Excuse me, net positive projects, which the company appears to have made a major commitment to.

01:13:18.000 --> 01:13:20.000
We've just gone through the second year.

01:13:20.000 --> 01:13:32.000
of training each year, like a cohort of 100 Hispanic-orient… Hispanic background, um, managers and executives in sustainability and net positive. Fascinating process.

01:13:32.000 --> 01:13:34.000
Um, um, um…

01:13:34.000 --> 01:13:36.000
planting seeds in…

01:13:36.000 --> 01:13:40.000
Hundreds of companies about that.

01:13:40.000 --> 01:13:43.000
But Tuesday, we had a presentation from a guy named Andres Sjoborn,

01:13:43.000 --> 01:13:49.000
Uh, who was, I think, their lead, or one of their leads on AI, and it was a fascinating hour-long overview.

01:13:49.000 --> 01:13:55.000
of the field, and how he sees it, and the kind of integrations that companies are doing.

01:13:55.000 --> 01:14:02.000
Uh, in applications of AI. I've asked him for the slides, I don't know if they're publicly available. If I do get them, I'll share them here.

01:14:02.000 --> 01:14:08.000
Um, but might be somebody to track. And, um…

01:14:08.000 --> 01:14:21.000
One other thing here, um, two other things here. Um, David, uh, really intrigued to hear your discussion about growth and where… and your evolution around the conversation that you're in.

01:14:21.000 --> 01:14:26.000
You and I have intersected on a little bit of, and I'd love to talk with you more about that.

01:14:26.000 --> 01:14:33.000
Um, because I think as we go into… when we look at the biological roots of the world that we live in,

01:14:33.000 --> 01:14:36.000
Um…

01:14:36.000 --> 01:14:38.000
A lot of them are human stories don't hold up very well.

01:14:38.000 --> 01:14:42.000
I'll just leave it at that for now and talk more about that another time.

01:14:42.000 --> 01:14:46.000
Even… well, I'll say one other thing. Even, like, our current…

01:14:46.000 --> 01:14:51.000
widespread irateness about what we call corruption.

01:14:51.000 --> 01:14:58.000
Which maybe is not that different from predator-prey relationships and parasite relationships, and the things that living systems do.

01:14:58.000 --> 01:15:00.000
Just as kind of part of the game.

01:15:00.000 --> 01:15:04.000
You know, there's a striving for advantage, and there's a countervailing.

01:15:04.000 --> 01:15:09.000
Opposition to the advantage, and that may just be what it is. And so, not to say we shouldn't resist corruption,

01:15:09.000 --> 01:15:13.000
But we maybe should not be quite as surprised.

01:15:13.000 --> 01:15:17.000
and outraged, and hopeful of a world without it.

01:15:17.000 --> 01:15:24.000
Um, given that. And the other thing I just wanted to comment on, from the top of the call, Scott, I was fascinated by what you talked about.

01:15:24.000 --> 01:15:31.000
Um, about the, um, you know, Kevin McLeod and being the most widespread is… do we lose Scott?

01:15:31.000 --> 01:15:34.000
Oh gosh, too bad.

01:15:34.000 --> 01:15:35.000
Okay.

01:15:35.000 --> 01:15:36.000
No, Scott's still on the call. He's muted and no video, but I think he's listening.

01:15:36.000 --> 01:15:38.000
He's hidden… I'm not seeing the video, okay.

01:15:38.000 --> 01:15:39.000
Yep.

01:15:39.000 --> 01:15:41.000
So, um, um…

01:15:41.000 --> 01:15:48.000
It just echoed for me the line from whoever it was, somebody here knows about it's amazing what you can get done if you're not concerned with getting credit for it.

01:15:48.000 --> 01:15:49.000
Yep.

01:15:49.000 --> 01:15:51.000
And so I was thinking about… about…

01:15:51.000 --> 01:16:04.000
Kevin McCloud and Scott and Scott's deck and other decks that I've seen other people do, and the thought about, you know, revolutionary Legos.

01:16:04.000 --> 01:16:07.000
like, building… building the pieces of the puzzle and just…

01:16:07.000 --> 01:16:12.000
throwing them out there in a way where they self-propagate, like, apparently, Kevin McLeod's been able to do.

01:16:12.000 --> 01:16:20.000
Rather than trying to build complete strategies, but to feed the development of multiple strategies around the world. It's just an intriguing idea.

01:16:20.000 --> 01:16:26.000
Um, that resonated very powerfully from what Scott said. So, Scott, thank you for that.

01:16:26.000 --> 01:16:30.000
And I'll be chewing on that for a while, I'm complete.

01:16:30.000 --> 01:16:36.000
Thank you. There's a bunch of stuff. Pass, please.

01:16:36.000 --> 01:16:41.000
Yeah, I mean, it's also AI-related, but I'm… I've been running into.

01:16:41.000 --> 01:16:51.000
a really interesting phenomena, where. If every part of the economy is working to integrate AI into their flow.

01:16:51.000 --> 01:17:02.000
But when you are talking with NGOs, and uh… uh, you know, particularly in my food and agriculture space.

01:17:02.000 --> 01:17:08.000
Okay, it's almost an emotional rejection of wanting to deal with AI.

01:17:08.000 --> 01:17:15.000
And the reasons given is it uses too much electricity, is the first thing you hear, it uses water.

01:17:15.000 --> 01:17:20.000
Um, but there is the… there has been… and I'm now thinking.

01:17:20.000 --> 01:17:27.000
you know, how can something like this penetrate so effectively into an entire sector?

01:17:27.000 --> 01:17:32.000
Yeah, because the, the, uh… these opinions are being held.

01:17:32.000 --> 01:17:41.000
pretty universally, and it disempowers non-profit groups. from engaging with this technology.

01:17:41.000 --> 01:17:53.000
Which is so powerful, you know? I mean, I can do a search, I mean, uh, David just, uh… asked a question about the Bill Gates Foundation. It took me, like, 5 seconds to find this.

01:17:53.000 --> 01:17:59.000
Not because my AI is customized into. into these particular topics.

01:17:59.000 --> 01:18:06.000
So we now have a relationship with the Schumacher Institute and George Pooh.

01:18:06.000 --> 01:18:16.000
is taking it on. to work with the open food, uh, the open global food system. It's a non-profit group, operates in 20 countries.

01:18:16.000 --> 01:18:23.000
And they're working with food hubs now and they're in this small CSA farmer's markets.

01:18:23.000 --> 01:18:28.000
type of environment, and we are going to track them into the AI world.

01:18:28.000 --> 01:18:34.000
Um, and there's two objectives. One is to get them into wholesale markets.

01:18:34.000 --> 01:18:36.000
And the other one is to get farmers paid.

01:18:36.000 --> 01:18:43.000
for environmental services. You know, and I already know it can be done. I mean, I've done the basic research.

01:18:43.000 --> 01:18:53.000
with the AI to figure this out. And here in Oregon, the Oregon… community foundation, which is a $20 billion foundation.

01:18:53.000 --> 01:18:59.000
has also agreed to work with me, and so I have a meeting next week where.

01:18:59.000 --> 01:19:08.000
the director, uh, you know, wants to… set up a conversation with a catering organization, the hospital co-op.

01:19:08.000 --> 01:19:19.000
catering director, and see how could we do this. And so it's just to… to make this, you know, incredible technology available.

01:19:19.000 --> 01:19:26.000
You know, to support these industries. And to build a bridge. I mean, you really have to see yourself as a translation service.

01:19:26.000 --> 01:19:30.000
You know, because all the knowledge is already out there.

01:19:30.000 --> 01:19:36.000
But to track these groups in. And I must say, the effectiveness.

01:19:36.000 --> 01:19:42.000
with which NGOs have been neutralized from using AI and working with it.

01:19:42.000 --> 01:19:46.000
It's just stunning. You know, I don't know who came up with this idea.

01:19:46.000 --> 01:19:52.000
of, uh, of, uh, emphasizing this discussion around, uh.

01:19:52.000 --> 01:20:04.000
Don't use it, it's bad for the environment. Uh, but it sure worked, and it's taking… a lot of effort to overcome that rejection.

01:20:04.000 --> 01:20:07.000
Thanks, Klaus. Uh, Rick, please.

01:20:07.000 --> 01:20:15.000
Uh, maybe bounce off of that, because I've had conversations with academics who have this anti-AI mindset.

01:20:15.000 --> 01:20:20.000
Um, for the environmental, et cetera, et cetera reasons, and I just had one with a colleague of mine who's a…

01:20:20.000 --> 01:20:23.000
a professor of global studies at UNC.

01:20:23.000 --> 01:20:28.000
about this very issue. He talked about that last… our last breakfast coffee meeting.

01:20:28.000 --> 01:20:31.000
Um, and one of the things I'm… I'm…

01:20:31.000 --> 01:20:35.000
I'm absolutely fascinated with, and I'm trying to develop my own skills.

01:20:35.000 --> 01:20:38.000
is the whole notion of co-intelligence.

01:20:38.000 --> 01:20:44.000
Not just from an individual level, but from a collective perspective, which, you know, touches on Klaus's point.

01:20:44.000 --> 01:20:47.000
Because what you're able to do and achieve

01:20:47.000 --> 01:20:52.000
is mind-blowing. I'm just blown away by what you can do.

01:20:52.000 --> 01:20:58.000
Um, but what I've been working on for some time, and I've dropped it here and there in different places,

01:20:58.000 --> 01:21:03.000
is trying… and I mentioned this phrase before, and I remember Mike saying, keep it simple.

01:21:03.000 --> 01:21:06.000
Uh, about, uh, HAN, which is…

01:21:06.000 --> 01:21:08.000
you know, humanist-guided,

01:21:08.000 --> 01:21:11.000
AI-enabled.

01:21:11.000 --> 01:21:15.000
Emancipatory neolarning. And I've made some little vignettes

01:21:15.000 --> 01:21:18.000
explaining that, what Hen means.

01:21:18.000 --> 01:21:23.000
And, you know, we live in this incredible culture of indoctrination.

01:21:23.000 --> 01:21:25.000
And it's so pervasive.

01:21:25.000 --> 01:21:31.000
that, uh, the point that you put in your little chat there, Mike, about curiosity, I mean, to me,

01:21:31.000 --> 01:21:34.000
Curiosity is the most fundamental skill

01:21:34.000 --> 01:21:36.000
An inquiry skills.

01:21:36.000 --> 01:21:39.000
that we need to have to be able to, um…

01:21:39.000 --> 01:21:44.000
mitigate against the pervasive forces of neoliberalism, reductionist,

01:21:44.000 --> 01:21:47.000
indoctrination Yard.

01:21:47.000 --> 01:21:49.000
And I'll just share something, which is…

01:21:49.000 --> 01:21:53.000
Still work in progress, um, and it's about how to create

01:21:53.000 --> 01:21:58.000
emancipatory learning communities, and developing learning assets

01:21:58.000 --> 01:22:04.000
that stimulates people's curiosities and inquiries to ask even better questions and more complex questions.

01:22:04.000 --> 01:22:06.000
or compound questions, I should say.

01:22:06.000 --> 01:22:11.000
And that's the beauty about AI, is that you can… I mean, it, you know…

01:22:11.000 --> 01:22:15.000
We have a reductionist mindset to questioning, and we frame it, and we… whatever, well…

01:22:15.000 --> 01:22:22.000
In a compound philosophical question, you can put everything in the kitchen sink in there and see what comes out, and then you can go back and see patterns that you would

01:22:22.000 --> 01:22:26.000
or even come across a new concept you've never heard of before, and we've all done that sort of stuff.

01:22:26.000 --> 01:22:29.000
And the question is, how can we create

01:22:29.000 --> 01:22:33.000
Um, you know, an environment where people can learn how to become

01:22:33.000 --> 01:22:39.000
open-minded, true-seeking, virtuous freethinkers, so that we can mitigate against all the propaganda that we're

01:22:39.000 --> 01:22:44.000
bombarded with on a regular basis. Anyway, I'll share something in the chat.

01:22:44.000 --> 01:22:48.000
For those who are curious about it. It's still a work in progress, and I still feel like

01:22:48.000 --> 01:22:52.000
I've got some ways to go in terms of how to create learning communities around this.

01:22:52.000 --> 01:22:54.000
Which are, um…

01:22:54.000 --> 01:22:57.000
designed to be more, uh, group-centered.

01:22:57.000 --> 01:23:03.000
And based on principles of sociocracy and whatever. So I'll put it in the chat, so…

01:23:03.000 --> 01:23:08.000
Um, I just think the opportunities for co-intelligence is just…

01:23:08.000 --> 01:23:12.000
You know, CloudSight, I concur with you.

01:23:12.000 --> 01:23:17.000
Let's up our skills. How can we… how can we learn from each other more effectively about how to do that, so…

01:23:17.000 --> 01:23:19.000
Thanks.

01:23:19.000 --> 01:23:25.000
Thanks, Rick. Uh, Alex.

01:23:25.000 --> 01:23:30.000
Right, Mike, I'll make it easy for you, for your investment conundrum.

01:23:30.000 --> 01:23:33.000
And it goes like this. I'm the world's worst.

01:23:33.000 --> 01:23:41.000
possible investor. I have always. sold when I should have bought, and… bought what I should have sold.

01:23:41.000 --> 01:23:49.000
So, a few years back, and quite a few years back, I made the really great decision to keep 40% of my money in cash.

01:23:49.000 --> 01:23:58.000
pure cash, because obviously the market's gonna crash. And it's still there. And I haven't yet bought into the market.

01:23:58.000 --> 01:24:03.000
But as soon as I'm tempted and I buy the market.

01:24:03.000 --> 01:24:07.000
I will actually let all of you know that I've done it, because that is a time to get out of it.

01:24:07.000 --> 01:24:10.000
But so far, I have not done that.

01:24:10.000 --> 01:24:18.000
But the critical question is, is it… cash in dollars, or cash in Swiss francs?

01:24:18.000 --> 01:24:22.000
It's cash… it's cash that's losing its value wherever you are.

01:24:22.000 --> 01:24:25.000
Except… except if you'd put it in Swiss francs, we were just in Switzerland.

01:24:25.000 --> 01:24:26.000
Except, yes, no, I didn't put it there. I didn't put it there.

01:24:26.000 --> 01:24:37.000
Yeah. Well, thank you, Alex. This is what… the reason I ask is I am in the same category you are. I have… You know, I have sold… sold shares that, you know, went on to increase in value by a factor of 6.

01:24:37.000 --> 01:24:45.000
Yeah. As if your nightmare stories behind that, but so… so that was… I just wanted to point that out.

01:24:45.000 --> 01:24:49.000
The other thing is, Pete, I'm interested in the sheath methodology.

01:24:49.000 --> 01:25:01.000
I'd love to get in touch with you. And one more, someone said something… oh, and Doug, a long time ago, a year ago, we spoke about a game you wanted?

01:25:01.000 --> 01:25:07.000
Do you remember that conversation? Let's touch… let's get together, because… I can show you how to do it yourself.

01:25:07.000 --> 01:25:11.000
Given all these AI things that we've spoken about.

01:25:11.000 --> 01:25:15.000
You won't even have to… you don't even need me, just… I just need to show you.

01:25:15.000 --> 01:25:23.000
But Pete Cannon, everybody can, really. So that's it. Thank you.

01:25:23.000 --> 01:25:26.000
Um, thank you much, Pete.

01:25:26.000 --> 01:25:32.000
Real quick, I put some of it in the chat, and I feel like I didn't express it very well, but…

01:25:32.000 --> 01:25:42.000
Mike, your question is really interesting, um, about whether we're in a bubble. I think we've been in a bubble since about the PC era, more or less. I don't… it's one big tech bubble inflation thing going on.

01:25:42.000 --> 01:25:48.000
It goes up and down, but the whole thing is going big and changing the world, and changing us.

01:25:48.000 --> 01:25:52.000
Um, AI feels like some kind of endgame.

01:25:52.000 --> 01:26:00.000
Um, either it's gonna usher in, you know, amazing productivity and social justice, and everything's gonna be wonderful, or…

01:26:00.000 --> 01:26:11.000
The whole thing's gonna crash hard, because we've turned all… all matter into GPUs, or something, right? I think what's really going to happen is some weird thing in the middle,

01:26:11.000 --> 01:26:14.000
But it feels…

01:26:14.000 --> 01:26:19.000
it's hard to say this out loud to professional people who care, but it feels like we're kind of

01:26:19.000 --> 01:26:26.000
post, you know, what do I do with the market? Do I do cash? Do I do gold? Do I do Bitcoin? You know, all of that.

01:26:26.000 --> 01:26:33.000
All of that seems like the wrong questions at this point. So, when we said, are we in a bubble, and I go like this, it's like,

01:26:33.000 --> 01:26:36.000
We're… we're past…

01:26:36.000 --> 01:26:41.000
where we've ever even thought about bubbles, where they go. You know, it's… we're in weird territory here.

01:26:41.000 --> 01:26:46.000
So, I think that's kind of what to… what to discuss.

01:26:46.000 --> 01:26:51.000
We're sort of in too big to fail territory, and I've seen a couple articles there, too.

01:26:51.000 --> 01:26:54.000
Which is like, this has eaten so much of the economy that…

01:26:54.000 --> 01:26:57.000
This is called the government put, or something like that.

01:26:57.000 --> 01:27:01.000
Where you get to a scale where you can just keep

01:27:01.000 --> 01:27:05.000
Plowing ahead confidently because the government's gonna backstop you somehow.

01:27:05.000 --> 01:27:06.000
And you'll wind up okay.

01:27:06.000 --> 01:27:11.000
And now… and now we put AI instead of, you know, AI is going to backstop you. Will it? Won't it?

01:27:11.000 --> 01:27:19.000
Um, uh, maybe that will be great, but we won't be able to eat, because, as Klaus has told us, you know, um, we're… we're

01:27:19.000 --> 01:27:25.000
we need to figure out fast how to apply AI to actually growing things and delivering food.

01:27:25.000 --> 01:27:31.000
And making soil regenerative, and all that stuff. So, it's a weird place.

01:27:31.000 --> 01:27:33.000
Absolutely. Um, thank you.

01:27:33.000 --> 01:27:38.000
Gil, you may have the last word. Actually, no, Doug's gonna have the last word, so go ahead, Gil.

01:27:38.000 --> 01:27:41.000
Well, let me try to be fast so there's room for Doug. So, um…

01:27:41.000 --> 01:27:50.000
Golly, I had a thing to say, but Pete, what you just said, it's not like we need AI to figure out how to grow food regeneratively, we actually know how to do that.

01:27:50.000 --> 01:27:55.000
And many of our problems are not technology problems, they're social organization and ownership problems.

01:27:55.000 --> 01:27:58.000
Um, but that's a conversation for another time.

01:27:58.000 --> 01:28:00.000
Um…

01:28:00.000 --> 01:28:07.000
AI and bubbles, I mean, it's not one big bubble, it's a series of bubbles. Some of them have stabilized, some of them have not.

01:28:07.000 --> 01:28:14.000
Um, the backstopping, uh, you know, in 2008, the government bailed out some of the banks and not others.

01:28:14.000 --> 01:28:18.000
Um, this is the whole too-big-to-fail game, and we were in another one of those now.

01:28:18.000 --> 01:28:24.000
Jerry, to your point, the numbers I hear from Ben Hunt are that, um…

01:28:24.000 --> 01:28:27.000
The tech sector, which is about 5% of GDP,

01:28:27.000 --> 01:28:32.000
is now pouring more money into the economy than the consumer sector, which is about 30% of the GDP.

01:28:32.000 --> 01:28:36.000
And that's the only thing keeping the economy afloat at this point.

01:28:36.000 --> 01:28:37.000
And so, yeah, will something…

01:28:37.000 --> 01:28:42.000
And masking the effects of tariffs, or whatever else might be happening is totally swamping all that other stuff.

01:28:42.000 --> 01:28:46.000
For the moment, and so… and so that's a… that's, you know, somebody's on a pump.

01:28:46.000 --> 01:28:47.000
Yeah.

01:28:47.000 --> 01:28:49.000
Pumping that bubble to keep that afloat. There's that.

01:28:49.000 --> 01:28:51.000
Um, um, um…

01:28:51.000 --> 01:28:54.000
On the data center, um…

01:28:54.000 --> 01:28:57.000
footprint question, which you've talked about a bunch before.

01:28:57.000 --> 01:29:05.000
Uh, it's not just turning AI to solve the problems of agriculture, we're turning AI to design the data centers of the future, and it turns out that Microsoft has been doing that over the last

01:29:05.000 --> 01:29:13.000
Eight years or so. There's a book out just now by Joanne Garvin called, um, Innovation at Microsoft. She led the Data Center of the Future project.

01:29:13.000 --> 01:29:18.000
And they've designed data centers that are net-zero energy and net-zero water.

01:29:18.000 --> 01:29:24.000
Are they building them? Doesn't seem to be, and I don't know if anybody is yet, but at least there's that as a theoretical possibility.

01:29:24.000 --> 01:29:29.000
Not just for data centers, but potentially for everything, if we chose to go that way.

01:29:29.000 --> 01:29:33.000
But again, it's not, can we do it technically, it's will we do it?

01:29:33.000 --> 01:29:36.000
politically, economically, financially, and so forth.

01:29:36.000 --> 01:29:43.000
Um, last, back to Mike's question about where do I move the money if I'm taking it out of Tech and Golden Bitcoin and so forth?

01:29:43.000 --> 01:29:50.000
Uh, my financial advisor and just about every smart financial… well, who I'm calling a smart financial advisor, if I've spoken to,

01:29:50.000 --> 01:29:56.000
says, find your asset allocation strategy and stay there.

01:29:56.000 --> 01:30:01.000
And I periodically go back to my guy and say, well, like, shouldn't we be moving some money here? He says, no. No. Just…

01:30:01.000 --> 01:30:07.000
Allocate and stay there and rebalance periodically, and that's the way that you move tech profits into other sectors.

01:30:07.000 --> 01:30:12.000
The one place where I'm disagreeing with him, I think, is that, um…

01:30:12.000 --> 01:30:15.000
Um, I think we should be much more offshore.

01:30:15.000 --> 01:30:24.000
than we are now. Um, you know, diversify out of the United States into international much more than we are now, just because of the instability of here.

01:30:24.000 --> 01:30:29.000
So that's, you know, that's what I'm thinking about it.

01:30:29.000 --> 01:30:31.000
We've had this group's got nothing to say.

01:30:31.000 --> 01:30:37.000
I just… I just drove Mike off the screen by saying that, oh my god.

01:30:37.000 --> 01:30:38.000
There is.

01:30:38.000 --> 01:30:41.000
No, no, no, uh, Doug, your hand is down, but that might have been accidental.

01:30:41.000 --> 01:30:48.000
I… I just… I just wanted to throw in… And this is empirical.

01:30:48.000 --> 01:30:53.000
with the rate at which… Energy, technology, battery technology.

01:30:53.000 --> 01:30:58.000
Quantum computing and the rest are. progressing.

01:30:58.000 --> 01:31:10.000
these massive billion-dollar data center. investments… are obsolete in 18 months.

01:31:10.000 --> 01:31:18.000
How does it make sense? like, just on a intuitive, logical level.

01:31:18.000 --> 01:31:23.000
It doesn't make sense. it's gotta blow up.

01:31:23.000 --> 01:31:29.000
You know, when it's too good to be true, it just doesn't… it defies reality.

01:31:29.000 --> 01:31:36.000
And when… the biggest players in the world are.

01:31:36.000 --> 01:31:44.000
canning $30,000, $40,000, 50,000 people. to divert resource to constructing data centers.

01:31:44.000 --> 01:31:48.000
That's the end. That's not the beginning of the end. That's not a bubble.

01:31:48.000 --> 01:31:54.000
That's the end of whatever this has been. It's sort of reached its insane.

01:31:54.000 --> 01:32:01.000
logical conclusion. as a… as a capitalist.

01:32:01.000 --> 01:32:02.000
But we only need $7 trillion, and we can push right through that little eye of the needle and get into the singularity. Don't you want that?

01:32:02.000 --> 01:32:09.000
you know, train run amok. So…

01:32:09.000 --> 01:32:16.000
So… so, yeah, I don't… I don't know what's after that, but I do know what's ending.

01:32:16.000 --> 01:32:21.000
Thank God. Like, it needed to go away.

01:32:21.000 --> 01:32:25.000
we need to do us differently and better if we're not going to go extinct.

01:32:25.000 --> 01:32:26.000
this conversation… this conversation does not want to go extinct either. Go ahead, Alex.

01:32:26.000 --> 01:32:34.000
Jerry, can I just come… Sorry, Doug mentioned a couple of things, it was really interesting.

01:32:34.000 --> 01:32:53.000
So, I was… in the throes of the, um… blockchain… temperature 2015 or thereabouts. Nvidia shares skyrocketed because they were making the cars that were being used by the… the people that were mining Bitcoin and all the good things.

01:32:53.000 --> 01:32:59.000
And the prices went up. But what actually happened, Doug, and this is interesting what you said.

01:32:59.000 --> 01:33:05.000
is that the technology was moving so quickly that the cards, the graphics cards they were producing.

01:33:05.000 --> 01:33:12.000
became obsolete, exactly as you said. Within 18 months. So what happened, there was a backwash in the market.

01:33:12.000 --> 01:33:20.000
very good cards. Which were useful for games. So there was a second-hand market whilst they were building the latest ones.

01:33:20.000 --> 01:33:24.000
So, so it's just reminded me of this situation where.

01:33:24.000 --> 01:33:31.000
all these data centers that have become obsolete, maybe that can… that will produce cheap.

01:33:31.000 --> 01:33:40.000
alternative computing power for something. What that something is? I have no idea, but I just thought I'd… mention it.

01:33:40.000 --> 01:33:43.000
Thank you very much.

01:33:43.000 --> 01:33:46.000
Okay, let's take a deep breath.

01:33:46.000 --> 01:33:53.000
And then, uh, we can close out this call. We have several different topics proposed for future calls, really juicy ones, so…

01:33:53.000 --> 01:33:57.000
I'll pick from among those, and uh…

01:33:57.000 --> 01:34:00.000
Alright, thank you very much. This has been…

01:34:00.000 --> 01:34:04.000
Delightful.

01:34:04.000 --> 01:34:11.000
Let's be careful out there.

