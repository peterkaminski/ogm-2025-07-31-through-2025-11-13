WEBVTT

1
00:00:02.210 --> 00:00:08.869
Jerry Michalski: This is the Open Global Mind Community Call for Thursday, August 21st, 2025.

2
00:00:09.850 --> 00:00:12.729
Jerry Michalski: We had a very nice call last week.

3
00:00:13.890 --> 00:00:20.490
Jerry Michalski: About, kind of, reliability of sources. Like, what do… what sources do we trust? How do we know what information to trust?

4
00:00:20.750 --> 00:00:31.869
Jerry Michalski: And, today's topic is kind of along the same line, and was sparked in part by, Ted Joya, who sent

5
00:00:32.290 --> 00:00:38.549
Jerry Michalski: a post I'm going to put in the chat right now called, Assured Reality Will Self-Destruct in the Next 12 Months.

6
00:00:39.230 --> 00:00:44.130
Jerry Michalski: And Ted Julia has a substack called The Honest Broker. He's a music expert.

7
00:00:44.240 --> 00:01:00.109
Jerry Michalski: And he's been doing… in particular, the things that have gone viral for him have been his takes… his occasional takes on the music industry and the content industry, and how it's being eaten by TikTok and instant gratification and a bunch of other things.

8
00:01:00.290 --> 00:01:07.020
Jerry Michalski: And I don't know how he writes as much as he does, he's really prolific, but the writing is always high quality.

9
00:01:07.540 --> 00:01:19.759
Jerry Michalski: And in this one, he's kind of saying, hey, I used to be able to sort of tell when a song was made by a robot, you know, by AI. I'm pretty sure that boundary is blurring, and there's a whole mess of other boundaries blurring.

10
00:01:19.760 --> 00:01:23.060
Klaus Mager: And this could have some calamitous,

11
00:01:23.890 --> 00:01:25.359
Jerry Michalski: Ways of playing out.

12
00:01:26.080 --> 00:01:42.610
Jerry Michalski: consequences. And I actually haven't managed to read it all the way to the end, but that got me thinking about a lot of the things that we were talking about last week. So I figured, well, let's put a stake in the sand there and see where it takes us, because if we… if we lose…

13
00:01:42.780 --> 00:01:54.100
Jerry Michalski: So you could argue that once the major… once the three news networks that were on American TV every evening, you know, Cronkite, ABC, CBS, and NBC,

14
00:01:54.100 --> 00:02:06.950
Jerry Michalski: once those melted away into the intertubes, and a whole bunch of other things showed up, and then also it was aided by CNN and Fox and a whole bunch of other players sort of creeping in first before the internet sort of ate things.

15
00:02:07.240 --> 00:02:18.699
Jerry Michalski: and now we're kind of awash. You could argue that we've lost this shared narrative or collective experience of listening to the same people and the same news in roughly the same way. That's an interesting argument.

16
00:02:18.910 --> 00:02:36.850
Jerry Michalski: But now, I think the argument might be we're not even gonna agree on the same facts of what happened, because we won't be sure what actually happened, and there will be lots of people spinning things wildly in every direction, and there'll be lots of misdirection planted in our environments, and, and, and, and, and…

17
00:02:36.960 --> 00:02:45.290
Jerry Michalski: And this could have some pretty severe consequences. 3 things you want to say, 3 words that will solve the problem. Gil, what 3?

18
00:02:45.590 --> 00:02:49.000
Gil Friend • Sustainability OG • CxO Coach: I think there's at least 3 questions that you're raising, Jerry.

19
00:02:49.000 --> 00:02:51.789
Jerry Michalski: Excellent. Tease them out for us, will ya?

20
00:02:51.790 --> 00:02:54.380
Gil Friend • Sustainability OG • CxO Coach: Let me see if I can.

21
00:02:57.070 --> 00:03:06.500
Gil Friend • Sustainability OG • CxO Coach: … that's the problem with the brain. One is that the, … the, the, the…

22
00:03:06.840 --> 00:03:11.220
Gil Friend • Sustainability OG • CxO Coach: The three news networks were a shared conversation.

23
00:03:11.360 --> 00:03:25.269
Gil Friend • Sustainability OG • CxO Coach: It was not… excuse me, sorry, it was not facts, because even then, there were a lot of alternate views. I mean, I think of I.F. Stone, first, you know, first and foremost, who would be deconstructing the news back in the, you know, back in the early 60s.

24
00:03:25.270 --> 00:03:33.490
Gil Friend • Sustainability OG • CxO Coach: with great rigor, and say, no, these guys are not covering the whole story. They had, you know, they had 25 minutes to summarize the world, and they were very selective in what they did.

25
00:03:33.660 --> 00:03:40.310
Gil Friend • Sustainability OG • CxO Coach: And there's an iterative common narrative behind what they did that they also created, but it did provide a national coherence.

26
00:03:40.770 --> 00:03:47.800
Gil Friend • Sustainability OG • CxO Coach: for most people in… who paid attention to news at all in some kind of common story. That's number one. …

27
00:03:48.240 --> 00:03:49.190
Gil Friend • Sustainability OG • CxO Coach: …

28
00:03:50.940 --> 00:03:59.800
Gil Friend • Sustainability OG • CxO Coach: Number two, Stuart Brand was the first person I saw raise this, and I think it was in the mid-70s, when he was talking about photo…

29
00:03:59.900 --> 00:04:14.120
Gil Friend • Sustainability OG • CxO Coach: The early advent of whatever the Photoshop precursor was, was saying that, you know, photographic evidence will no longer be documentary evidence. That was his claim and prediction, I think, in 76 or something like that.

30
00:04:14.210 --> 00:04:22.429
Gil Friend • Sustainability OG • CxO Coach: And it really stuck with me, and of course, we've seen that unfold in various ways, and now what you're talking about is the more highly developed version of that.

31
00:04:22.690 --> 00:04:26.740
Gil Friend • Sustainability OG • CxO Coach: We're even, you know, any, any…

32
00:04:26.890 --> 00:04:36.160
Gil Friend • Sustainability OG • CxO Coach: Whether it's narrative, or photographic, or moving images, or music, what have you, you know, we can't tell anymore, so there's that.

33
00:04:36.500 --> 00:04:38.240
Gil Friend • Sustainability OG • CxO Coach: Excuse me.

34
00:04:38.660 --> 00:04:47.860
Gil Friend • Sustainability OG • CxO Coach: … Third is that, The cultural commitment to veracity.

35
00:04:49.990 --> 00:04:54.959
Gil Friend • Sustainability OG • CxO Coach: is gone or going. Or let's put it this way, there are a lot of people who don't give a shit about that.

36
00:04:56.230 --> 00:04:58.510
Gil Friend • Sustainability OG • CxO Coach: Where maybe they used to. And there

37
00:04:58.910 --> 00:05:11.269
Gil Friend • Sustainability OG • CxO Coach: The fourth, the fourth. And the fourth is just, you know, is into the more ontological, philosophical, which is how do we know anyway? Because we are all of us constructing, living in interpretations of the world.

38
00:05:11.380 --> 00:05:26.490
Gil Friend • Sustainability OG • CxO Coach: And even what I'm seeing on my screen right now is not what I'm seeing on my screen, it's what's… it's an experience that's assembled in my brain from photonic input on receptor cells in my eyes. It's a constructed image.

39
00:05:26.740 --> 00:05:27.240
Jerry Michalski: Yep.

40
00:05:27.240 --> 00:05:30.330
Gil Friend • Sustainability OG • CxO Coach: As we are constructing meaning all the time.

41
00:05:30.670 --> 00:05:31.520
Gil Friend • Sustainability OG • CxO Coach: So…

42
00:05:31.840 --> 00:05:39.080
Gil Friend • Sustainability OG • CxO Coach: I guess I needed 4, but that's… that's where I start with this. It's a lot. It's a huge topic. Oh, and number 5?

43
00:05:39.080 --> 00:05:39.930
Jerry Michalski: Wait, there's more?

44
00:05:39.930 --> 00:05:57.769
Gil Friend • Sustainability OG • CxO Coach: wait, there's… but wait, there's more! It slices, it dices. You provoke this call around the topic of reality, and it always brings me back to Lily Tomlin in that wonderful one-person show, Search for Intelligent Life of the Universe, where Trudie the Bag Lady, one of the characters she plays, says, reality.

45
00:05:58.040 --> 00:06:00.479
Gil Friend • Sustainability OG • CxO Coach: Highly overrated, it's just a collective hunch.

46
00:06:02.220 --> 00:06:05.540
Gil Friend • Sustainability OG • CxO Coach: So I always think about reality as a highly overrated topic.

47
00:06:07.110 --> 00:06:08.380
Gil Friend • Sustainability OG • CxO Coach: I'll stop there.

48
00:06:08.910 --> 00:06:10.580
Jerry Michalski: That was funny. I've used….

49
00:06:10.580 --> 00:06:13.619
Gil Friend • Sustainability OG • CxO Coach: I've used up my time for the morning, I'll be quiet from now on.

50
00:06:14.480 --> 00:06:15.280
Jerry Michalski: Excellent.

51
00:06:15.280 --> 00:06:15.730
Rick Botelho: Right.

52
00:06:15.780 --> 00:06:22.120
Jerry Michalski: Or not, or not. Kalia, the shadow that the sun is throwing behind you makes it look like you're in prison somewhere?

53
00:06:24.130 --> 00:06:27.959
Jerry Michalski: Are you, are you, like, in solitary right now, or what's the deal?

54
00:06:30.410 --> 00:06:36.569
Kaliya Identity Woman: No, it's just my… It's like a curtain, so if I straighten it out.

55
00:06:37.430 --> 00:06:40.249
Jerry Michalski: Oh my god, you can move the bars! That's so cool!

56
00:06:40.250 --> 00:06:40.610
Kaliya Identity Woman: Yeah.

57
00:06:41.960 --> 00:06:46.240
Kaliya Identity Woman: And that's, like, the window… Waves, yeah.

58
00:06:46.240 --> 00:06:59.070
Jerry Michalski: Oh, okay, so that's the curtain and the windows right behind it, which are both behind you. I thought you were against a wall, and it was a shadow being cast on the wall, but I was reading the image back wrong. See? Reality is fuzzy anyway.

59
00:07:00.270 --> 00:07:02.399
Jerry Michalski: Anyway, thanks for… thanks for being here.

60
00:07:02.930 --> 00:07:10.390
Jerry Michalski: Anyone want to take any of Gil's 5, 6, 7 questions and expand on them, play with them, add to them, whatever?

61
00:07:10.650 --> 00:07:12.820
Alex Kladitis: Can I just make one point?

62
00:07:12.820 --> 00:07:14.570
Jerry Michalski: Please. I've recently watched.

63
00:07:14.600 --> 00:07:21.100
Alex Kladitis: we're in the middle of Series 2, something called The Capture, on Netflix.

64
00:07:21.550 --> 00:07:22.779
Alex Kladitis: Anyone seen that?

65
00:07:23.400 --> 00:07:30.430
Alex Kladitis: No idea why it didn't get more traction. Came out, the first series in 2019 or thereabouts. Second series in 2022.

66
00:07:30.660 --> 00:07:32.959
Jerry Michalski: Vick Henry, I guess it just landed on it.

67
00:07:33.090 --> 00:07:34.480
Alex Kladitis: the capture.

68
00:07:34.830 --> 00:07:35.340
Jerry Michalski: Huh.

69
00:07:35.340 --> 00:07:40.439
Alex Kladitis: If you want to take on board what Gil was saying about you can't… can't believe what you see, or whatever.

70
00:07:40.970 --> 00:07:44.090
Alex Kladitis: I think in 2019, it would have been way ahead of its time.

71
00:07:44.670 --> 00:07:45.590
Alex Kladitis: Now….

72
00:07:45.590 --> 00:07:46.240
Jerry Michalski: Okay.

73
00:07:46.400 --> 00:07:54.300
Alex Kladitis: I almost saw it as a documentary, but anyway, it's an interesting play on… you cannot believe what, you know, what you see anymore, and whatever. It's a very….

74
00:07:54.300 --> 00:08:07.899
Jerry Michalski: I can read the Netflix blurb, which is right to the point, that says, an ambitious London detective takes on a case involving an Afghanistan war veteran that drags her into a shadowy world where nothing is what it seems.

75
00:08:09.070 --> 00:08:13.930
Alex Kladitis: And it… just watch it, that's all I can say. Cool!

76
00:08:14.250 --> 00:08:16.999
Gil Friend • Sustainability OG • CxO Coach: There are about 3 dozen movies with the same one-liner.

77
00:08:17.300 --> 00:08:21.379
Jerry Michalski: Yeah, no, nothing is what it seems. Well, there's also the truth is out there.

78
00:08:23.530 --> 00:08:26.089
Alex Kladitis: This is a… this is a slightly different…

79
00:08:26.490 --> 00:08:28.770
Alex Kladitis: Take on… nothing's what it seems.

80
00:08:28.920 --> 00:08:29.860
Jerry Michalski: Yeah, yeah.

81
00:08:29.860 --> 00:08:32.400
Alex Kladitis: Watch it. It'll be good.

82
00:08:33.130 --> 00:08:35.509
Jerry Michalski: So the series premiered in 2019.

83
00:08:37.100 --> 00:08:37.970
Alex Kladitis: Yeah.

84
00:08:38.230 --> 00:08:41.299
Jerry Michalski: Thank you. Anyone else want to take us, ….

85
00:08:42.850 --> 00:08:43.260
Klaus Mager: Yeah.

86
00:08:43.260 --> 00:08:44.890
Jerry Michalski: I guess it… please.

87
00:08:45.130 --> 00:08:48.489
Klaus Mager: I would, … I'm actually more…

88
00:08:48.680 --> 00:08:55.430
Klaus Mager: distraught by, this in-your-face reality.

89
00:08:55.600 --> 00:09:02.900
Klaus Mager: like, for example, you know, the Texas redistricting effort, Not to steal 5 seeds.

90
00:09:03.500 --> 00:09:09.899
Klaus Mager: And no one caring whether this is, you know, understood as reality or not.

91
00:09:10.190 --> 00:09:22.800
Klaus Mager: I mean, the brutality of this, to just, what are you gonna do about it, kind of thing. So you don't even have to misconstrue reality, you know? It's, …

92
00:09:22.880 --> 00:09:31.259
Klaus Mager: You know, you are in a modern 21st century society faced with, …

93
00:09:31.680 --> 00:09:48.490
Klaus Mager: watching things that shouldn't happen, you know? And here they do. And so the… there's… and they're spinning it in ways where they're, whatever, 30, 40% of the population that are in favor of all this.

94
00:09:48.630 --> 00:10:00.029
Klaus Mager: just go along with the argumentations. The Democrats are doing it, everybody does it, or the Democrats are doing it worse, and so on. And so that's really distressing.

95
00:10:02.180 --> 00:10:16.650
Jerry Michalski: Now, redistricting is messing… it's very real. I mean, what you're describing to me is barefists politics that's happening, because usually redistricting happens every decade or so after the census.

96
00:10:16.740 --> 00:10:40.990
Jerry Michalski: Trump is also trying to mess with the census and change how the census is collected and who counts and all that, so that's happening. But this Texas move is unusual because it's very obviously a play to win more votes and keep, you know, the House or whatever. And Newsom had a clever answer to it, except if you look at all the states that are in play, apparently there are… there's more wiggle room in red states than blue states, so that

97
00:10:40.990 --> 00:10:51.269
Jerry Michalski: could be a losing battle if it cascades across the states, all of which is terrible, because I think we should have statistical districting. We should… we should actually force districts to be roughly, like.

98
00:10:51.270 --> 00:10:57.919
Jerry Michalski: representative in some… in some neutral way, so that we can't do the gerrymandering thing, which I promise I did not invent.

99
00:10:58.140 --> 00:10:59.230
Jerry Michalski: …

100
00:10:59.490 --> 00:11:09.320
Jerry Michalski: and figure out, like, how this all might work. But we… I think we're just terribly stupid about this, so we've allowed these districts to become weapons, political weapons. But…

101
00:11:10.130 --> 00:11:13.860
Jerry Michalski: But I don't know if that's the melting of reality, Klaus, is it? Is it to you?

102
00:11:14.790 --> 00:11:32.629
Klaus Mager: Well, I'm just mentioning this as an example. I mean, there are dozens of other things where you just know, you know, the system is being gamed, and they're doing it no matter what you think about it.

103
00:11:32.720 --> 00:11:41.099
Klaus Mager: And, and it's… they don't even… I mean, it's not even really an attempt to hide it, it's just out in the….

104
00:11:42.050 --> 00:11:43.659
Jerry Michalski: Agreed. Alex?

105
00:11:44.860 --> 00:11:49.340
Alex Kladitis: Does anybody else want to talk? I've spoken quite a lot so far, so anyone else want to get in?

106
00:11:49.550 --> 00:11:50.599
Jerry Michalski: They can put their hand up.

107
00:11:50.600 --> 00:11:51.010
Alex Kladitis: Okay.

108
00:11:51.040 --> 00:11:53.259
Jerry Michalski: As you did. Go for it.

109
00:11:53.550 --> 00:12:05.030
Alex Kladitis: Thank you. So, … It is… amazes me how much our… free, democratic…

110
00:12:05.780 --> 00:12:10.060
Alex Kladitis: Western… whatever brilliant system we have.

111
00:12:10.560 --> 00:12:17.770
Alex Kladitis: basically tolerates, to me its corruption. It's not even a pretense of corruption. Gerrymandering, no matter which country you do it in.

112
00:12:18.100 --> 00:12:33.589
Alex Kladitis: it is you doing it in your favor. Even when you say, I mean, in the UK, we have a neutral, independent board that does this or that, forget it. It's as compromised as everything else. Magically, the border changes seem to favor the party in power.

113
00:12:33.650 --> 00:12:39.050
Alex Kladitis: And it just absolutely amazes me how we tolerate it, but it's the machinery of the system.

114
00:12:39.360 --> 00:12:44.689
Alex Kladitis: And… nothing seems to stop it. I mean, literally, we're talking about it.

115
00:12:44.970 --> 00:12:49.570
Alex Kladitis: And we tolerate it, you know, we just said, well, nothing to do, well, you know that.

116
00:12:49.760 --> 00:13:01.310
Alex Kladitis: And to me, it amazes me that we talk about democracy and all the rest of the values we have, and I could go on about crony capitalism, but let's leave that for another time.

117
00:13:01.680 --> 00:13:02.600
Alex Kladitis: And yet…

118
00:13:03.460 --> 00:13:09.120
Alex Kladitis: we don't question it. It's just so low in our worry… in our worry… things to worry about.

119
00:13:09.630 --> 00:13:11.170
Jerry Michalski: I think some people are questioning it.

120
00:13:11.740 --> 00:13:22.679
Jerry Michalski: I think some people are questioning it, they're just ineffectual. I mean, there are people yelling about these problems, but they're not winning any major vote or anything like that.

121
00:13:23.960 --> 00:13:29.820
Jerry Michalski: It's not complete… it's not completely below everybody's radar. There are people shouting, this is, you know, really terrible.

122
00:13:32.290 --> 00:13:43.179
Alex Kladitis: But the press and everyone doesn't question it. They don't kind of say, oh, you know, the mainstream press we're talking about, okay, if you go off-piste, you know, I'm sure that everybody will have a different opinion. But the main…

123
00:13:43.630 --> 00:13:44.680
Alex Kladitis: players.

124
00:13:45.840 --> 00:13:49.860
Alex Kladitis: Don't seem to challenge it. Challenge is the right word. Challenge it.

125
00:13:50.990 --> 00:13:58.869
Alex Kladitis: And I don't know if anyone will say anything more about the gerrymandering thing. I've got something else to do with we can't believe what we see anymore, but that's….

126
00:13:58.870 --> 00:13:59.210
Jerry Michalski: Yeah.

127
00:13:59.210 --> 00:14:00.260
Alex Kladitis: leave us a link.

128
00:14:00.260 --> 00:14:08.019
Jerry Michalski: We can go there. I did want to throw in that, just to what you just said, Alex, I'm… there's a thought in my brain, T91,

129
00:14:08.350 --> 00:14:28.210
Jerry Michalski: Which is the 91 indictments that Trump was under. And I was tracking the 5 or 6 different cases in all the different courts, and I was pretty sure that one of them would be very hard to evade and pin, you know, pin on, and would pin him to the wall somehow. And they managed very elegantly to evade

130
00:14:28.320 --> 00:14:40.750
Jerry Michalski: all of them, and are currently weaponizing the Department of Justice and other parts of the government to attack all the people who were persecuting them. And the system's lack of ability

131
00:14:40.930 --> 00:14:54.589
Jerry Michalski: to stop corruption and to roll it up and do something about it has astonished me. I've been blown away by that. And then a part of that whole thing is to scrub history later.

132
00:14:54.590 --> 00:15:05.199
Jerry Michalski: and figure out how you can sort of change everybody's perception of events, or whatever, whatever. And this is very dangerous, because we humans are very adaptable, and anything you say three times is true.

133
00:15:05.280 --> 00:15:12.259
Jerry Michalski: Which is the Bellman's Fallacy from Lewis Carroll. And so, in the middle of all this.

134
00:15:12.560 --> 00:15:22.799
Jerry Michalski: one problem is we don't have a common memory, we don't have a common place to put things, but the other one is that everything is becoming squishier, and if you want to head in that direction, Alex, please do.

135
00:15:24.770 --> 00:15:29.210
Alex Kladitis: My only comment on… on… sorry, on the… on the… Trump situation.

136
00:15:29.480 --> 00:15:33.999
Alex Kladitis: I could have a pretty… you'd think I was crazy with the theories I have about things.

137
00:15:34.230 --> 00:15:38.299
Alex Kladitis: I'll throw this… this is what I didn't… isn't what I wanted to say, but I'll throw one thing in.

138
00:15:39.170 --> 00:15:46.010
Alex Kladitis: President Trump has… do we agree that he's stopped illegal immigration across the border or not?

139
00:15:46.170 --> 00:15:47.510
Alex Kladitis: I don't know, it's….

140
00:15:47.510 --> 00:15:48.739
Jerry Michalski: It's way, it's way down.

141
00:15:48.740 --> 00:15:52.089
Alex Kladitis: So… The sell is that it's way down, okay?

142
00:15:52.170 --> 00:15:53.070
Jerry Michalski: Nope.

143
00:15:53.070 --> 00:15:54.759
Alex Kladitis: I want you to think about this.

144
00:15:55.160 --> 00:16:04.249
Alex Kladitis: All that, no matter what you think, all that migration, 95% of it was done by people tra… the mafia, basically. Illegal…

145
00:16:04.440 --> 00:16:06.310
Alex Kladitis: entities, right?

146
00:16:06.410 --> 00:16:11.390
Alex Kladitis: So, he's taken away from them between $50 and $100 billion a year.

147
00:16:12.100 --> 00:16:16.190
Alex Kladitis: My life experience tells me that you don't take 50 or 100 billion

148
00:16:16.380 --> 00:16:21.040
Alex Kladitis: Out of the… someone's… those kind of people's mitts, hands.

149
00:16:21.540 --> 00:16:23.199
Alex Kladitis: And live to tell a tale.

150
00:16:23.470 --> 00:16:30.100
Alex Kladitis: And he's quite happily doing it, and it's fairly safe. I'm not saying he's part of it, don't even think I'm saying that, I'm not.

151
00:16:30.440 --> 00:16:35.329
Alex Kladitis: But whatever power he has, It is a lot of power.

152
00:16:35.570 --> 00:16:42.260
Alex Kladitis: And I don't know under what… you know, it's legal power, but he's managed to do it. So do not underestimate.

153
00:16:42.690 --> 00:16:45.050
Alex Kladitis: The man's ability to do things.

154
00:16:45.680 --> 00:16:52.030
Alex Kladitis: Right? And I think it's doing it legally, I don't think it's doing anything wrong. But anyway, that's… on the issue of, …

155
00:16:52.530 --> 00:16:54.839
Alex Kladitis: Of the, … so don't underestimate.

156
00:16:55.110 --> 00:17:00.969
Alex Kladitis: what that man can do. That's all I'm saying. On the other issue of We don't know what's…

157
00:17:01.570 --> 00:17:11.509
Alex Kladitis: what's stable anymore. I was literally today speaking to a friend of mine, where we kind of… okay, my background is so diverse that I never believe anyone or anything.

158
00:17:11.680 --> 00:17:12.430
Alex Kladitis: I listened to the

159
00:17:12.770 --> 00:17:20.579
Alex Kladitis: say, oh, they could have started this way, they gave it 5 minutes instead of 2, they gave it 2 instead of 10 seconds, or whatever. You know, I'm always in this world of.

160
00:17:20.700 --> 00:17:21.589
Alex Kladitis: Whatever.

161
00:17:21.920 --> 00:17:28.140
Alex Kladitis: And I was talking to this other person who was more or less in league, and we agreed that right now.

162
00:17:28.610 --> 00:17:33.639
Alex Kladitis: Whether we're left or right or center, we have no idea what the hell is going on.

163
00:17:34.430 --> 00:17:38.659
Alex Kladitis: It used to be that I used to know my position. I used to see clearly what's going on.

164
00:17:39.100 --> 00:17:40.980
Jerry Michalski: But what's going on at the moment.

165
00:17:41.170 --> 00:17:45.010
Alex Kladitis: I have… I have… I can… Guess.

166
00:17:45.700 --> 00:17:57.649
Alex Kladitis: you know, you call me a conspiracy theorist as to what's going on, but I cannot for certain say what's going on, because everything is like, I thought, this is what would happen, this is what happened, and it's all in flux.

167
00:17:58.370 --> 00:18:00.529
Alex Kladitis: And I will say, just as an example.

168
00:18:00.640 --> 00:18:05.210
Alex Kladitis: That's something that I don't think… I don't think was reported in the mainstream press.

169
00:18:05.370 --> 00:18:08.460
Alex Kladitis: That… do you know that on the day

170
00:18:09.010 --> 00:18:17.910
Alex Kladitis: President Putin went to meet Trump in Alaska. That morning, before he left, He signed papers authorizing

171
00:18:18.240 --> 00:18:24.740
Alex Kladitis: American energy companies, ExxonMobil to be precise, to be able to go and do business in Russia.

172
00:18:26.020 --> 00:18:29.509
Alex Kladitis: That morning before I left the flight, he signed those papers.

173
00:18:30.830 --> 00:18:34.560
Alex Kladitis: Right? And that's a fact, I'm not making it up. That is….

174
00:18:34.560 --> 00:18:35.480
Jerry Michalski: Are you sure that's a fact?

175
00:18:35.480 --> 00:18:35.990
Alex Kladitis: Good.

176
00:18:36.570 --> 00:18:37.770
Alex Kladitis: It is a fact, yes.

177
00:18:38.750 --> 00:18:39.510
Alex Kladitis: Okay, so….

178
00:18:39.510 --> 00:18:41.670
Jerry Michalski: part of our conversation here. Yeah.

179
00:18:41.670 --> 00:18:49.960
Alex Kladitis: Yeah. So, so… there's things afoot that I have no idea what's… Coming next.

180
00:18:50.600 --> 00:19:05.380
Alex Kladitis: Okay, because, by the way, these laws or whatever against foreign companies and energy came in after the sanctions came in and whatever, so Russia retaliated with these things. But specifically in energy, Putin signed a decree saying that they can come back in.

181
00:19:06.830 --> 00:19:09.360
Alex Kladitis: So… and then he went to meet Trump.

182
00:19:09.550 --> 00:19:11.339
Alex Kladitis: Was there a condition to meeting Trump?

183
00:19:12.700 --> 00:19:16.200
Alex Kladitis: What? I have no idea anymore what….

184
00:19:16.200 --> 00:19:16.520
Jerry Michalski: Ma'am.

185
00:19:16.520 --> 00:19:22.720
Alex Kladitis: what is going on? You know, is it financial? Is it politics? Is it international? Whatever?

186
00:19:23.190 --> 00:19:28.140
Alex Kladitis: And what are we going to do with Rockin' Man in the corner there, top left corner, top right corner of the map?

187
00:19:28.500 --> 00:19:31.140
Alex Kladitis: Well, top left for you guys, but top right for me.

188
00:19:31.480 --> 00:19:32.520
Jerry Michalski: Which man?

189
00:19:33.280 --> 00:19:35.259
Alex Kladitis: Kenjim in North Korean person.

190
00:19:35.260 --> 00:19:39.760
Jerry Michalski: Oh, oh, North Korea, sorry, yeah, rocket, that rocket thing. Yeah, yeah.

191
00:19:39.760 --> 00:19:44.059
Alex Kladitis: I mean, here's the force.

192
00:19:44.270 --> 00:19:47.669
Alex Kladitis: So, either the Russians have to control him.

193
00:19:49.240 --> 00:19:55.859
Alex Kladitis: Because I'm sure America doesn't want him uncontrolled, so what's… what's… where's the deal in that? That's where my brain goes, by the way, because.

194
00:19:55.860 --> 00:19:56.260
Jerry Michalski: Damn.

195
00:19:56.260 --> 00:20:00.569
Alex Kladitis: weird, and… But there's so many things at play, I have no idea anymore.

196
00:20:00.960 --> 00:20:10.119
Jerry Michalski: Let's go meta a little bit on the topic, so that we can kind of look at… let's not talk about specific facts or things, but rather the effect of

197
00:20:10.350 --> 00:20:13.959
Jerry Michalski: of… of the melting on us, if we can. Alright, Gil?

198
00:20:17.980 --> 00:20:19.210
Gil Friend • Sustainability OG • CxO Coach: So, …

199
00:20:22.990 --> 00:20:31.559
Gil Friend • Sustainability OG • CxO Coach: Alex, if you could please share with us your sources for the Exxon story and the Mafia revenue story. I haven't seen those, and I'd love to run those down.

200
00:20:31.560 --> 00:20:33.490
Alex Kladitis: The source… hold on, hold on.

201
00:20:33.490 --> 00:20:34.639
Gil Friend • Sustainability OG • CxO Coach: So let's put them in favor.

202
00:20:34.640 --> 00:20:35.850
Alex Kladitis: Revenue is my fault.

203
00:20:36.410 --> 00:20:39.060
Alex Kladitis: My thought… my thought is the Mafia revenue, okay?

204
00:20:39.060 --> 00:20:39.850
Gil Friend • Sustainability OG • CxO Coach: Oh, okay.

205
00:20:39.850 --> 00:20:40.490
Alex Kladitis: what I meant.

206
00:20:40.490 --> 00:20:42.209
Gil Friend • Sustainability OG • CxO Coach: Okay, got it.

207
00:20:42.490 --> 00:20:46.950
Gil Friend • Sustainability OG • CxO Coach: Thank you. And, and the Exxon story, if you could put something in the chat on that, that would be great.

208
00:20:46.950 --> 00:21:03.239
Gil Friend • Sustainability OG • CxO Coach: The, … you know, the immigrate… the borders seem to have been shut down. The Times reported this morning that immigrant population in the United States is down a million and a half, declined for the first time in, like, a really long time. That's not just illegals, of course, that's a lot of other people.

209
00:21:03.300 --> 00:21:04.540
Gil Friend • Sustainability OG • CxO Coach: Who have split.

210
00:21:04.720 --> 00:21:08.250
Gil Friend • Sustainability OG • CxO Coach: Because it doesn't feel safe to them here. …

211
00:21:12.250 --> 00:21:21.679
Gil Friend • Sustainability OG • CxO Coach: I think it's not accurate to say we don't know what's going on, you know, with the… Jerry, you were talking about the, the, the, the rolling out of the,

212
00:21:22.410 --> 00:21:29.310
Gil Friend • Sustainability OG • CxO Coach: of the Trump authoritarian strategy. This was all laid out in enormous detail in Project 2025.

213
00:21:29.810 --> 00:21:39.489
Gil Friend • Sustainability OG • CxO Coach: 900-some-odd page strategy document. Last time I checked, which was 3 weeks ago, I'm sure the number is higher, 42% of the initiatives in Project 25 had been implemented. The game plan was there.

214
00:21:39.660 --> 00:21:48.209
Gil Friend • Sustainability OG • CxO Coach: Out in the open, and let's say, you know, nobody, in quotes, believed they were going to really do that, and they've just been marching through and executing.

215
00:21:48.650 --> 00:21:54.359
Gil Friend • Sustainability OG • CxO Coach: And they have identified and exploited the weak points in the checks and balances system.

216
00:21:54.560 --> 00:22:03.019
Gil Friend • Sustainability OG • CxO Coach: That lets them get away with things, or, you know, face court actions. There have been a couple of hundred major court actions brought. They've lost most of them.

217
00:22:03.380 --> 00:22:18.429
Gil Friend • Sustainability OG • CxO Coach: But the question of enforceability remains. And so, you know, I see our lawyer nodding his head there. It's like a big legal mess, a carefully constructed constitutional checks and balances system that had some flaws in it, one of which is that it presumed good faith.

218
00:22:19.120 --> 00:22:26.179
Gil Friend • Sustainability OG • CxO Coach: on the part of the actors, and it didn't provide for checks and balances against not good faith, so we've got that. So we know that that's what's happening.

219
00:22:26.290 --> 00:22:30.749
Gil Friend • Sustainability OG • CxO Coach: But to Alex's other point about, I have no idea what's coming next, you said, Alex?

220
00:22:31.050 --> 00:22:33.710
Gil Friend • Sustainability OG • CxO Coach: That's the fucking reality right now.

221
00:22:34.240 --> 00:22:52.790
Gil Friend • Sustainability OG • CxO Coach: And we've… all of us lived our lives with some amount of expectation and predictability of what would unfold next within certain kind of boundaries, and that game seems to be over for now. And so the big question is, how do we live in the midst of utter uncertainty?

222
00:22:53.140 --> 00:23:12.650
Gil Friend • Sustainability OG • CxO Coach: You know, optimism, pessimism is not the question anymore, because you can't predict anything anymore. And so how do you live, and how do you move, and how do you… you, we, I, how do we be effective in a world of such uncertainty, you know, and standing on quicksand instead of on granite?

223
00:23:12.760 --> 00:23:27.859
Gil Friend • Sustainability OG • CxO Coach: How do we, you know, how do we live and, be and, you know, feel, you know, feel reasonably human and, you know, and live a creative, contributory life with each other in that world? That's, you know, that's where I go with these questions.

224
00:23:28.030 --> 00:23:30.369
Gil Friend • Sustainability OG • CxO Coach: Since you teed it up that way, Alex, thank you.

225
00:23:31.500 --> 00:23:32.920
Gil Friend • Sustainability OG • CxO Coach: And I'm complete.

226
00:23:34.970 --> 00:23:36.160
Jerry Michalski: Doug, please.

227
00:23:36.160 --> 00:23:37.100
Gil Friend • Sustainability OG • CxO Coach: Or the moment.

228
00:23:42.580 --> 00:23:43.355
Doug Breitbart: So…

229
00:23:47.190 --> 00:23:54.430
Doug Breitbart: If everything in the theme of reality, if everything is… Period.

230
00:23:55.290 --> 00:23:58.119
Doug Breitbart: And the universe of possibilities that…

231
00:23:58.290 --> 00:24:01.469
Doug Breitbart: are presenting themselves and happening are…

232
00:24:05.690 --> 00:24:11.930
Doug Breitbart: The question… the living question… to me.

233
00:24:13.080 --> 00:24:18.050
Doug Breitbart: Has more to do with, as a person, as a human being.

234
00:24:21.280 --> 00:24:35.360
Doug Breitbart: … What… what would it take For me to… reorient.

235
00:24:36.350 --> 00:24:46.599
Doug Breitbart: And recognize and acknowledge… that, in fact, I don't have any control over anything.

236
00:24:47.150 --> 00:24:48.610
Doug Breitbart: beyond my skin.

237
00:24:56.560 --> 00:24:59.420
Doug Breitbart: And extrapolating that.

238
00:25:03.210 --> 00:25:14.430
Doug Breitbart: In, you know, larger… Collaborative context, collective context, legal, administrative, you know, territorial context.

239
00:25:15.340 --> 00:25:16.020
Doug Breitbart: …

240
00:25:17.860 --> 00:25:27.629
Doug Breitbart: you know, Western culture and civilization is rooted in the opposite belief, that somehow we actually have dominion and control over things.

241
00:25:29.150 --> 00:25:32.199
Doug Breitbart: And that things are predictable when they're not.

242
00:25:33.300 --> 00:25:40.669
Doug Breitbart: And that by force of will, we have the ability to control reality.

243
00:25:42.520 --> 00:25:43.479
Doug Breitbart: Which we can't.

244
00:25:46.830 --> 00:25:52.139
Doug Breitbart: So, you know, the cultural, societal, evolutionary delusion.

245
00:25:52.780 --> 00:25:55.449
Rick Botelho: It's at the root of everything.

246
00:26:00.160 --> 00:26:09.060
Doug Breitbart: ends up… Making the active ingredients in all of this individual attachment.

247
00:26:09.490 --> 00:26:11.669
Doug Breitbart: To a set of beliefs.

248
00:26:12.360 --> 00:26:18.509
Doug Breitbart: A set of projections, a set of… Fears, a set of attachments.

249
00:26:20.830 --> 00:26:23.890
Doug Breitbart: That are subjective truths.

250
00:26:26.830 --> 00:26:33.920
Doug Breitbart: And… groups of people that share the same subjective delusion.

251
00:26:34.800 --> 00:26:36.130
Doug Breitbart: Find each other.

252
00:26:39.050 --> 00:26:45.840
Doug Breitbart: Now, the reality is, We're all in the same soup.

253
00:26:46.230 --> 00:26:50.080
Doug Breitbart: We all live on the same planet. We all breathe the same air.

254
00:26:50.200 --> 00:26:55.979
Doug Breitbart: We share it with a whole bunch of other living creatures that are not our species.

255
00:26:56.090 --> 00:27:01.820
Doug Breitbart: We're subject to the forces and powers of the natural world, whether we like it or not.

256
00:27:02.320 --> 00:27:11.360
Doug Breitbart: And… the actions we take have consequences that affect ourselves, others, and the rest of the planet.

257
00:27:13.440 --> 00:27:14.610
Doug Breitbart: And…

258
00:27:17.980 --> 00:27:29.060
Doug Breitbart: At least for me, … Figuring out how to recognize that they are us.

259
00:27:29.180 --> 00:27:33.000
Doug Breitbart: That we're all… of a part.

260
00:27:33.540 --> 00:27:34.739
Doug Breitbart: Of the same hole.

261
00:27:36.690 --> 00:27:38.350
Doug Breitbart: Is the first step.

262
00:27:40.140 --> 00:27:50.300
Doug Breitbart: And second, corollary, that I have no power, control, authority, or ability to affect anybody or anything else.

263
00:27:52.100 --> 00:27:57.140
Doug Breitbart: The only living thing that I have control over is me.

264
00:27:57.980 --> 00:27:59.819
Doug Breitbart: and what I do with me.

265
00:28:01.470 --> 00:28:09.739
Doug Breitbart: And my words, my actions… And my consciousness or awareness of how I'm affecting others.

266
00:28:12.870 --> 00:28:16.129
Doug Breitbart: … And… and I'm complete.

267
00:28:18.560 --> 00:28:19.660
Jerry Michalski: Thanks, Doug.

268
00:28:20.210 --> 00:28:21.520
Jerry Michalski: Close, please.

269
00:28:22.850 --> 00:28:32.720
Klaus Mager: Yeah, Sherry, when you introduced this, our shared reality will self-destruct in the next 12 months. My mind was going into a completely different place.

270
00:28:32.950 --> 00:28:34.939
Jerry Michalski: Yeah, which is where?

271
00:28:34.940 --> 00:28:35.850
Klaus Mager: Which is…

272
00:28:36.110 --> 00:28:47.950
Klaus Mager: Of course, our reality is going to self-destruct, because the economic predictions and the rules that have been put in place are going to lead to consequences that are not intended.

273
00:28:48.030 --> 00:28:58.060
Klaus Mager: So Dave Brooks did, … I listened to him, he is really on fire, you know, giving, presentations.

274
00:28:58.790 --> 00:29:02.009
Klaus Mager: based on his Christian background.

275
00:29:02.260 --> 00:29:09.299
Klaus Mager: No, I mean, he's a very faith-focused individual, New Testament-based.

276
00:29:09.440 --> 00:29:11.009
Klaus Mager: And, …

277
00:29:11.110 --> 00:29:20.010
Klaus Mager: He… he was going into this historic perspective of every time that you have an authoritarian regime.

278
00:29:20.070 --> 00:29:28.300
Klaus Mager: And it doesn't matter if authoritarian left or right, it all ends up in the same place, you know, restricting individual freedoms.

279
00:29:28.300 --> 00:29:40.770
Klaus Mager: Making decreed, decrease in economics and, and, policies that are, that are just nonsensical, or they are, they are, poorly conceived.

280
00:29:40.770 --> 00:29:45.280
Klaus Mager: always leads into some sort of calamity. So…

281
00:29:45.480 --> 00:29:49.900
Klaus Mager: You know, the economy is going to start crashing in multiple spots.

282
00:29:50.230 --> 00:29:50.630
Rick Botelho: ticker.

283
00:29:50.630 --> 00:29:54.620
Klaus Mager: For the most vulnerable people in the society.

284
00:29:54.850 --> 00:30:13.980
Klaus Mager: So then what? Then that leads to authoritarianism, that leads to suppression. You know, that leads to violence, basically, to prevent people from expressing their opinions. So that's really more where I was going. Our reality is going to fall apart.

285
00:30:14.040 --> 00:30:17.970
Klaus Mager: You know, because the promises that have been made,

286
00:30:18.190 --> 00:30:27.160
Klaus Mager: when the big, beautiful budget bill was passed, for example, are not going to turn out the way they were presented, then what?

287
00:30:27.360 --> 00:30:36.450
Klaus Mager: you know, then it doesn't matter what pictures are being falsified, because this is real. I don't have food, you know, my food stamps.

288
00:30:36.450 --> 00:30:36.960
Rick Botelho: Easter.

289
00:30:36.960 --> 00:30:50.070
Klaus Mager: canceled. My medical bill, my Medicare just got… Medicaid just got canceled. You know, this is the way, this is, this is real and raw for a significant share of the population.

290
00:30:50.580 --> 00:30:52.100
Jerry Michalski: I'm possi…

291
00:30:52.430 --> 00:31:06.140
Jerry Michalski: what you're saying is true for a moment and a perspective on politics, but what you're saying isn't that different from people saying what Biden's actions and policies were going to be was going to destroy the country because

292
00:31:06.210 --> 00:31:24.809
Jerry Michalski: name your… name your other, you know, from the other side of the field, looking at politics, oh my god, we're all gonna die because the country's gonna be in a mess. So, what you're describing is normal, I think, normal politics playing out these days in a more… it seems, in a much more extreme way, because previous

293
00:31:24.810 --> 00:31:33.750
Jerry Michalski: Political actors didn't try to shred the government, like, like, immediately, you know, putting a dagger, into its, into its carotid.

294
00:31:34.110 --> 00:31:41.280
Jerry Michalski: But… It doesn't seem… it seems like extreme, but not unusual politics. No?

295
00:31:41.280 --> 00:31:47.099
Klaus Mager: I see it different, because… Somebody is losing health insurance.

296
00:31:47.100 --> 00:31:48.250
Jerry Michalski: But that….

297
00:31:48.250 --> 00:31:51.130
Klaus Mager: Supposing somebody's paying their food stamps.

298
00:31:51.390 --> 00:32:05.970
Klaus Mager: The food bank just ran out of food, you know, because their crayons got cut off. The local NGO that has done Meals on Wheels, for example, just lost their funding.

299
00:32:05.970 --> 00:32:12.270
Klaus Mager: That's real stuff, you know, that's not some political theory.

300
00:32:12.750 --> 00:32:31.250
Jerry Michalski: But a lot of places that lost government funding, their donations went way up, and I don't know, I haven't looked at all of these things, but part of what the big shake-up is about is redistributing how these things work in the world, and fixing perceived

301
00:32:31.250 --> 00:32:43.110
Jerry Michalski: flaws in how to fix social problems. That's, I think, the Project 2025 agenda, is to implement an entirely different regime of how to deal with

302
00:32:43.240 --> 00:33:03.070
Jerry Michalski: the country, the government, belief systems, et cetera, et cetera, like it or not. And these things happen periodically in this country. The big deal, the far-right conservatives have been fighting the big deal since the 30s, since the big deal. They hated it, the New Deal. And also the Great Society later, under LBJ.

303
00:33:03.120 --> 00:33:21.759
Jerry Michalski: And the far right thinks that the whole country has been screwed up completely by these programs, and these are the programs that are being presently undermined. So for them, this is a big circle back to a better time when we didn't have so much reliance on government, blah blah blah blah blah, but it's a political conversation.

304
00:33:22.450 --> 00:33:28.290
Jerry Michalski: For me. And these political conversations have enormous human consequences.

305
00:33:28.580 --> 00:33:38.279
Jerry Michalski: Right. Because politics is so big, and has so many, you know, influences everywhere, and can take money and shift it around, and build stuff and do stuff, or hurt people, so….

306
00:33:38.480 --> 00:33:50.690
Klaus Mager: We're talking about $1 trillion taken out of the base-off permit economy, $1 trillion. You know, the $290 billion out of SNAP.

307
00:33:51.080 --> 00:34:02.490
Klaus Mager: Those numbers are not even… are not recoverable at the state level, so they lead to major disruptions that no one is dialing in on. I mean, but no one's paying attention.

308
00:34:02.990 --> 00:34:03.800
Klaus Mager: Okay.

309
00:34:03.800 --> 00:34:06.029
Jerry Michalski: Thank you. Rick?

310
00:34:07.460 --> 00:34:22.300
Rick Botelho: Yeah, I was off-camera because I was eating breakfast, and I was listening attentively, and, you know, just maybe to dovetail on what Klaus was just saying. Actually, I'm hoping for a profound economic depression that will demonstrate,

311
00:34:22.420 --> 00:34:33.869
Rick Botelho: how, ineffective the counter-response may be in accommodating these huge deficits in, social support services. So…

312
00:34:34.060 --> 00:34:40.969
Rick Botelho: What I'm hoping, actually, is the opposite. That it won't be a turn to authoritarianism, it will be a turn against it.

313
00:34:41.130 --> 00:34:45.679
Rick Botelho: Because the adversity is going to be so bad that people are going to say.

314
00:34:45.969 --> 00:34:58.600
Rick Botelho: Or maybe this isn't working. Having said that, I do want to acknowledge something that was said earlier, which is, you know, reality is in the perceptions and biases of the observers.

315
00:34:58.780 --> 00:35:09.199
Rick Botelho: Which means the people who are least well-informed, who still drink the Trump Kool-Aid, will have to say, hey, this Kool-Aid isn't working for me.

316
00:35:09.660 --> 00:35:18.699
Rick Botelho: And so, I framed a question which I will do a deep dive into, and we don't have time to do a deep dive into this one.

317
00:35:18.900 --> 00:35:24.719
Rick Botelho: But I want to return to a theme last week of where I was, proposing complex questions.

318
00:35:25.070 --> 00:35:31.440
Rick Botelho: And, you know, Jerry, you asked me to, you know, ask me about curiosity and whatever. I'll respond to that later.

319
00:35:31.810 --> 00:35:41.140
Rick Botelho: But the whole point of compound philosophical questions is they're designed not to be understood on first reading.

320
00:35:41.580 --> 00:35:47.840
Rick Botelho: That's the whole purpose, to get out of our fast-thinking mode of instant clarity and comprehension.

321
00:35:48.010 --> 00:35:57.600
Rick Botelho: They are… this is our reductionist mindset that's propagated by the business community, the marketing community, which reinforces

322
00:35:57.890 --> 00:36:00.379
Rick Botelho: the MAGA brand, and supports it.

323
00:36:00.580 --> 00:36:05.439
Rick Botelho: So people will keep on believing, having blind faith in the MAGA bank.

324
00:36:05.620 --> 00:36:18.139
Rick Botelho: So I'm going to put a question in, and if people want to pursue it, that's fine. I'm going to pursue it offline as people are… and I've already started doing an AI search on this question. I'm not going to read it out either, because you're not supposed to read them out.

325
00:36:18.270 --> 00:36:26.409
Rick Botelho: you're supposed to take it piecemeal, digest it a little bit. What does that mean? How does it relate to this? What's the interconnections with this, that, and the other?

326
00:36:26.540 --> 00:36:28.570
Rick Botelho: And that is anti-Kiss.

327
00:36:28.720 --> 00:36:34.700
Rick Botelho: And I've mentioned this before, the KISS principle, which is keep sophistication this side of simplicity.

328
00:36:34.770 --> 00:36:46.020
Rick Botelho: The problem is that we're so locked into reductions thinking that we are disabled from dealing with complexity. So I'll put the question in, read it. If you want to respond to it, that's fine. I'm going to do a deep dive.

329
00:36:46.020 --> 00:36:58.369
Rick Botelho: and demonstrate how you can use HEN. I won't explain what that means, because I got a question about last week. You need some time to digest it, guys! And I will show you next week, if I can make it.

330
00:36:58.440 --> 00:37:01.230
Rick Botelho: What you can generate as

331
00:37:01.310 --> 00:37:04.980
Rick Botelho: A learning module to co-elevate our

332
00:37:05.530 --> 00:37:10.440
Rick Botelho: Generative dialogues… generative and strategic dialogues about how to deal with our

333
00:37:10.570 --> 00:37:21.999
Rick Botelho: complex entanglement of self-inflicted wicked problems, which we're failing to address, and all I'm hoping for is let the depression come, because incompetence deserves it.

334
00:37:22.090 --> 00:37:32.470
Rick Botelho: And when it comes, let the pain be so bad that people are gonna say, hey, I don't want to drink, Don Pecom's Fool Aid anymore. I'm Rick and I'm done speaking.

335
00:37:34.140 --> 00:37:35.180
Jerry Michalski: Thanks, Rick.

336
00:37:36.960 --> 00:37:37.760
Jerry Michalski: …

337
00:37:44.180 --> 00:37:52.359
Jerry Michalski: I'm… I'm interested in… A little bit puzzled that we haven't really talked about facts melting

338
00:37:52.470 --> 00:37:59.029
Jerry Michalski: and other things happening, and what effects that might have on society. We kind of haven't gone there much at all.

339
00:37:59.400 --> 00:38:06.909
Jerry Michalski: And I want to maybe change the conversation a little bit by heading toward that topic, but on the positive side.

340
00:38:07.030 --> 00:38:08.960
Jerry Michalski: Because I think there's… there are some…

341
00:38:09.100 --> 00:38:14.000
Jerry Michalski: Positive aspects to, what's happening to reality.

342
00:38:14.280 --> 00:38:20.280
Jerry Michalski: And the biggest one I can see is, it might actually drive us closer together.

343
00:38:20.880 --> 00:38:34.790
Jerry Michalski: For example, oh my god, every student can use ChatGPT now to cheat on every exam and every resume and every job application. Oh my god, oh my god, they're getting harder, if not impossible, to detect. Oh my god, civilization is doomed. I'm like.

344
00:38:34.800 --> 00:38:43.180
Jerry Michalski: Well, no, we used to do oral exams, and it's really, really hard to sit in front of somebody and bullshit credibly for an hour.

345
00:38:43.400 --> 00:38:58.449
Jerry Michalski: about a topic you're supposed to have mastered. Like, mastery shows up in interaction. This is what apprenticeships are really, really great for. I remember years ago, talking to the guy who founded the University of the Earth in Oaxaca, and he said.

346
00:38:58.480 --> 00:39:05.189
Jerry Michalski: If you think you might want to be an agricultural lawyer, we can get you an internship with one right away.

347
00:39:05.260 --> 00:39:22.740
Jerry Michalski: And in a couple months, you will probably know that this is a career for you or not. In a year, they will know how you're going to fit, what you need to learn, etc, etc, and the process lets you, you know, try things out pretty quickly. And I'm like, what if the melting of

348
00:39:23.290 --> 00:39:25.670
Jerry Michalski: intermediated reality.

349
00:39:25.690 --> 00:39:32.989
Jerry Michalski: Meaning, the image of Doug sitting right now in one of the little squares in my grid view of the Zoom.

350
00:39:32.990 --> 00:39:46.950
Jerry Michalski: is intermediated reality, and for all I know, it's actually an avatar, and Doug is out on his bicycle, like, like, you know, exercising, and he's just had… he's commissioned a bot to look like him sitting calmly in a chair.

351
00:39:47.070 --> 00:39:54.310
Jerry Michalski: But when Doug and I are sitting in a cafe together, I think that's gonna be very hard to spoof.

352
00:39:54.430 --> 00:40:13.460
Jerry Michalski: And the fact that, you know, we have time logged in conversations like these, where I think, more or less it was actually him and all that, that's great, that's fantastic. So it might be that all of this digital artifice drives us back into analog reality, and into the comfort and

353
00:40:14.330 --> 00:40:22.550
Jerry Michalski: Mystery and sometimes, unpredictability of, you know, us squishy meat… bags of meat, and the things that… the funny things that we do.

354
00:40:23.350 --> 00:40:25.870
Jerry Michalski: … But, yeah.

355
00:40:26.010 --> 00:40:30.179
Jerry Michalski: Stacy, did you miss something? What do you mean?

356
00:40:30.180 --> 00:40:35.810
Stacey Druss: Oh, no. Sorry, that was not meant to go to everybody.

357
00:40:35.810 --> 00:40:42.360
Jerry Michalski: Oh, okay, sorry. Yeah, that went to everybody, and do you mean about Rick's post in the chat?

358
00:40:42.510 --> 00:40:44.790
Jerry Michalski: No. Oh, something else, okay.

359
00:40:45.690 --> 00:40:48.980
Jerry Michalski: … Alex, you're muted.

360
00:40:50.500 --> 00:40:53.820
Alex Kladitis: I was just gonna say, I think, Stacey, you can delete your, your com… your…

361
00:40:54.360 --> 00:40:57.540
Alex Kladitis: If you click on it or something. I think you can delete it, can't you?

362
00:40:59.490 --> 00:41:00.960
Alex Kladitis: If you want to. Might be possible.

363
00:41:01.040 --> 00:41:01.860
Jerry Michalski: Yeah.

364
00:41:02.600 --> 00:41:03.280
Stacey Druss: Thank you, Alice.

365
00:41:04.940 --> 00:41:05.870
Jerry Michalski: …

366
00:41:07.390 --> 00:41:13.160
Jerry Michalski: So yeah, so for me, that's a positive side of all of this. Don't know if you all agree, disagree.

367
00:41:14.040 --> 00:41:15.149
Jerry Michalski: Care about it.

368
00:41:20.690 --> 00:41:29.800
Klaus Mager: Yeah, I mean, I just submitted a project to Business Oregon, which they accepted, and they're asking for business partners to join.

369
00:41:29.970 --> 00:41:33.840
Klaus Mager: To rethink the local food system.

370
00:41:34.120 --> 00:41:40.019
Klaus Mager: And, to, to anticipate, you know, the, the, …

371
00:41:40.030 --> 00:41:57.339
Klaus Mager: changes that will happen to the funding source, funding levels of soup kitchens, and food banks, and meals on wheels, and school meals, because they all got, like, you know, categorically hit. So how do you… how do you deal with this? Well, you have to, you know, come… come and… and…

372
00:41:57.340 --> 00:42:02.500
Klaus Mager: Build a community-based response where everyone has to pull in, including

373
00:42:02.500 --> 00:42:08.929
Klaus Mager: you know, Cisco, and US Foods, and Walmart, and, you know, the big, the big companies.

374
00:42:08.960 --> 00:42:25.859
Klaus Mager: So, my hope is that something really good can come out of this, because rather than putting money, putting band-aids out there to feed the people without any change inside, maybe

375
00:42:26.430 --> 00:42:36.309
Klaus Mager: Maybe we can do something to fundamentally change the economics of the base, which right now is an extractive

376
00:42:36.440 --> 00:42:46.010
Klaus Mager: system, you know, by national, multinational corporations, you know, why don't we try to make this self-sufficient instead?

377
00:42:46.550 --> 00:43:02.879
Jerry Michalski: Love that. And then my own energies and intentions are in that direction a lot. It's like, well, when things are broken, it's a chance to re-engineer things, and they were badly designed before this mess hit. So why don't we try to find ways where things actually work beautifully.

378
00:43:02.880 --> 00:43:12.719
Jerry Michalski: or better, and make it… make them flexible enough that people can appropriate them, et cetera, et cetera. We've had some of those conversations in other calls. Kaliyah, please.

379
00:43:14.230 --> 00:43:20.830
Kaliya Identity Woman: Yeah, I mean, building on this, I… at Bioneers last year…

380
00:43:22.410 --> 00:43:24.880
Kaliya Identity Woman: Maybe it was this… I went to… anyways.

381
00:43:25.770 --> 00:43:37.099
Kaliya Identity Woman: There was a panel all about, like, the super cool, like, farm-to-school… school lunch stuff, where basically schools are buying directly from farmers and cooking the food on-site.

382
00:43:37.930 --> 00:43:50.799
Kaliya Identity Woman: And I was like, okay, great, you're helping kids, but, like, I live alone, I cook all my own meals, organic, because I refuse to, like, engage with the industrial food system.

383
00:43:51.690 --> 00:43:55.780
Kaliya Identity Woman: It's a lot, and like, I'm single, right? So…

384
00:43:56.090 --> 00:44:08.070
Kaliya Identity Woman: you know, people with… anyways, so… and it just became super clear, especially… I've been radicalized against, like, ultra-processed food, which are, like, in everything. So, like, the Walmarts and the…

385
00:44:08.230 --> 00:44:14.439
Kaliya Identity Woman: They're selling us shit. It's killing us. I mean, this is where RFK and Maha are right.

386
00:44:14.650 --> 00:44:22.600
Kaliya Identity Woman: But what if we figured out how we had, like, community kitchens that were…

387
00:44:24.480 --> 00:44:42.540
Kaliya Identity Woman: where people like me paid, and people on food stamps bought… because you can't buy hot food, right? So you have to figure out how you get bulk food shipped to, like, the site. People show up and buy rice, and then, like, carry it to the kitchen, and that's their payment.

388
00:44:42.610 --> 00:44:45.329
Kaliya Identity Woman: So you're technically supporting them

389
00:44:45.740 --> 00:44:58.790
Kaliya Identity Woman: And then they can get, like, hot food out of the community kitchen, and you're also potentially, like, having people spend an afternoon every month or every two weeks in the kitchen with their neighbors cooking for each other.

390
00:44:58.790 --> 00:45:08.040
Kaliya Identity Woman: We need community right now, and we need… and I believe, like, food preparation is, like, actually… …

391
00:45:08.220 --> 00:45:22.569
Kaliya Identity Woman: My experience when I do it in community is incredible. I'm like, it's just so positive, and that's what humans have been doing for literally millions of years. And we've now outsourced this sort of fundamental, like, thing to, like, corporations.

392
00:45:22.730 --> 00:45:25.240
Kaliya Identity Woman: Like, it's very broken.

393
00:45:25.490 --> 00:45:30.020
Kaliya Identity Woman: So… I had this whole vision, …

394
00:45:30.380 --> 00:45:36.519
Kaliya Identity Woman: in this Bioneers meeting of, like, can we do what we're doing for school kids for, like, everybody?

395
00:45:36.890 --> 00:45:47.829
Kaliya Identity Woman: I think there's inspiration from religious communities that feed people a lot. Like, the Sikhs really know how to feed people at scale. They do it. ….

396
00:45:48.930 --> 00:45:53.570
Kaliya Identity Woman: Anyways, so those are my thoughts, building on what was just shared.

397
00:45:54.140 --> 00:46:01.169
Jerry Michalski: Oh yeah, thank you. You reminded me of Jose Andres' op-ed recently in the Times, I just put a GIF link to it in the chat.

398
00:46:01.550 --> 00:46:12.339
Jerry Michalski: Where he was writing about feeding people in Gaza, and he said, the problem with bulk food is that people are showing up, holding out their pan, and you're going to pour some raw rice into the pan?

399
00:46:12.350 --> 00:46:30.059
Jerry Michalski: Like, that's not gonna work. And also that the interception and misuse of raw foods is what gangs and everybody else wants to do, has a fully functioning economy around. So if you can create kitchens that make hot food and distribute them small enough

400
00:46:30.060 --> 00:46:38.700
Jerry Michalski: in enough places, everybody can just show up and get hot meals, and that works really well. And I think it's an extremely compelling

401
00:46:38.750 --> 00:46:42.510
Jerry Michalski: solution to the famine that is hitting Gaza.

402
00:46:42.720 --> 00:47:01.039
Jerry Michalski: But also, it's a very nice solution to the broader social issue you just described, Kalia. Because, you know, if everybody doesn't have to go source their own ingredients, cook their own thing, but we can show up and get to know one another, and sit together over meals, I think a lot of good things happen.

403
00:47:01.240 --> 00:47:16.149
Jerry Michalski: But this is different from the commercial restaurant thing we have set up. It's quite different from the commercial restaurant thing we have set up. Then, restaurant food waste is a horrible thing. I, like, whenever I go to an event and I see a buffet.

404
00:47:16.270 --> 00:47:31.810
Jerry Michalski: Especially a fancy event, and the buffet, at the end of the meal, they've kept the buffet full, so there is a whole mess of very edible food, and I don't really know what's going to happen to it that is filling those trays, because a nice hotel doesn't ever want these things to look empty.

405
00:47:32.180 --> 00:47:39.269
Jerry Michalski: It's bad. Anyway, Klaus and others know way too much about all the brokenness of the current food system.

406
00:47:39.380 --> 00:47:40.649
Jerry Michalski: Rick, please.

407
00:47:42.580 --> 00:47:48.440
Rick Botelho: Yeah, just in response to what's just said, I just put a… for people who are not familiar with nutritionfacts.org.

408
00:47:48.640 --> 00:47:54.210
Rick Botelho: It's Michael Greger's life's work on the effect of nutrition on health.

409
00:47:54.560 --> 00:48:14.280
Rick Botelho: And it's evidence-based. He has a particular style that reminds me… I met him once, and I described him as the John Cleans of nutrition. He's a really nice, eccentric guy, and very knowledgeable. His latest book on how not to age is 600 pages through 20,000 references.

410
00:48:14.380 --> 00:48:32.890
Rick Botelho: And talk about deep dive. You gotta… if you wanna know how not to age, you want to look at his book. I'm going through it at the moment. I tell you, there's so much information there, so many great tips. And it's all about what Klaus knows, which is nutrition, exercise, healthy lifestyle, social cohesion, stupid!

411
00:48:32.890 --> 00:48:40.140
Rick Botelho: Anyway, that's my rant, but I will return to, … I'll leave this for you to look at later, Gerry.

412
00:48:40.140 --> 00:48:49.630
Rick Botelho: Which is about curiosity and using compound questions, and I… I really took the things to heart last week when… when Mike was challenging me. Can you get it down to…

413
00:48:49.750 --> 00:48:54.080
Rick Botelho: five to seven words short film, and my immediate reaction was, hell no!

414
00:48:54.200 --> 00:49:01.619
Rick Botelho: We wanted to go in the opposite direction. If you've got something semantically dense, we should be curious about what the… what does that mean? What is humanism?

415
00:49:01.890 --> 00:49:04.570
Rick Botelho: You know, how can AI actually help us?

416
00:49:04.880 --> 00:49:09.159
Rick Botelho: Emancipatory learning? What's that? And we need new learning methods? What's that?

417
00:49:09.570 --> 00:49:12.739
Rick Botelho: So I'm inviting you to take your time.

418
00:49:12.810 --> 00:49:16.599
Rick Botelho: to reflect on something I've said there, and it challenges

419
00:49:16.650 --> 00:49:29.919
Rick Botelho: the holy grail of some of the assumptions that are made in this group, and people have challenged me repeatedly on them, and I'm deciding, okay, I'm learning from you guys, and … whenever you encounter resistance to change.

420
00:49:29.920 --> 00:49:40.679
Rick Botelho: It's a phenomenal opportunity to learn, and I have to go at the top of the hour, Jerry. So, you know, if you want to look at it, fine. If you don't, that's fine, but,

421
00:49:40.680 --> 00:49:55.689
Rick Botelho: I will certainly share my output of the question I posed there, and put it in next week so that you can see how you can create a hand learning module that you can then use as a platform for generative dialogues.

422
00:49:56.050 --> 00:49:58.140
Rick Botelho: I'm done speaking. Thank you.

423
00:49:58.140 --> 00:49:59.110
Jerry Michalski: Thanks, Rick.

424
00:49:59.300 --> 00:50:00.210
Jerry Michalski: …

425
00:50:01.250 --> 00:50:15.949
Jerry Michalski: Sorry. I'm interested in what audience you do find, Rick, and who loves what you write, and I have a suspicion that your MO that you've just described for us, it works for somebody.

426
00:50:16.240 --> 00:50:22.559
Jerry Michalski: I'm sure it does, because humans are really varied, et cetera, et cetera, and some people will like the puzzles you're putting in front of them.

427
00:50:22.670 --> 00:50:33.460
Jerry Michalski: We've… several of us have just been reporting to you that your puzzles aren't working for us, and they don't work for me, and I agree with everything you're trying to convince us of.

428
00:50:33.460 --> 00:50:45.040
Jerry Michalski: Like, when we sit here and talk, you and I are, like, on board. I think, yep, yep, yep, check, check, check, check, check, check, and then I go look at your writings, and I can't begin to get into them.

429
00:50:45.040 --> 00:51:01.549
Jerry Michalski: Because they feel like you're putting everything into every post. Like, it's like a… it's like a hologram. Every post is a hologram of everything you care about, and like, I can't unpack every post that way. My brain doesn't follow that way, but I bet you some people's brains do.

430
00:51:01.880 --> 00:51:16.349
Jerry Michalski: Right? And I don't know who those people are and what interactions you're having with them, and happy to help with that. But I'm just reporting back that what you've just told us is, you guys tried to give me advice, I listened to it carefully, but I'm not going to follow it, because….

431
00:51:16.350 --> 00:51:17.460
Rick Botelho: That's not what I said.

432
00:51:17.460 --> 00:51:21.530
Jerry Michalski: My approach is to give you the tangle and let you puzzle through it.

433
00:51:22.290 --> 00:51:23.109
Jerry Michalski: That's what I heard.

434
00:51:23.110 --> 00:51:30.840
Rick Botelho: Yeah, well, that's what you heard, that's not my perception, and that's your perception, and that's why we have to work through our classing perceptions.

435
00:51:30.910 --> 00:51:43.720
Rick Botelho: And, so I would suggest you look at Section 2 of that. That is work in progress. And so, you know, I've talked about the KSSS principle, which is keep sophistication this side of simplicity.

436
00:51:43.950 --> 00:51:51.759
Rick Botelho: And, this is a mindset shift, mindset shift from an educational paradigm that focuses on content

437
00:51:52.020 --> 00:52:00.309
Rick Botelho: to a learning paradigm that focused on process. We've got too much friggin' content in the world, and we don't know how to process it. So we're disabled.

438
00:52:00.310 --> 00:52:18.240
Rick Botelho: And so, I'm not, you know, I'm rolling with your resistance, not against it. I'm thinking, well, how could I maybe open some people's door to get a little inquisitive or curious? Okay, I don't understand what this question means, and I put one in, I'll come back and demonstrate. That's not a particularly complex question.

439
00:52:18.320 --> 00:52:24.090
Rick Botelho: But it's the orientation that people bring that's predominantly reductionistic.

440
00:52:24.190 --> 00:52:39.719
Rick Botelho: looking for instant clarity, instant comprehension with fast thinking, and we don't have… we haven't helped people overcome their indoctrination of fast thinking in order to enter the world of slow thinking. That's a different world.

441
00:52:40.590 --> 00:52:47.039
Rick Botelho: And we can continue the conversation offline, or next week, thank you, but I have to go at the top of the hour.

442
00:52:48.060 --> 00:53:00.659
Jerry Michalski: Thanks, Rick. Welcome to the call, Ken. As you have detected, likely, we're not particularly talking about the melting of reality and its effects, but we've had an interesting, sort of, journey through a bunch of other related things.

443
00:53:01.460 --> 00:53:02.160
Jerry Michalski: …

444
00:53:03.050 --> 00:53:10.379
Jerry Michalski: And, Alex, I think you were, interested in interjecting a moment ago. Did you want to pick that… pick up that thought, or… you good?

445
00:53:11.090 --> 00:53:14.930
Alex Kladitis: Merely jokingly say, all these things are bought…

446
00:53:15.450 --> 00:53:23.869
Alex Kladitis: being healthy by eating good food and whatever, you know, I'm all for it. Can someone give me a thing that says, if you'd eat this good food.

447
00:53:24.050 --> 00:53:27.359
Alex Kladitis: You will be healthy, but you don't need to do the exercise with it.

448
00:53:27.480 --> 00:53:33.040
Alex Kladitis: It's just… That will be the winner!

449
00:53:33.470 --> 00:53:39.260
Jerry Michalski: That'll be the winner. We'll just use electrostimulation to pretend like we're working our muscles. Klaus!

450
00:53:39.260 --> 00:53:54.670
Rick Botelho: Can I just interrupt there? All you have to do is… 150 minutes walking per week. It's not a big demand. That's the minimum, not a target, okay? So, Alex, I suspect there's a little hope for you. You may already be achieving your goals, and you don't even know that that is the minimum standard.

451
00:53:55.650 --> 00:54:12.969
Stacey Druss: And Jerry Klaus, I wanted to interject something real quick, if I could. Just to Rick's thoughts about process. I just wanted to say that there's something very useful, and I'm not talking about the whole reading, I'm just talking about the question. There's something very useful

452
00:54:13.270 --> 00:54:26.619
Stacey Druss: about just seeing where somebody tunes in, just in terms of gauging where their curiosity is. So for that, I do understand why Rick does what he does. Over.

453
00:54:26.860 --> 00:54:27.860
Jerry Michalski: Thank you, Stacey.

454
00:54:28.580 --> 00:54:30.180
Jerry Michalski: Mr. Mogger.

455
00:54:31.190 --> 00:54:47.420
Klaus Mager: Yeah, just to spin out, that's not what I wanted to… what I raised my hand for, but just to spin this out, Rick, I use biodynamics to instruct the AI, well, but you're not instructing the AI to address an audience that you have preselected.

456
00:54:47.500 --> 00:54:51.409
Klaus Mager: Right, because if you use biodynamics, …

457
00:54:51.500 --> 00:55:10.970
Klaus Mager: and say this is, you know, an orange audience, or a blue audience, or a faith-based audience, or, you know, a environmentally-focused audience, then the AI will change its language, and you will not talk about humanist-guided AI-enabled emancipatory neo-learning.

458
00:55:11.150 --> 00:55:26.159
Klaus Mager: Because no one understands what the hell that is. So that's sort of the, you know, the idea. So, I mean, maybe your, maybe your audience is, you know, quite narrow in this particular aspect, but if you want to be heard.

459
00:55:26.230 --> 00:55:34.119
Klaus Mager: you know, then… then there's a lot of people who… who wouldn't dial in on this. But what I… what I wanted to… to… to, …

460
00:55:34.320 --> 00:55:38.740
Klaus Mager: To mention, the other reality that we're facing is

461
00:55:38.890 --> 00:55:58.399
Klaus Mager: that the business community is so strong that our best intentions don't materialize. So Kennedy, for example, had this great idea, you know, take chemicals out of food, because we're killing our children, you know, reduce the amount of processed foods.

462
00:55:58.420 --> 00:56:07.000
Klaus Mager: And guess what? None of it made it through the Farm Bill, and made it through the latest regulatory hurdles. Nothing.

463
00:56:07.180 --> 00:56:07.950
Jerry Michalski: Nothing?

464
00:56:07.950 --> 00:56:27.769
Klaus Mager: And nothing. I mean, of course, the Sarah Club and all these groups that I'm traveling with, they are tracking this closely, and in great detail, and a lot of the… I mean, the Sarah Club had an internal debate before the election about supporting Kennedy or not.

465
00:56:27.830 --> 00:56:37.220
Klaus Mager: And we collectively decided that it's the best of bad options to support Kennedy because of what he stands for.

466
00:56:37.620 --> 00:56:50.169
Klaus Mager: And how strong he came on, and, you know, how much support he had. And he just crashed and burned. You know, another issue, for example, SNAP benefits, you're not allowed to buy a hot meal.

467
00:56:50.240 --> 00:56:52.859
Klaus Mager: Now, so, you have soup kitchens.

468
00:56:52.930 --> 00:57:05.349
Klaus Mager: who are not able to accept a SNAP voucher, you know, because it's a hot meal. And then you go, well, wait a second, I mean, you know, who makes up these rules? It's just an arbitrary rule.

469
00:57:05.350 --> 00:57:22.310
Klaus Mager: You know, and in all reality, you should be able to change that rule, you know, bingo, gone. You know, now you can put money into soup kitchens, which employ local people. It can be a church that has a kitchen, you know, they can buy local food, so…

470
00:57:22.310 --> 00:57:29.900
Klaus Mager: so the changes that the Heritage Foundation and, you know, these groups are really…

471
00:57:30.030 --> 00:57:45.479
Klaus Mager: want to do, and what a lot of them I agree with, are common sense-based changes, are simply getting squished, you know, in the political process by corporations that have too much power. And Trump is not…

472
00:57:45.540 --> 00:58:01.469
Klaus Mager: this Trump administration is not the group that's going to challenge this power. So that's what I'm saying, as reality unfolds over the next few months, you know, we will recognize some

473
00:58:01.470 --> 00:58:10.260
Klaus Mager: Some serious bottlenecks that, you know, where… where story changes from… from impact and reality.

474
00:58:12.140 --> 00:58:13.060
Jerry Michalski: Thanks, Hans.

475
00:58:14.260 --> 00:58:16.350
Jerry Michalski: Rick, back to you.

476
00:58:16.350 --> 00:58:30.770
Rick Botelho: Well, I just have to respond to you, Klaus. I agree with you, and let me be clear or something about something. I've heard this feedback repeatedly, and I think I… you don't know how I'm listening to you and responding to it, so it's a presumption of you, Jerry, to say I'm not listening.

477
00:58:31.130 --> 00:58:33.390
Jerry Michalski: No, no, no, I went and looked at your post.

478
00:58:33.390 --> 00:58:35.350
Rick Botelho: Go on the book that you post?

479
00:58:35.350 --> 00:58:39.970
Jerry Michalski: Which I assumed to be a reply to what we're telling you. And it was nothing like that.

480
00:58:40.890 --> 00:58:54.470
Rick Botelho: Well, I'm talking about here and now, about you saying… you said it a moment ago, you can go back and replay it, and see whether my perception of what you said was what I said. So, this is where we need to have some healthy disagreements.

481
00:58:54.470 --> 00:58:56.520
Gil Friend • Sustainability OG • CxO Coach: Could you guys take this offline?

482
00:58:57.310 --> 00:59:01.220
Jerry Michalski: And Rick, let's not, let's not tap the whole group with this.

483
00:59:01.630 --> 00:59:03.549
Rick Botelho: No, well, let me, let me just finish, Klaus.

484
00:59:03.550 --> 00:59:05.950
Gil Friend • Sustainability OG • CxO Coach: No, Rick, Rick, could you take this offline?

485
00:59:06.300 --> 00:59:07.390
Rick Botelho: No.

486
00:59:07.390 --> 00:59:09.250
Gil Friend • Sustainability OG • CxO Coach: I'm kidding jacked here.

487
00:59:09.940 --> 00:59:13.560
Rick Botelho: Well, that's okay. Let's just hang in there for a second, let me finish my.

488
00:59:13.560 --> 00:59:13.990
Jerry Michalski: That's beer.

489
00:59:13.990 --> 00:59:15.250
Rick Botelho: Before you interrupt me.

490
00:59:15.440 --> 00:59:25.549
Rick Botelho: All I was going to say, Klaus, I'm doing a search on the very question that you asked, and I will share it if people are interested in how to explain that construct.

491
00:59:26.850 --> 00:59:31.620
Rick Botelho: Using spiral dynamics. I do it differently, I do it on reading level, that's fine.

492
00:59:31.790 --> 00:59:38.579
Rick Botelho: But what I am impressed with is how this is triggering certain people and, going off-screen.

493
00:59:38.700 --> 00:59:50.239
Rick Botelho: And that's fine. I think we need to have more healthy disagreements, because I don't think we're good at it, and people don't listen. They get their… they get emotionally hijacked and leave or turn out.

494
00:59:50.640 --> 01:00:02.940
Rick Botelho: That's something that reflexivity is worthwhile using to understand that process. And I said I had to leave at the top of the hour, I'm sorry to leave, but I will leave at the request of Gil for reasons other than his request. Thank you.

495
01:00:03.930 --> 01:00:13.390
Jerry Michalski: I don't think he requested you leave, I think he just requested that we take that issue offline, which we are doing now. Anyway, excuse me. Alex, please.

496
01:00:15.180 --> 01:00:16.710
Alex Kladitis: Only, …

497
01:00:17.420 --> 01:00:32.099
Alex Kladitis: Well, the issue that we were talking earlier on, I actually wasn't here last week, so I don't know what the red conversation's really about, but I did put up Spiral Dynamics, I'm intrigued by the colors on the webpage. But, so I'll look at it later, but…

498
01:00:33.090 --> 01:00:36.520
Alex Kladitis: I think it was Klaas that said that when…

499
01:00:37.790 --> 01:00:43.020
Alex Kladitis: Whatever the current administration is doing hits the buffers.

500
01:00:43.220 --> 01:00:44.650
Alex Kladitis: People will wake up.

501
01:00:45.310 --> 01:00:46.900
Alex Kladitis: That's not my experience.

502
01:00:47.600 --> 01:01:04.700
Alex Kladitis: Remember, the reason this administration's there, as far as I'm concerned, is because the previous ones and one before, and the one before that didn't do the right thing. And we have administrations that are the opposite political perspective from the US one, and trust me, they're not doing anything either.

503
01:01:04.820 --> 01:01:10.020
Alex Kladitis: Everything we just spoke about, Hardly anything gets really done.

504
01:01:10.350 --> 01:01:15.130
Alex Kladitis: So, what I was going to say was, and I'm not challenging what you said, because I think

505
01:01:15.560 --> 01:01:16.860
Alex Kladitis: You're basically right.

506
01:01:17.190 --> 01:01:22.089
Alex Kladitis: But people's capacity to say, It was bad before, at least he tried.

507
01:01:22.910 --> 01:01:40.850
Alex Kladitis: Because that is how previous UK political parties have stayed in power and won elections, one after the other after the other, by saying, look, at least we're trying, maybe we didn't fail, but if the other people come in, they're the monsters, you don't want it. And literally, they kept getting the votes.

508
01:01:41.640 --> 01:01:45.110
Alex Kladitis: So, I'm just urging a bit of caution on…

509
01:01:45.680 --> 01:01:47.490
Alex Kladitis: When the train hits the buffer.

510
01:01:47.790 --> 01:01:49.830
Alex Kladitis: That people are going to change their minds.

511
01:01:50.450 --> 01:02:03.500
Alex Kladitis: And then there comes the politics, how many are middle-of-the-ground people, and what happens, and, you know, gerrymandering, and all the rest of it. But that was just my… people's capacity to excuse is quite dramatically

512
01:02:04.890 --> 01:02:06.320
Alex Kladitis: It's very deep.

513
01:02:07.170 --> 01:02:10.009
Alex Kladitis: That was my, my, my point, really.

514
01:02:10.010 --> 01:02:14.179
Jerry Michalski: Thanks, Alex. To excuse, and also, I think, to forget.

515
01:02:14.930 --> 01:02:20.680
Jerry Michalski: And maybe just to adapt. Like, oh, new reality, okay, I guess I'll just live with this new reality.

516
01:02:21.270 --> 01:02:25.240
Jerry Michalski: Partly also, I think, we're overwhelmed.

517
01:02:26.700 --> 01:02:35.220
Alex Kladitis: Yeah, exactly. But they also… their ability for the UK side, their ability to paint the other side as the monsters was quite…

518
01:02:35.410 --> 01:02:36.560
Alex Kladitis: Impressive.

519
01:02:37.090 --> 01:02:41.710
Alex Kladitis: With the support of the press, mainly, the mainstream media.

520
01:02:42.040 --> 01:02:42.910
Jerry Michalski: Right. And….

521
01:02:42.910 --> 01:02:47.970
Alex Kladitis: I don't know how that is reflected in the US, because I don't look at any US press, really, so….

522
01:02:52.770 --> 01:03:01.419
Jerry Michalski: This has been a strange call for me, because I feel like we've sort of glided nowhere near the topic I thought I had proposed, which is fine, that's how we roll.

523
01:03:01.610 --> 01:03:02.180
Alex Kladitis: Can you propose.

524
01:03:02.180 --> 01:03:02.559
Jerry Michalski: Oh my god.

525
01:03:02.560 --> 01:03:05.779
Alex Kladitis: So we can maybe pick up on it now.

526
01:03:06.090 --> 01:03:08.049
Jerry Michalski: Well, the question was.

527
01:03:08.220 --> 01:03:15.149
Jerry Michalski: If the things we're pointing to, if the artifacts we're pointing to, are now no longer reliable, because anything can be faked or made up.

528
01:03:15.400 --> 01:03:23.570
Jerry Michalski: and somebody can pose as Obama, like, supporting Trump, which I think will be a cold day in hell when that happens.

529
01:03:23.820 --> 01:03:39.300
Jerry Michalski: But, you know, you could make a very credible deepfake of that and post it, and then once everybody's sort of written about it, it sort of melds its way into the human consciousness, and we adapt, and all these things are having some effect on society.

530
01:03:39.750 --> 01:03:55.939
Jerry Michalski: And, there might be a couple silver linings. I tried to point to one, which is we might be driven back to an analog presence with each other, which I would like. I think that's a good answer… a good outcome. But what else, and what… how do we handle this? You know, if we think that this is imminent.

531
01:03:56.020 --> 01:04:02.750
Jerry Michalski: And, and then there's also, will these large language models eat their own tail?

532
01:04:02.990 --> 01:04:18.110
Jerry Michalski: Because the synthetic output that they're training themselves on is now going to be more numerous, more voluminous than the human output that we're worried that they're training themselves on, which is a series of complicated questions all interacting right there.

533
01:04:19.600 --> 01:04:22.789
Jerry Michalski: What happens? How does this play out? Go ahead, Ox.

534
01:04:25.130 --> 01:04:27.329
Alex Kladitis: Can I make a couple points on this? Which was….

535
01:04:27.330 --> 01:04:27.940
Jerry Michalski: Yeah.

536
01:04:28.400 --> 01:04:33.300
Alex Kladitis: Mostly, the, … Okay.

537
01:04:33.560 --> 01:04:35.090
Alex Kladitis: How do I say this?

538
01:04:35.890 --> 01:04:36.940
Alex Kladitis: people

539
01:04:37.480 --> 01:04:52.159
Alex Kladitis: you'll be… okay, we… we question things. We're interested in the… a bit more, the next layer down, let's say. Some of you are really, really good thinkers all the way very deep, but generally, in our forum, we question the next layer of

540
01:04:52.320 --> 01:04:57.420
Alex Kladitis: let me call it reality, because that's what I… that's what I'll call it in a lazy academic fashion.

541
01:04:58.130 --> 01:05:07.530
Alex Kladitis: You… what is your experience? Because my experience is that a hell of a lot of people do not care anymore. They care about the fact that things are getting expensive.

542
01:05:07.850 --> 01:05:10.849
Alex Kladitis: But I don't think they care whether the news is real or not.

543
01:05:11.510 --> 01:05:16.420
Alex Kladitis: And… how is that going to change? So, it's your point, Gerry.

544
01:05:16.560 --> 01:05:25.720
Alex Kladitis: But I'm thinking, your point's relevant to us, very relevant to us. There's a whole load of people that watch the news, don't listen to the radio news, there isn't music.

545
01:05:26.020 --> 01:05:29.229
Alex Kladitis: And is that something that you guys experience as well?

546
01:05:30.540 --> 01:05:41.849
Jerry Michalski: Sure. This is one of the outcome… this is one of the outcomes of the flood the zone with shit strategy that is currently being employed, is that when you confuse everybody about everything, they give up.

547
01:05:41.890 --> 01:06:00.449
Jerry Michalski: And when they've given up, they're way easier to steer towards stuff, and they're… sometimes they'll seize on things that are particularly stupid or strong because they seem strong, for example. But we're seeing that a ton. I think that… that that dynamic has been influencing our last several electoral cycles, etc, etc. And then.

548
01:06:00.590 --> 01:06:03.799
Jerry Michalski: When people are overwhelmed and confused, they give up.

549
01:06:06.300 --> 01:06:07.650
Jerry Michalski: Anybody else's….

550
01:06:07.650 --> 01:06:08.310
Alex Kladitis: What does that mean?

551
01:06:08.600 --> 01:06:13.700
Jerry Michalski: What else? What does that mean? What… Anybody else worried about this?

552
01:06:19.180 --> 01:06:19.600
Gil Friend • Sustainability OG • CxO Coach: Yes.

553
01:06:19.600 --> 01:06:20.920
Jerry Michalski: I remember… go ahead, Klaus.

554
01:06:20.920 --> 01:06:28.020
Klaus Mager: It brings me back to a different way of looking at this breaking reality, yeah?

555
01:06:28.030 --> 01:06:41.229
Klaus Mager: Because at some point in time, you can be flooded with images that are confusing, and stories that are depicting a reality that simply doesn't exist.

556
01:06:41.340 --> 01:06:47.769
Klaus Mager: But when it hits you, and you lose your health insurance, and you lose your food access.

557
01:06:47.890 --> 01:06:57.200
Klaus Mager: Then that's a different story. That's when… when it gets real. Now, and then you can only blame so many. So… so that's…

558
01:06:57.300 --> 01:07:16.050
Klaus Mager: And that's, you know, what I was referring to earlier for when Dave Poles, you know, laid this thing out. Historically, you know, that's when an autocrat turns into suppression… suppression and violence.

559
01:07:16.220 --> 01:07:22.109
Klaus Mager: And then you can't control the story anymore, no matter what fancy tools you have.

560
01:07:22.200 --> 01:07:41.829
Klaus Mager: People don't respond to, to being misled any longer because it's real, it's personal, it's, it's, it's hurting you, you know, and, and then, and then… I mean, look, they're already, they're already setting in place, preventive measures for people to get into the street.

561
01:07:42.000 --> 01:08:00.179
Klaus Mager: Right? I mean, you're already deploying a militarized force, so in case you have protests breaking out, they're already ready for that. So this is… I mean, we're gonna head into October, November is when these cuts really start hitting.

562
01:08:00.320 --> 01:08:01.750
Jerry Michalski: ….

563
01:08:02.340 --> 01:08:07.840
Klaus Mager: And no one's focused on it. People are simply, maybe they're not focused on it.

564
01:08:08.280 --> 01:08:08.960
Jerry Michalski: So…

565
01:08:09.120 --> 01:08:22.039
Jerry Michalski: again, I'll say some people are very focused on this, like, really worried about it, and I think there's an interesting sense… sense-making question here. So, one of the things I've been tracking is, hey, I… logically.

566
01:08:22.130 --> 01:08:41.780
Jerry Michalski: if you look at the Trump tariff war, that should be causing economic chaos. Then prices should be rising, we should be spiking inflation, stuff like that. Not happening yet. Employment is really full, inflation hasn't spiked, and there's a bunch of economic thinkers and other and geopolitical thinkers who are trying to figure out, is there a lag period? What's going on? But I think

567
01:08:41.779 --> 01:08:58.060
Jerry Michalski: posing some hypotheses and watching the data to figure out what happens. And Trump is, of course, strong-arming companies to swallow the price hikes. He's like, yeah, you're not going to raise prices, or I'm going to shame you in public, and we've figured out that that's a really powerful tool that Trump has invented.

568
01:08:58.380 --> 01:09:14.840
Jerry Michalski: like, Trump has more elbow room than any politician in recent memory for me. He can do kind of anything, and that's really impressive to his followers. Gotta… gotta… I have to admit that, that he…

569
01:09:14.970 --> 01:09:28.400
Jerry Michalski: Obama did 8 years of no scandals. One day he wears a light gray suit at a press briefing, and that's the biggest scandal of the Obama administration. But he ran 8 years as president of the superpower in the world in a straitjacket.

570
01:09:29.100 --> 01:09:31.340
Jerry Michalski: He was in a straitjacket.

571
01:09:31.609 --> 01:09:44.009
Jerry Michalski: And didn't try to tear the straitjacket. This is my own read of, like, the administration in the rearview mirror, just in the rearview mirror. And I wish he had torn the straitjacket, and so does the country.

572
01:09:44.050 --> 01:10:02.909
Jerry Michalski: the country said, okay, we'll give this guy a go, and then they voted for him again. I remember an interview in 2016, just before Trump gets elected. I remember one journalist went across the country, and they meet somebody in West Virginia, and they pick up this woman's grandfather who joins them in the car, and he's talking, he says, I voted for the black guy twice.

573
01:10:03.640 --> 01:10:07.469
Jerry Michalski: And things aren't any better. And so… and so he votes for Trump.

574
01:10:07.760 --> 01:10:32.069
Jerry Michalski: And I'm like, that's what… that's, like, the reality of what's on the ground for me, but if we're a sense-making community, then asking questions like, hey, when you kick out Medicare, when you take away SNAP, when you do all these things, it's going to have these effects, and Klaus, when you say, this kicks in in October, November, that's really nice, that's empirical. I can go… and I don't have the time or the expertise to watch the actual numbers on the

575
01:10:32.070 --> 01:10:34.119
Jerry Michalski: thread database, or wherever, but…

576
01:10:34.120 --> 01:10:42.979
Jerry Michalski: I'm looking for people who are doing exactly that, people whose voices I trust, which is a second layer or second order question from last week's call, kinda.

577
01:10:43.040 --> 01:11:02.769
Jerry Michalski: But I'm very interested in that question and how it plays out, but I'm not willing to presume those results, because sometimes complex systems adjust and adapt, and the people get fed some other way, which I think Kalia had to drop off the call, but I think that's what Kalia and I would really wish, that the entire way we get food and shelter just changes.

578
01:11:03.180 --> 01:11:12.649
Jerry Michalski: And if the reason, if the motive force for the change was somebody tore apart a government program that was feeding a lot of people, then that sucks, but that's interesting.

579
01:11:12.920 --> 01:11:23.710
Jerry Michalski: And so I'm not willing to pre… I'm worried sick, but I'm not willing to prejudge the outcome, or to assume an outcome, because I see all too often that outcomes we thought were going to happen

580
01:11:23.710 --> 01:11:37.830
Jerry Michalski: I have the T91. I was pretty sure that Trump was going to be corralled by one of the legitimate cases. There were a couple cases that should never have been brought, but there were some very legitimate cases that I thought were going to stick, and guess what? Velcro.

581
01:11:37.830 --> 01:11:40.310
Jerry Michalski: Absolutely, no, not Velcro, Teflon.

582
01:11:40.320 --> 01:11:48.410
Jerry Michalski: Like, Teflon Ron, like, you know, Reagan, nothing would stick to him. But, you know, Donnie is more Teflon than Ronnie.

583
01:11:48.460 --> 01:11:51.550
Jerry Michalski: That was kind of… that was kind of rhymey.

584
01:11:52.130 --> 01:11:55.669
Jerry Michalski: Anyway, I'll get off my soapbox for a second and go to Doug.

585
01:11:57.780 --> 01:12:05.189
Doug Breitbart: … There's something about, like, the empirical thing about things that work.

586
01:12:06.350 --> 01:12:13.290
Doug Breitbart: And I… yesterday, there was this fascinating piece on one of the channels about how,

587
01:12:13.470 --> 01:12:19.260
Doug Breitbart: Gavin Newsom has been channeling Trump But with his message.

588
01:12:19.450 --> 01:12:23.049
Jerry Michalski: Oh my… and the tweets and whatever's are hilarious.

589
01:12:23.050 --> 01:12:28.219
Doug Breitbart: And, and, and… energetically, It's working.

590
01:12:30.020 --> 01:12:31.990
Doug Breitbart: in a very Trumpian way.

591
01:12:32.340 --> 01:12:46.989
Doug Breitbart: And it's sort of the first time somebody said, well, if it's working for him, maybe I should do what's working for him for me, and see what happens. And what's working is the vernacular.

592
01:12:49.450 --> 01:12:57.680
Doug Breitbart: And to a certain extent, what he's proving is that it's not Trump in persona, it's his vernacular.

593
01:12:59.290 --> 01:13:05.649
Doug Breitbart: In a very Andy Warhol, everybody's famous for 15-minute kind of way.

594
01:13:07.120 --> 01:13:14.560
Doug Breitbart: As an energetic, semantic, You know, textural, cultural.

595
01:13:15.430 --> 01:13:22.109
Doug Breitbart: Expression, mode of expression, of a moment, in history.

596
01:13:23.220 --> 01:13:27.139
Doug Breitbart: That, energetically.

597
01:13:30.490 --> 01:13:32.150
Doug Breitbart: activates people.

598
01:13:32.590 --> 01:13:34.709
Doug Breitbart: Is pushing people's buttons.

599
01:13:34.870 --> 01:13:44.310
Doug Breitbart: And I wouldn't discount dimensionally all the moving parts of that, which is also using X as a channel of communication.

600
01:13:44.550 --> 01:13:50.510
Doug Breitbart: Like, you know, there, there are, like, all of these… dimensions.

601
01:13:51.040 --> 01:13:54.590
Doug Breitbart: To the phenomena of what's going on culturally.

602
01:13:54.760 --> 01:13:59.410
Doug Breitbart: And so, in the face of… Either…

603
01:13:59.830 --> 01:14:08.259
Doug Breitbart: Cultural norms, morals, ethics, values, institutions, practices, Either becoming meh.

604
01:14:10.600 --> 01:14:16.629
Doug Breitbart: in a digital vernacular, becoming irrelevant, like, okay, I don't care.

605
01:14:16.920 --> 01:14:21.650
Doug Breitbart: Like, not relevant to me, not an active ingredient in my experience.

606
01:14:22.380 --> 01:14:25.470
Doug Breitbart: Perceptually, on a human, experiential level.

607
01:14:26.490 --> 01:14:32.990
Doug Breitbart: Or… I'm sick and tired of it, or I don't like it, or whatever.

608
01:14:33.560 --> 01:14:36.000
Doug Breitbart: But there's a… there is a…

609
01:14:36.960 --> 01:14:40.400
Doug Breitbart: There has been a de facto shift.

610
01:14:43.120 --> 01:14:49.420
Doug Breitbart: in… The way people are communicating with each other.

611
01:14:49.750 --> 01:14:55.110
Doug Breitbart: And the way they're responding, or not, To what matters to them.

612
01:14:57.270 --> 01:15:08.790
Doug Breitbart: And… There's a quaintness… To the folks that are trapped in the previous cultural, post-industrial, Freed.

613
01:15:09.000 --> 01:15:09.980
Jerry Michalski: Do you mean us?

614
01:15:10.310 --> 01:15:19.510
Doug Breitbart: Yeah. Yeah, I didn't want to… I didn't want to stir an… I didn't want to stir an attribution, but yeah!

615
01:15:19.640 --> 01:15:20.110
Doug Breitbart: Yeah.

616
01:15:20.380 --> 01:15:24.130
Doug Breitbart: It's sort of like, you know, pining for the good old days.

617
01:15:24.700 --> 01:15:29.800
Doug Breitbart: And, and, and, like, the concept that data is important.

618
01:15:30.090 --> 01:15:36.600
Doug Breitbart: the concept that truth and accuracy in reporting is important, and representation is important.

619
01:15:36.600 --> 01:15:37.430
Jerry Michalski: You're right.

620
01:15:37.430 --> 01:15:43.849
Doug Breitbart: these things are, like, really, really quaint. But, you know, that ship has sailed!

621
01:15:44.560 --> 01:15:53.690
Doug Breitbart: And QAnon, and gaming culture, and Generation, you know, X, Y, and Z, like.

622
01:15:53.940 --> 01:15:56.440
Doug Breitbart: They were raised in a post-digital era.

623
01:15:56.890 --> 01:15:59.570
Doug Breitbart: And it's a whole different orientation.

624
01:16:00.800 --> 01:16:02.000
Doug Breitbart: …

625
01:16:02.110 --> 01:16:14.810
Doug Breitbart: And I… I'll be the first one to say, I'm coming up on 70, and I have no capacity to tell you what that gestalt is. I don't recognize it, I don't understand it, and I'm…

626
01:16:14.860 --> 01:16:25.989
Doug Breitbart: wrestling with how to gain insight into it. And it doesn't help reading articles about people doing observations about Gen XYZ employees. It's like…

627
01:16:26.060 --> 01:16:30.829
Doug Breitbart: It is a very… it's a different species, it's a different animal of human.

628
01:16:31.430 --> 01:16:40.529
Doug Breitbart: That, is much more… first, I think they're much smarter. I think they see things much clearer.

629
01:16:40.730 --> 01:16:49.280
Doug Breitbart: In terms of bottom lines, And I think there's a general rejection of…

630
01:16:51.150 --> 01:16:55.010
Doug Breitbart: The principles on which the old system operated.

631
01:16:55.640 --> 01:17:05.319
Doug Breitbart: Because they can't buy a house, and they don't see a future in which they will be able to, even earning, you know, $200,000, $300,000 a year.

632
01:17:05.320 --> 01:17:05.970
Klaus Mager: Yeah.

633
01:17:06.930 --> 01:17:10.220
Jerry Michalski: You're painting a very nice description of a future OGM topic.

634
01:17:10.220 --> 01:17:26.870
Doug Breitbart: They cannot rationalize any of the old stuff meaning anything, because it doesn't mean anything to them. They're trying to calculate for X on how to get what they want, and have the lifestyle they want, and do the things they want to do with the friends they have.

635
01:17:26.870 --> 01:17:37.659
Doug Breitbart: None of them, by the way, are on LinkedIn, or Facebook, or X, or any of these services. They're all outside of social media completely.

636
01:17:38.210 --> 01:17:47.129
Doug Breitbart: My son checks, you know, it's all he can do to check his email, and he said last visit, like, as soon as I can get rid of a smartphone, I will be.

637
01:17:48.350 --> 01:17:50.300
Doug Breitbart: Has no interest in that.

638
01:17:50.930 --> 01:17:56.839
Doug Breitbart: So, like… You know, what are we looking at?

639
01:17:57.770 --> 01:18:01.540
Doug Breitbart: Like, what's… you know… and I think Gavin Newsom is on to something.

640
01:18:02.060 --> 01:18:03.949
Klaus Mager: I don't know what that is.

641
01:18:03.950 --> 01:18:04.860
Doug Breitbart: That's right.

642
01:18:04.860 --> 01:18:07.630
Jerry Michalski: But I think he's not as complete. And I'm complete.

643
01:18:07.630 --> 01:18:08.250
Klaus Mager: I gotta leave.

644
01:18:08.430 --> 01:18:09.020
Jerry Michalski: Fair enough, guys.

645
01:18:09.620 --> 01:18:14.749
Jerry Michalski: I have a hard stop at the half hour, and I'm eager to hear Ken if he has a poem, but given that.

646
01:18:15.660 --> 01:18:17.139
Jerry Michalski: Let's go to kill.

647
01:18:18.680 --> 01:18:22.419
Gil Friend • Sustainability OG • CxO Coach: I'll be as quick as I can. Douglas, thank you for the rant.

648
01:18:22.420 --> 01:18:46.999
Gil Friend • Sustainability OG • CxO Coach: Gavin's really interesting. People yelled at him a few months ago when he was having right-wing influencers on his podcast, thinking he had sold out. And I thought, no, he has not at all. He's, number one, researching, and number two, positioning. And he's going all in on this. He's clearly declared for president. And he's going right into the heart of MAGA, and it's brilliant, crazy stuff, and it may work, and it may backfire, and who knows?

649
01:18:47.000 --> 01:19:00.230
Gil Friend • Sustainability OG • CxO Coach: Scott Galloway, Prop G, had a podcast in the last day or two saying that, like, everybody's running for president. It's so fascinating. He said everybody's calling him to say, can I be on your show? Can you give me money?

650
01:19:00.550 --> 01:19:18.130
Gil Friend • Sustainability OG • CxO Coach: is to say, I'm running for president. He's really worth following. Very… a lot of stuff on the cohort that Doug is talking about, and young men in particular, and the alienation throughout society. And it reminds me, Jerry, to what you were saying about before, or maybe somebody was, that

651
01:19:18.250 --> 01:19:22.319
Gil Friend • Sustainability OG • CxO Coach: We are a really, really, really rarefied group of people here.

652
01:19:22.540 --> 01:19:38.640
Gil Friend • Sustainability OG • CxO Coach: we pay attention to shit that most people don't pay any attention to, and don't know about, and don't care about. You know, I mean, Jerry, you talked about the three major networks, you know, years ago. You know, most people look at news, get their news from Fox News, but most people get their news from TikTok.

653
01:19:39.340 --> 01:19:48.410
Gil Friend • Sustainability OG • CxO Coach: friends, or other things. So, like, you know, we're… we operate in a strange little bubble ourselves. And the last thing I wanted to say, and then toss it over to Ken, is that

654
01:19:48.410 --> 01:20:01.409
Gil Friend • Sustainability OG • CxO Coach: I think… I think both Klaus and Jerry talked about how it might be good if everything completely falls to shit, and then there'll be impetus for change. In the completely falling to shit, millions of people die.

655
01:20:01.820 --> 01:20:04.289
Gil Friend • Sustainability OG • CxO Coach: And tens of millions of people suffer.

656
01:20:04.290 --> 01:20:23.090
Gil Friend • Sustainability OG • CxO Coach: Horribly. And we've already… people are already estimating the death count from the MAGA moves of just the last 8 months. And this was always… this was always a thing that I heard from some of my Marxist-Leninist friends back in the old days, where they talked about heightening the contradictions. It's really good if things get worse, because then the revolutionary fervor will rise. Well, no.

657
01:20:23.470 --> 01:20:32.659
Gil Friend • Sustainability OG • CxO Coach: No, it's really bad if things get worse, because people die, and the wrong kinds of revolutionary fervor arrive, and I… I'm not playing that game. So I just wanted to…

658
01:20:32.850 --> 01:20:36.410
Gil Friend • Sustainability OG • CxO Coach: throw that back at you. Sorry.

659
01:20:36.930 --> 01:20:41.340
Gil Friend • Sustainability OG • CxO Coach: I didn't expect… I didn't expect that much ire to rise with it.

660
01:20:41.340 --> 01:20:41.980
Jerry Michalski: Yeah, yeah, yeah.

661
01:20:42.670 --> 01:20:46.039
Jerry Michalski: I'm hoping Stacy has something brighter and warmer to share with us.

662
01:20:46.290 --> 01:20:46.780
Gil Friend • Sustainability OG • CxO Coach: Hope so.

663
01:20:46.780 --> 01:20:48.740
Stacey Druss: Quicker. Quicker.

664
01:20:48.740 --> 01:20:49.260
Gil Friend • Sustainability OG • CxO Coach: Sean.

665
01:20:49.260 --> 01:20:49.820
Stacey Druss: Bye, Sean.

666
01:20:49.820 --> 01:20:50.200
Jerry Michalski: price on.

667
01:20:50.200 --> 01:20:56.529
Stacey Druss: Yeah, I just want to say, going back to what Doug was saying about how quaint

668
01:20:57.260 --> 01:21:08.279
Stacey Druss: this group… I was just thinking, how quaint to think that we ever had honest day. Somebody last week said something, and I kind of took offense to it. I was like.

669
01:21:08.550 --> 01:21:12.690
Stacey Druss: Don't include me in that we, but the truth is, they said.

670
01:21:12.820 --> 01:21:16.840
Stacey Druss: Everybody exaggerates and lies to make their point.

671
01:21:17.130 --> 01:21:21.870
Stacey Druss: And for the most part, that's kinda true, and it's always been that way.

672
01:21:21.990 --> 01:21:25.819
Stacey Druss: So, I just wanted to mention that.

673
01:21:26.160 --> 01:21:26.920
Jerry Michalski: Thank you.

674
01:21:27.550 --> 01:21:33.710
Jerry Michalski: That seems like one of those truths, like, that 63.7% of all statistics are made up on the spot.

675
01:21:37.120 --> 01:21:38.300
Jerry Michalski: Sir Ken.

676
01:21:39.460 --> 01:21:42.419
Jerry Michalski: Over to you in the booth, if you have a poem.

677
01:21:42.420 --> 01:21:43.690
Ken Homer • SF Bay Area: I do have a poem.

678
01:21:44.650 --> 01:21:50.389
Ken Homer • SF Bay Area: This one's called Afraid So by Jean Marie Beaumont. Is it starting to rain?

679
01:21:51.130 --> 01:21:52.720
Ken Homer • SF Bay Area: Did the check bounce?

680
01:21:52.940 --> 01:21:54.539
Ken Homer • SF Bay Area: Are we out of coffee?

681
01:21:55.020 --> 01:21:56.569
Ken Homer • SF Bay Area: Is this going to hurt?

682
01:21:57.350 --> 01:21:58.969
Ken Homer • SF Bay Area: Could you lose your job?

683
01:21:59.550 --> 01:22:00.969
Ken Homer • SF Bay Area: Did the glass break?

684
01:22:01.740 --> 01:22:03.729
Ken Homer • SF Bay Area: Was the baggage misrouted?

685
01:22:04.510 --> 01:22:06.420
Ken Homer • SF Bay Area: Will this go on my record?

686
01:22:06.760 --> 01:22:08.500
Ken Homer • SF Bay Area: Are you missing much money?

687
01:22:09.080 --> 01:22:10.730
Ken Homer • SF Bay Area: Was anybody injured?

688
01:22:11.290 --> 01:22:12.779
Ken Homer • SF Bay Area: Is the traffic heavy?

689
01:22:13.630 --> 01:22:15.649
Ken Homer • SF Bay Area: Do I have to remove my clothes?

690
01:22:16.340 --> 01:22:17.820
Ken Homer • SF Bay Area: Will it leave a scar?

691
01:22:18.600 --> 01:22:19.750
Ken Homer • SF Bay Area: Must you go?

692
01:22:20.320 --> 01:22:21.919
Ken Homer • SF Bay Area: Will this be in the papers?

693
01:22:22.530 --> 01:22:24.529
Ken Homer • SF Bay Area: Is my time up already?

694
01:22:25.230 --> 01:22:26.980
Ken Homer • SF Bay Area: Are we seeing the understudy?

695
01:22:27.770 --> 01:22:29.570
Ken Homer • SF Bay Area: Would affect my eyesight?

696
01:22:30.240 --> 01:22:32.260
Ken Homer • SF Bay Area: Did all the books burn?

697
01:22:32.620 --> 01:22:34.370
Ken Homer • SF Bay Area: Are you still smoking?

698
01:22:35.070 --> 01:22:36.660
Ken Homer • SF Bay Area: Is the bone broken?

699
01:22:37.490 --> 01:22:39.519
Ken Homer • SF Bay Area: Will I have to put him to sleep?

700
01:22:40.190 --> 01:22:41.739
Ken Homer • SF Bay Area: Was the car totaled?

701
01:22:42.590 --> 01:22:45.049
Ken Homer • SF Bay Area: Am I responsible for these charges?

702
01:22:45.680 --> 01:22:47.220
Ken Homer • SF Bay Area: Are you contagious?

703
01:22:48.080 --> 01:22:49.759
Ken Homer • SF Bay Area: Will we have to wait long?

704
01:22:50.250 --> 01:22:51.889
Ken Homer • SF Bay Area: Is the runway icy?

705
01:22:52.710 --> 01:22:54.419
Ken Homer • SF Bay Area: Was the gun loaded?

706
01:22:54.970 --> 01:22:56.940
Ken Homer • SF Bay Area: Could this cause side effects?

707
01:22:57.780 --> 01:22:59.759
Ken Homer • SF Bay Area: Do you know who betrayed you?

708
01:23:00.510 --> 01:23:02.469
Ken Homer • SF Bay Area: Is the wound infected?

709
01:23:03.250 --> 01:23:04.529
Ken Homer • SF Bay Area: Are we lost?

710
01:23:05.460 --> 01:23:07.039
Ken Homer • SF Bay Area: Can it get any worse?

711
01:23:10.510 --> 01:23:11.300
Ken Homer • SF Bay Area: Oh.

712
01:23:12.070 --> 01:23:14.699
Jerry Michalski: That was like a tour of the universe.

713
01:23:15.970 --> 01:23:18.709
Jerry Michalski: That was like a tour of the universe, damn.

714
01:23:18.710 --> 01:23:23.180
Ken Homer • SF Bay Area: Amazing. What was the title again? Afraid So. Afraid So. Afraid So.

715
01:23:23.180 --> 01:23:25.049
Jerry Michalski: Oh my god.

716
01:23:25.280 --> 01:23:26.750
Ken Homer • SF Bay Area: Wow. You wanna hear it one more time?

717
01:23:26.750 --> 01:23:27.570
Jerry Michalski: Oh, yes, please.

718
01:23:27.570 --> 01:23:29.620
Gil Friend • Sustainability OG • CxO Coach: Let's been nice to meet you, too.

719
01:23:29.620 --> 01:23:32.599
Jerry Michalski: Gil might, Gil might drop out halfway.

720
01:23:32.950 --> 01:23:35.249
Ken Homer • SF Bay Area: I'm afraid so, by Jean Marie Beaumont.

721
01:23:35.250 --> 01:23:37.110
Gil Friend • Sustainability OG • CxO Coach: Is it starting to rain?

722
01:23:37.450 --> 01:23:38.950
Ken Homer • SF Bay Area: Did the check bounce?

723
01:23:39.210 --> 01:23:40.649
Ken Homer • SF Bay Area: Are we out of coffee?

724
01:23:41.330 --> 01:23:42.749
Ken Homer • SF Bay Area: Is this going to hurt?

725
01:23:43.600 --> 01:23:45.179
Ken Homer • SF Bay Area: Could you lose your job?

726
01:23:45.810 --> 01:23:47.220
Ken Homer • SF Bay Area: Did the glass break?

727
01:23:47.790 --> 01:23:49.709
Ken Homer • SF Bay Area: Was the baggage misrouted?

728
01:23:50.400 --> 01:23:52.170
Ken Homer • SF Bay Area: Will this go on my record?

729
01:23:52.800 --> 01:23:54.430
Ken Homer • SF Bay Area: Are you missing much money?

730
01:23:55.130 --> 01:23:56.759
Ken Homer • SF Bay Area: Was anybody injured?

731
01:23:57.470 --> 01:23:58.940
Ken Homer • SF Bay Area: Is the traffic heavy?

732
01:23:59.740 --> 01:24:01.740
Ken Homer • SF Bay Area: Do I have to remove my clothes?

733
01:24:02.650 --> 01:24:04.080
Ken Homer • SF Bay Area: Will it leave a scar?

734
01:24:04.700 --> 01:24:05.889
Ken Homer • SF Bay Area: Must you go?

735
01:24:06.720 --> 01:24:08.399
Ken Homer • SF Bay Area: Will this be in the papers?

736
01:24:09.120 --> 01:24:11.040
Ken Homer • SF Bay Area: Is my time up already?

737
01:24:11.500 --> 01:24:13.189
Ken Homer • SF Bay Area: Are we seeing the understudy?

738
01:24:14.100 --> 01:24:15.730
Ken Homer • SF Bay Area: Will it affect my eyesight?

739
01:24:16.260 --> 01:24:17.969
Ken Homer • SF Bay Area: Did all the books burn?

740
01:24:18.590 --> 01:24:20.160
Ken Homer • SF Bay Area: Are you still smoking?

741
01:24:20.810 --> 01:24:22.280
Ken Homer • SF Bay Area: Is the bone broken?

742
01:24:23.140 --> 01:24:24.940
Ken Homer • SF Bay Area: Will I have to put him to sleep?

743
01:24:25.690 --> 01:24:27.200
Ken Homer • SF Bay Area: Was the car totaled?

744
01:24:27.870 --> 01:24:30.450
Ken Homer • SF Bay Area: Am I responsible for these charges?

745
01:24:30.930 --> 01:24:32.340
Ken Homer • SF Bay Area: Are you contagious?

746
01:24:33.250 --> 01:24:34.769
Ken Homer • SF Bay Area: Will we have to wait long?

747
01:24:35.630 --> 01:24:37.220
Ken Homer • SF Bay Area: Is the runway icy?

748
01:24:37.940 --> 01:24:39.550
Ken Homer • SF Bay Area: Was the gun loaded?

749
01:24:40.270 --> 01:24:42.100
Ken Homer • SF Bay Area: Could this cause side effects?

750
01:24:42.870 --> 01:24:44.920
Ken Homer • SF Bay Area: Do you know who betrayed you?

751
01:24:45.710 --> 01:24:47.430
Ken Homer • SF Bay Area: Is the wound infected?

752
01:24:48.350 --> 01:24:49.620
Ken Homer • SF Bay Area: Are we lost?

753
01:24:50.370 --> 01:24:51.920
Ken Homer • SF Bay Area: Can it get any worse?

754
01:24:54.230 --> 01:24:54.860
Jerry Michalski: Yum.

755
01:24:56.380 --> 01:24:58.050
Jerry Michalski: Thank you so much, that's sad.

756
01:24:59.410 --> 01:25:01.539
Doug Breitbart: That's a gift, that's a gift.

757
01:25:01.540 --> 01:25:02.659
Jerry Michalski: Useless and stuff.

758
01:25:02.660 --> 01:25:05.560
Doug Breitbart: Priceless and troubling all at once.

759
01:25:06.260 --> 01:25:06.790
Jerry Michalski: ….

760
01:25:06.790 --> 01:25:10.540
Gil Friend • Sustainability OG • CxO Coach: It's so versatile, like, it's really handy. Any time, pull that one out.

761
01:25:11.620 --> 01:25:12.410
Jerry Michalski: Great, so….

762
01:25:14.210 --> 01:25:15.210
Gil Friend • Sustainability OG • CxO Coach: Thank you.

763
01:25:15.690 --> 01:25:19.560
Jerry Michalski: Let's, call this one now, and, thank you all.

764
01:25:19.730 --> 01:25:23.430
Jerry Michalski: See you in a week, and some of you sooner in different venues, but ….

765
01:25:23.430 --> 01:25:24.150
Gil Friend • Sustainability OG • CxO Coach: Okay.

766
01:25:24.150 --> 01:25:25.369
Jerry Michalski: Let's be careful out there.

767
01:25:25.370 --> 01:25:26.810
Gil Friend • Sustainability OG • CxO Coach: Okay, Carol. Bye-bye.

768
01:25:26.970 --> 01:25:27.480
Jerry Michalski: Done.

