WEBVTT

1
00:00:00.000 --> 00:00:00.660
Jerry Michalski: Good.

2
00:00:02.640 --> 00:00:09.499
Jerry Michalski: Welcome to the Open Global Mind weekly call on Thursday, October 9th, 2025.

3
00:00:10.270 --> 00:00:28.900
Jerry Michalski: Last week, in lieu of a check-in, which is what we normally do on the first month… first Thursdays of every month, we started talking about OGM and our process, and our protocols and how we… how we go about doing stuff. A conversation which went really nicely, and took us into some really interesting questions that are

4
00:00:29.200 --> 00:00:32.989
Jerry Michalski: Beyond the immediate needs of how do we do what we do, but rather

5
00:00:33.250 --> 00:00:41.230
Jerry Michalski: how do online communities actually communicate, and where is this whole damn thing going? And how does AI factor into the mix in a productive way?

6
00:00:41.290 --> 00:00:56.529
Jerry Michalski: So I'd like to continue that quest today. I'd like to see where we go. A couple people, Thorbjorn Mann and John Warner, sent very thoughtful replies to my invite to this topic, to the OGM list.

7
00:00:57.490 --> 00:00:59.480
Jerry Michalski: which I was just kind of reviewing.

8
00:00:59.650 --> 00:01:01.200
Jerry Michalski: And,

9
00:01:02.490 --> 00:01:14.069
Jerry Michalski: And I'm wondering if there's any… first, if anybody has any afterthoughts from last week, or just on the topic in general. I just want to open the floor and see what's… what's on… what's on our minds about this.

10
00:01:14.250 --> 00:01:15.509
Jerry Michalski: Yeah, and here's John.

11
00:01:17.250 --> 00:01:18.150
Jerry Michalski: Stacy.

12
00:01:18.800 --> 00:01:26.219
Stacey Druss: Well, I just had one afterthought, and Jesse actually covered it really well, but.

13
00:01:26.560 --> 00:01:28.630
John Warinner: Better do runs with this.

14
00:01:28.630 --> 00:01:29.180
Stacey Druss: You bet.

15
00:01:29.890 --> 00:01:31.839
Stacey Druss: Is there a… is there an echo?

16
00:01:31.860 --> 00:01:36.239
Jerry Michalski: That was John, and I just… he's in his car on his phone, so I just muted his line.

17
00:01:36.240 --> 00:01:37.579
Stacey Druss: Oh, as long as it's not…

18
00:01:37.740 --> 00:01:42.069
Jerry Michalski: No, no worries, John. It's not anybody invading the, the call.

19
00:01:42.980 --> 00:01:51.049
Stacey Druss: The idea of Facebook had come up, you know, as a negative, and I just wanted to point out that

20
00:01:51.240 --> 00:01:57.460
Stacey Druss: My understanding is that when Facebook was developed, it was developed for them.

21
00:01:57.940 --> 00:02:02.739
Stacey Druss: And the idea of developing a platform for yourself.

22
00:02:03.120 --> 00:02:06.130
Stacey Druss: I think is an important piece to that.

23
00:02:07.080 --> 00:02:12.980
Stacey Druss: that aligns with what Jesse was explaining about the way women create.

24
00:02:13.730 --> 00:02:30.160
Stacey Druss: So I just wanted to make that parallel. Like, I know there's so many negatives to Facebook, but I think the negatives came in when the idea of monetizing something came in. So I just wanted to bring it back to creating for community.

25
00:02:30.600 --> 00:02:45.470
Jerry Michalski: That is a lovely thing to bring back into the conversation, I really appreciate that. And I want… and I… my memory of the Facebook birth story is that Zuckerberg created an app to basically find, excuse the language, hot babes on the Harvard campus.

26
00:02:45.720 --> 00:02:52.020
Jerry Michalski: That was the original purpose of Face Smash, I think is what it was called originally.

27
00:02:52.130 --> 00:03:06.160
Jerry Michalski: Because when you get to campuses like that, you often, at least you used to, you would get a Facebook, we also called it the Pig Book, because I went to Penn for a while, and it has photos and names of all the people in your class.

28
00:03:06.260 --> 00:03:16.520
Jerry Michalski: And, you know, you use it to figure out who the heck is that that I'm, you know, seeing down the hall or whatever. Anyway, so yeah, so Stacy, the birth story of Facebook is the opposite.

29
00:03:16.820 --> 00:03:24.280
Jerry Michalski: of the energy that you and Jesse were sort of bringing into the room, which I would love to explore much more.

30
00:03:24.540 --> 00:03:38.230
Stacey Druss: Well, but the point I want to make is, yes, there is that negative there, but there's always two pieces to everything, so while I want to throw out the negative, I don't want to lose the positive piece to it.

31
00:03:38.360 --> 00:03:55.570
Jerry Michalski: which, if I'm gonna reinterpret, correct me if I'm wrong, which is, they built something that was useful for them, that it was, that it was useful for a group of people trying to get something done, which I totally agree with. The best software… the best software pre-inshidification

32
00:03:55.570 --> 00:04:00.250
Jerry Michalski: Is, software that helps people get something done they really, really want to do.

33
00:04:00.500 --> 00:04:12.429
Jerry Michalski: And there's… and there's an open question right now about, and I don't think this… I don't think this will apply to the big software communication platforms, but there's an open question right now about, will we be looking at apps in the App Store anymore?

34
00:04:12.430 --> 00:04:22.559
Jerry Michalski: Or will we just talk to an, you know, AI, to our AI agent, which then generates ad hoc software that we need to get something done?

35
00:04:22.680 --> 00:04:47.630
Jerry Michalski: this kind of obviates persistent software like a community, because if you can't whip up new software every time you need to talk to your buddies, because you need to have been talking to them over time. There's no… I think that the… we'll just whip it up in a moment's notice model doesn't work at all for anything with some history or something like that. But there is this open question that AI is going to get rid of the need to have specialty, small-purpose software

36
00:04:47.630 --> 00:04:50.810
Jerry Michalski: programs, because it'll just go, oh, I can do that.

37
00:04:51.150 --> 00:04:53.180
Jerry Michalski: And, and here's a, here's a new app.

38
00:04:53.510 --> 00:04:54.990
Jerry Michalski: Kill.

39
00:04:56.370 --> 00:04:59.390
Gil Friend • Sustainability OG • CxO Coach: Oh, I can do that, but without the quality control.

40
00:05:00.240 --> 00:05:04.859
Jerry Michalski: that you might find with a professional software shop. I mean, I've been trying to… Oh, sure.

41
00:05:04.860 --> 00:05:08.329
Gil Friend • Sustainability OG • CxO Coach: I've been playing a lot with AI, and it takes a lot of iterations.

42
00:05:08.770 --> 00:05:11.520
Gil Friend • Sustainability OG • CxO Coach: To get it to where I wanted to get to. Right.

43
00:05:11.860 --> 00:05:16.660
Gil Friend • Sustainability OG • CxO Coach: But back to the platforms for them, platforms for us, etc.

44
00:05:17.060 --> 00:05:22.060
Gil Friend • Sustainability OG • CxO Coach: What I'm wondering is, is insidification tightly correlated with monetization?

45
00:05:23.450 --> 00:05:25.410
Jerry Michalski: I… so.

46
00:05:25.680 --> 00:05:41.019
Gil Friend • Sustainability OG • CxO Coach: And the related question is, how do you build something that has the appropriate reach for what's trying to be done, and can be sustained economically in the course of doing that, without falling into the acidification machine?

47
00:05:41.910 --> 00:05:54.600
Jerry Michalski: Yes, and I think that's, one of the… one of the big questions here is, is it possible to have viable, sustainable, ongoing concern software that doesn't need to go down the intridification rat hole?

48
00:05:55.060 --> 00:06:01.240
Gil Friend • Sustainability OG • CxO Coach: And related to that, Does the if-so factor depend on volunteer labor?

49
00:06:01.380 --> 00:06:02.620
Gil Friend • Sustainability OG • CxO Coach: to sustain.

50
00:06:02.720 --> 00:06:19.069
Gil Friend • Sustainability OG • CxO Coach: Which means that the volunteers have to have sources of income from somewhere else, so they're, you know, they're maybe tied into the intrification machine for most of their lives, and do volunteer good work, which is lovely, and it's how we've gotten a lot of the open source, you know, brilliance of the last 50 years, but it's not a sustainable strategy.

51
00:06:21.030 --> 00:06:23.520
Jerry Michalski: Yes.

52
00:06:24.020 --> 00:06:28.200
Jerry Michalski: And, oh, lost my thought.

53
00:06:28.330 --> 00:06:33.509
Jerry Michalski: Other thoughts? I'm sort of checking in for those of you who just joined the call, Kevin Jose.

54
00:06:33.610 --> 00:06:42.089
Jerry Michalski: I'm asking, are there any afterthoughts from last week's call, or about this topic in general? So we're just, kind of setting the stage for our conversation.

55
00:06:43.970 --> 00:06:49.830
Jose Leal: I wasn't here last week, so I'm not sure that I have any… Any thoughts yet?

56
00:06:50.230 --> 00:06:51.470
Jerry Michalski: No worries, thanks.

57
00:06:52.380 --> 00:06:53.769
Kevin Jones: I think John asked…

58
00:06:54.720 --> 00:07:02.029
Kevin Jones: a really interesting set of questions in the email that I'd like to… his questions would be better than the one I come up with.

59
00:07:02.720 --> 00:07:08.619
Jerry Michalski: I agree. John, thank you. You and Thorbjorn sent really thoughtful notes to the retreat list.

60
00:07:10.510 --> 00:07:18.910
Jerry Michalski: And I'm going to actually copy-paste your note into the chat, which I think will blow the chat

61
00:07:19.550 --> 00:07:21.699
Jerry Michalski: Let's see if it blows the chat text limit.

62
00:07:21.950 --> 00:07:24.549
Jerry Michalski: Yes, I have to cut it in half, hold on.

63
00:07:30.420 --> 00:07:40.840
Jerry Michalski: There we go. That, and that, and of course, it didn't do any of the carriage returns. Sorry about that. But John, if you're,

64
00:07:41.090 --> 00:07:47.430
Jerry Michalski: Near your… near your device. Would you like to, talk about what you posted.

65
00:07:48.730 --> 00:07:50.599
Jerry Michalski: And he… oh, good.

66
00:07:50.870 --> 00:07:53.469
John Warinner: Yeah, sorry. I,

67
00:07:55.990 --> 00:08:05.159
John Warinner: I don't know quite what to say that wouldn't just be repeating what I wrote, but I understand not everybody is… that's on the call has seen the…

68
00:08:05.820 --> 00:08:12.590
Jerry Michalski: I just pasted into the chat, although it doesn't look as nice as in your email, but it is available to everybody on this call right now.

69
00:08:12.860 --> 00:08:25.200
John Warinner: I mean, I mean, the main… the main theme of what I was getting at is, for me, it was a bit of an epiphany, and I'm sure that others, you know, this occurred to others, you know, long ago, but…

70
00:08:25.200 --> 00:08:35.999
John Warinner: in more than one setting, not just OGM, I'm having this experience where I personally am doing a lot with ChatGPT, you know, just as my own internal

71
00:08:36.299 --> 00:08:37.569
John Warinner: you know…

72
00:08:37.860 --> 00:08:49.879
John Warinner: feedback device, cogitation device, whatever. But I'm seeing more and more, like, in the OGM list, and… and elsewhere, where people are having, you know, like, these

73
00:08:49.880 --> 00:08:58.290
John Warinner: revealing, you know, experiences by one-on-one explorations with AI,

74
00:08:58.420 --> 00:09:10.240
John Warinner: And understandably, like, we're wanting to share that with others, but we're… so many of us are doing it, like, we don't really have time, you know, to fully digest

75
00:09:10.380 --> 00:09:29.940
John Warinner: you know, like, I'm having my own conversations, I don't know that I have… whether it's a matter of interest, time, whatever, bandwidth, you know, you just kind of, like, it's all we can do to get, you know, get through our own busy days and busy schedules, so it's like, when someone shares, hey, here's the, you know.

76
00:09:30.460 --> 00:09:44.650
John Warinner: 2,000, 4,000 word, you know, exchange that I had, which… which was probably even an excerpt, right? Because it's like the… the words pile up pretty quick, you know, with an exchange back and forth, and a…

77
00:09:44.800 --> 00:09:50.740
John Warinner: you know, dialogue through a one-hour hike or something, like what I'm doing occasionally with ChatGPT, and…

78
00:09:50.900 --> 00:09:54.689
John Warinner: Anyway, so the main point was, how do we share it? And…

79
00:09:54.800 --> 00:10:00.510
John Warinner: The other day was just kind of the first time that this light bulb went on in my head, that it was, like.

80
00:10:00.820 --> 00:10:11.369
John Warinner: Well, I guess you gotta back up and ask, like, why are we all having one-on-ones, with ChatGPT? Like, why aren't we having conversations like this, where…

81
00:10:11.730 --> 00:10:19.170
John Warinner: not just ChatGPT, but you guys get what I mean. Like, where the LLM is just a participant in the conversation.

82
00:10:19.380 --> 00:10:26.129
John Warinner: So that kind of prompted me to ask ChatGPT, like, well, how far off is that? Because I hadn't even really thought about it, you know?

83
00:10:26.270 --> 00:10:34.769
John Warinner: And the response I got was, oh yeah, you know, like, that's in the pipeline, and it's, you know, maybe a couple years away, because there's a lot to it.

84
00:10:35.250 --> 00:10:39.670
John Warinner: But it just really got me thinking about, that's gonna be a different…

85
00:10:39.750 --> 00:10:51.700
John Warinner: chapter of this deal than what… than what we're currently in. And maybe, in a way, it tempers some of the, you know, the internal frustrations, where we're… we're bombarding each other with these…

86
00:10:51.700 --> 00:11:00.010
John Warinner: you know, loquacious, you know, exchanges, because we really want to share it, right? Like, because we really feel like we are having some…

87
00:11:00.230 --> 00:11:05.970
John Warinner: You know, like, intellectual insights and breakthroughs, and so there's a strong desire to share it, but…

88
00:11:06.300 --> 00:11:13.030
John Warinner: the bandwidth, personal, individual bandwidth, just isn't there for each of us to share all this stuff. So, anyway.

89
00:11:13.330 --> 00:11:31.440
Jerry Michalski: John, thank you for that. That helps and amplifies what you said online. And I'm going to make an analogy that may or may not work for you, but… because I use the brain and do mind mapping, but I also talk to people, who do Kumu, system maps and all this and all that, and my experience of other people's maps

90
00:11:31.510 --> 00:11:33.879
Jerry Michalski: And this may ring a bell for people.

91
00:11:33.880 --> 00:11:56.629
Jerry Michalski: my experience of other people's maps is that maps that they think are really profound and explain a lot are often impenetrable to me, or need to be unpacked really slowly, which takes time. The times that it works best, like with Gene Bellinger, are when Gene starts with a little piece of the story, and then he adds on a loop, and then he adds on a piece over here, and then he basically builds it out.

92
00:11:56.890 --> 00:12:07.779
Jerry Michalski: which takes the effort on both of our sides. He has to… he has to go back. Once he's elaborated a large map, he has to go back and figure out what is… how do I then unpack this for people?

93
00:12:07.780 --> 00:12:20.999
Jerry Michalski: And I think what's happening in LLMs is that these LLMs are very capable of taking us down really fruitful and interesting paths. A couple years ago, early in this Gen AI era.

94
00:12:21.000 --> 00:12:30.260
Jerry Michalski: I went to pi.ai, it said, what's on your mind? I answered that, and I proceeded in 90 minutes to have the best interview I've ever had with anybody.

95
00:12:30.300 --> 00:12:39.689
John Warinner: And I think I've mentioned this in an OGM call before, but what it kept doing was, it would say, oh, it sounds like you said this, and it would raise the conversation, do you mean this?

96
00:12:39.690 --> 00:12:59.009
Jerry Michalski: And I'd be like, yeah, yeah, yeah, that. And then it would do it again. It was not just doing Eliza back to me, it wasn't a stochastic parrot, it was somehow deeply understanding what I was up to, and I was like, oh my god, oh my god! And I, you know, I haven't actually gone back to that thread. I think it sort of shook me a little bit, because I'm trying to figure out how to incorporate

97
00:12:59.010 --> 00:13:06.599
Jerry Michalski: this new set of AIs into my life more, which is a whole different conversation. But I say all that because

98
00:13:07.170 --> 00:13:21.709
Jerry Michalski: many of us are having these exciting conversations, and then we share them in to our media, which tends to be, at this point, the OGM list, and then it's a lot of words, it's a couple thousand words, and it's in an form that's

99
00:13:21.710 --> 00:13:36.180
Jerry Michalski: Relatively easy to digest, because bullets and bold help, and the separations help. These devices are smart enough to format nicely, but they're sort of not as exciting as they are to the person who had the conversation.

100
00:13:36.180 --> 00:13:37.470
Jerry Michalski: But, but worse.

101
00:13:37.470 --> 00:14:01.870
Jerry Michalski: they impose a long read on whoever is supposed to be, sort of, you know, doing that. And maybe… I'm going to propose a little solution along the way here. Maybe one way to do that is a lot of these LLMs give you a URL for the conversation, and instead of pasting the conversation into the email and sharing the whole thing, just to paste the link to the conversation, and anybody who's up for the long read can go

102
00:14:01.870 --> 00:14:07.040
Jerry Michalski: do that, and other people can then skip. But I don't know. I think part of our conversation here is.

103
00:14:07.040 --> 00:14:13.749
Jerry Michalski: What are the protocols that will make us less uncomfortable or unhappy with how we're going about

104
00:14:13.750 --> 00:14:29.550
Jerry Michalski: having these conversations, but I like the other part of the conversation, John, which you also opened up, which is, at some point, these agents will be part of our conversation. We will want them to be engaged with this group as Gil's Fathom note-taker is right now, which I sort of resent.

105
00:14:29.550 --> 00:14:30.850
Jerry Michalski: Because…

106
00:14:30.850 --> 00:14:46.220
Jerry Michalski: Because when Gil shows up, he shows up with, like, 3 different note-takers now. I'm like, no, no, don't want them. But if one of these were as good an interviewer or processor of group dialogue as my experience a couple years ago with Pi.ai.

107
00:14:46.430 --> 00:14:52.069
Jerry Michalski: I'm in. That could be really fruitful for where we're heading. And I'll go to John first, and then I'll go back to the queue.

108
00:14:52.070 --> 00:15:10.589
John Warinner: Yeah, so just a quick response to that, Jerry. It's funny that you mentioned Gene Bellinger, because he was the one that told me, this is several years ago, when I was getting into system thinking, systems mapping, Kumu, all that stuff, and he said, John, just don't make the mistake, he said.

109
00:15:10.720 --> 00:15:16.740
John Warinner: the value of this is the… is the gerund, right? Like, it's the mapping, it's not the map.

110
00:15:16.870 --> 00:15:25.970
John Warinner: And, like, just exact… he forewarned me of what you said, and I had had that experience myself. You do all this work, you bring people the map.

111
00:15:26.110 --> 00:15:28.540
John Warinner: They can't make any sense of your map.

112
00:15:28.660 --> 00:15:43.510
John Warinner: But it's… it's… but if you can lead a group through a mapping exercise, they get value from the mapping, not the map. And… and… and I would think that the same… totally what you said, the same…

113
00:15:43.600 --> 00:15:59.599
John Warinner: operative is at work when you're… when you're mapping, or thinking, or conversing with an LLM? I think the greatest value is in the conversation, not in the product of the conversation. I'll pass on to others.

114
00:15:59.600 --> 00:16:04.519
Jerry Michalski: Thank you, thank you, thank you, thank you. Gil, is your hand up from before, or…

115
00:16:05.170 --> 00:16:06.450
Jerry Michalski: Are you back in the queue?

116
00:16:06.450 --> 00:16:07.949
Gil Friend • Sustainability OG • CxO Coach: No, it's from before, sorry.

117
00:16:07.950 --> 00:16:09.700
Jerry Michalski: Okay, good. Alex?

118
00:16:12.600 --> 00:16:14.110
Jerry Michalski: You are muted, however.

119
00:16:14.930 --> 00:16:17.550
Jerry Michalski: We will only hear your wisdom if you unmute.

120
00:16:17.960 --> 00:16:18.800
Jerry Michalski: Thank you.

121
00:16:19.150 --> 00:16:22.719
Alex Kladitis: Sorry about that. Thanks, John, and

122
00:16:23.030 --> 00:16:30.330
Alex Kladitis: I've got something to say about that, but let me go back first to what you said about what we discussed last week, and

123
00:16:30.880 --> 00:16:35.630
Alex Kladitis: what we started off this week, and we've gone into one of the topics. And it's as follows.

124
00:16:35.790 --> 00:16:43.029
Alex Kladitis: Last week, I had the impression that we… it's like having multiple balloons up in the air at the same time.

125
00:16:43.210 --> 00:16:49.389
Alex Kladitis: And you could've kept… You know, batting away whatever comes down that balloons with the topics.

126
00:16:49.800 --> 00:16:51.780
Alex Kladitis: And I got the feeling that

127
00:16:52.620 --> 00:16:55.179
Alex Kladitis: We didn't really know what we wanted

128
00:16:55.320 --> 00:17:13.650
Alex Kladitis: to discuss, or we knew what we wanted to discuss, but there's so many aspects to it that we couldn't cohere to anything, come together to anything. So, that's what I was going to say before John spoke, and now I'll go John spoke. So, one of the things is, what are we here for? I'm here, maybe we should all say what we're here for.

129
00:17:13.700 --> 00:17:21.540
Alex Kladitis: I'm here because I hear New perspectives, or existing perspectives from different people, with a different viewpoint.

130
00:17:21.720 --> 00:17:30.679
Alex Kladitis: And it makes me think, okay? So that's why I'm here. But it strikes me that other people are here for other reasons. So if we're going to have a tool.

131
00:17:30.990 --> 00:17:38.049
Alex Kladitis: what is the tool that will address? And I'll just go back to… tied up to John's,

132
00:17:38.670 --> 00:17:40.930
Alex Kladitis: what John said, and what you said, Gerry.

133
00:17:41.750 --> 00:17:45.489
Alex Kladitis: If we take the conversation that someone pastes

134
00:17:46.320 --> 00:17:49.370
Alex Kladitis: A lot of text onto, email.

135
00:17:50.100 --> 00:17:54.369
Alex Kladitis: From the start, if I go into email to look at OGM,

136
00:17:55.010 --> 00:17:59.559
Alex Kladitis: or if I go into email, I'm looking for snappy, short things to act on.

137
00:18:00.290 --> 00:18:05.349
Alex Kladitis: If someone says, read this 20-minute thing, That's my view, okay?

138
00:18:05.670 --> 00:18:10.940
Alex Kladitis: But if someone says, read this 20-minute thing, I've got to say, right.

139
00:18:11.490 --> 00:18:18.479
Alex Kladitis: Okay, I've got all these things to do for my day, let me park it for later, let me do… there's all sorts of… it's a different mechanic.

140
00:18:18.760 --> 00:18:26.839
Alex Kladitis: So, so, the other thing about it, if the LLM is participating, I doubt if we'd let it go on for 20 minutes.

141
00:18:27.440 --> 00:18:38.219
Alex Kladitis: on a topic, because it would cut down what it says. One of the value of what we're doing is we're limited to 2-3 minutes. It's a self-limitation, I know, but, it could be endorsed.

142
00:18:38.830 --> 00:18:41.870
Alex Kladitis: But when we talk about LLMs, we're…

143
00:18:42.080 --> 00:18:44.870
Alex Kladitis: It's a different type of thinking and talking.

144
00:18:45.140 --> 00:18:45.970
Alex Kladitis: So…

145
00:18:46.890 --> 00:18:53.460
Alex Kladitis: come back to this. What is… what do people want out of this group, or this conversation? Not necessarily the group, the conversation?

146
00:18:53.640 --> 00:18:57.030
Alex Kladitis: And I think that's important. I'll tell you what I want, but, you know.

147
00:19:00.980 --> 00:19:04.750
Jerry Michalski: Thanks, Alex. That's really useful, and you're…

148
00:19:05.490 --> 00:19:19.770
Jerry Michalski: I did this when you were talking about you want emails to be short and crisp, because I run into way too many long reads that are in emails, not because that's a great place for them, but because there's somebody who just posted something super interesting, and I know I need to read it.

149
00:19:20.040 --> 00:19:21.599
Jerry Michalski: I'm remembering that my…

150
00:19:21.800 --> 00:19:22.580
Alex Kladitis: But…

151
00:19:22.610 --> 00:19:24.930
Jerry Michalski: Sorry, but it makes you feel…

152
00:19:25.080 --> 00:19:27.429
Alex Kladitis: Different, because suddenly it's a different

153
00:19:28.190 --> 00:19:30.450
Alex Kladitis: Sorry, the email is not the problem.

154
00:19:30.660 --> 00:19:33.719
Alex Kladitis: It's what you have allocated yourself time for.

155
00:19:33.930 --> 00:19:39.549
Alex Kladitis: So when I do OGMs, I have to say, I'm not doing email anymore, I am…

156
00:19:39.900 --> 00:19:43.579
Alex Kladitis: in deep study mode, or… and I'm allocating an hour of my time.

157
00:19:44.330 --> 00:19:50.579
Jerry Michalski: Which is our self-control, which is our ability to manage our time and our attention and all that kind of stuff as well.

158
00:19:50.810 --> 00:20:04.799
Jerry Michalski: This is a side dish, but it's related. One of my pet peeves for private mailing lists over time and private forums has been that I've seen people post brilliant things that are not in public.

159
00:20:04.800 --> 00:20:12.629
Jerry Michalski: So I can't point to them and, you know, retweet them, repost them, do whatever. I feel sad that retweet is now, like, a dead verb.

160
00:20:12.710 --> 00:20:14.070
Jerry Michalski: But…

161
00:20:15.130 --> 00:20:30.969
Jerry Michalski: A policy I tried to talk about years ago was post outside in, meaning if you have an interesting idea, go write a post about it on whatever your favorite, you know, if it's LiveJournal or Blogger, whatever, go write a post about it, then give us the link here.

162
00:20:31.340 --> 00:20:49.609
Jerry Michalski: And then we can sort of have the conversation in the more public view, in a different forum, perhaps. That would, you know, I thought that was interesting, because I think there's a lot of really good stuff just trapped in private, you know, private online spaces that are now in the bit bucket. Nobody goes back and sees them, communities move, whatever else happens.

163
00:20:49.710 --> 00:20:53.560
Jerry Michalski: I'm also reminded that my first experience online was on the well.

164
00:20:53.680 --> 00:20:55.579
Jerry Michalski: My first, my first sort of good set of…

165
00:20:58.780 --> 00:21:05.459
Jerry Michalski: And the well, was out of Sausalito. It was a really interesting thing that ran into a bulletin board system called PicoSpan.

166
00:21:05.620 --> 00:21:24.239
Jerry Michalski: And one of the problems with forums like The Well, and we had a… Pete set up a forum for us here on Discourse, one of my problems with those formats is I can't read through a long list of posts and remember that there was a really good one here and a really good one here, with a whole stretch of crap in between, and then…

167
00:21:24.460 --> 00:21:42.390
Jerry Michalski: I knew that there was really good stuff floating by, but I could never keep myself up-to-date in the conversation on those kinds of media, and that just may be a permanent problem. But I think that it's one of the many facets of what we're trying to solve for here, Alex, and you're right to point out that we've got 12 different questions, Dwayne?

168
00:21:43.020 --> 00:21:46.799
Alex Kladitis: I think the right tool isn't there, and, you know, we should get the right

169
00:21:47.180 --> 00:21:48.670
Alex Kladitis: But what we've got now isn't.

170
00:21:48.780 --> 00:21:54.249
Alex Kladitis: However, I would like to ask you, Jerry, why are you in this group? I know you're organizing and owning it, but…

171
00:21:54.830 --> 00:21:56.060
Alex Kladitis: What's in it for you?

172
00:21:57.360 --> 00:22:00.709
Jerry Michalski: First of all,

173
00:22:00.790 --> 00:22:20.300
Jerry Michalski: I learn a lot of stuff I would never figure out, because everybody shows up here and shares in stuff they're finding, stuff they're thinking, and I leave many, not all, but many of our conversations going, oh, I hadn't put that plus that together in my head, and it's really interesting and useful. And I'm busy curating that into my brain, so for me.

174
00:22:20.320 --> 00:22:37.930
Jerry Michalski: I get a long-term benefit because the curation over time gets better. Things… knowledge accumulates or accrues, and that really works well for me. Also, I love who shows up, and I just love the social interactions, I love getting to know more about you, I love…

175
00:22:38.030 --> 00:22:46.860
Jerry Michalski: I love that we show up and we're not like, oh, what do you do? Right? We're like, oh, we know what we do, we've been sitting here talking for, like, 4 or 5 years.

176
00:22:46.870 --> 00:22:57.139
Jerry Michalski: We're busy onto a whole set of things, and we've got history and all of that, so I love that. And I love also, maybe a third thing real quick, and then I'll go to Sean.

177
00:22:57.140 --> 00:23:15.780
Jerry Michalski: I love that bringing people together in places like this allows people to get to know each other, and then they go off and have conversations and do projects and do things, and that is a huge payoff for me. I'm like, if three people met someplace that I helped convene, and they did something they loved, I'm… that's…

178
00:23:15.780 --> 00:23:17.179
Jerry Michalski: That's gold for me.

179
00:23:18.750 --> 00:23:40.970
Jerry Michalski: So, that's top of mind, that's kind of why I'm here. There's probably a couple other reasons that I'm here that have been frustrated. Like, I would really like to build a global mind. I really… the big fungus that I've talked about a little bit on the calls here is my funny sort of analogy for what that could look like. We really haven't done that. We haven't done much of that at all.

180
00:23:41.000 --> 00:23:48.290
Jerry Michalski: And I'm wondering where those things might go as well as part of this process. And I'll also add.

181
00:23:48.290 --> 00:24:03.470
Jerry Michalski: It's very possible that LLMs are really nice companions for long, threaded discussions, because they might be excellent summarizers or highlighters of what actually happened in these super too-long-to-read threads. The TLDR agent

182
00:24:03.470 --> 00:24:11.720
Jerry Michalski: might be a real boon here as we walk into deeper conversations, and I think deeper conversations are very important to have.

183
00:24:11.790 --> 00:24:13.950
Jerry Michalski: Whew! Sean, thank you for your patience.

184
00:24:16.770 --> 00:24:27.530
Shawn Murphy: I'm, typing feverishly, because what I'm deploying is the very latest, latest version of, Thinkertoys, and on my own development machine.

185
00:24:27.920 --> 00:24:36.020
Shawn Murphy: It's working, and I'm having a hard time getting it deployed. If I could only deploy it, then I would be inviting you

186
00:24:36.230 --> 00:24:42.430
Shawn Murphy: Individually by email to come and start Knowledge?

187
00:24:43.290 --> 00:24:47.290
Shawn Murphy: Collaborating at the knowledge level in real time.

188
00:24:47.940 --> 00:24:49.870
Shawn Murphy: And so, I, I…

189
00:24:50.140 --> 00:25:08.870
Shawn Murphy: I… sorry, I keep saying this, but I think I'm building the tool that we're talking about. And I think this morning, I'm deploying it, if only the blessed thing… I've got… I think I've got one little configuration line that's problematic. But, what it is, is,

190
00:25:09.680 --> 00:25:22.300
Shawn Murphy: I was working it with a different crew last Saturday, and what we were doing was we were working a cycle where we were doing, editing a prompt together for an LLM,

191
00:25:22.400 --> 00:25:37.529
Shawn Murphy: then we were running the prompt through the LLM, and then it was generating… that prompt was asking for output in Turtle, which is a knowledge format, a graph knowledge representation technology.

192
00:25:37.530 --> 00:25:45.500
Shawn Murphy: And we were pumping that into… into Thinkertoys, and then immediately graphing

193
00:25:45.540 --> 00:26:04.329
Shawn Murphy: the, the output of that. And the graphing was just one of multiple visualization technologies, some tabular, some graphical, some wiggly graph things, and we were going through the cycle of basically exploring a topic

194
00:26:04.780 --> 00:26:05.700
Shawn Murphy: Ian…

195
00:26:05.840 --> 00:26:20.109
Shawn Murphy: almost real time, because it wasn't all fully wired together last weekend, but by Saturday it will be, in near real time, where it was a group of us collaborating with AI,

196
00:26:20.220 --> 00:26:26.509
Shawn Murphy: and then the AI was populating one of many shared

197
00:26:26.880 --> 00:26:34.529
Shawn Murphy: knowledge collaboration workspaces that Thinkertoys provides, so that

198
00:26:34.710 --> 00:26:41.349
Shawn Murphy: In effect, we were collaborating together. It was producing output that we were able to

199
00:26:41.760 --> 00:26:45.560
Shawn Murphy: Springboard off of and continue to work on together.

200
00:26:46.160 --> 00:26:52.559
Shawn Murphy: Okay, so there's too much to say about what its scope is, but I…

201
00:26:52.660 --> 00:26:56.030
Shawn Murphy: I think I'm making the tool

202
00:26:56.160 --> 00:26:58.940
Shawn Murphy: that we're talking about, and I invite

203
00:26:59.740 --> 00:27:06.600
Shawn Murphy: invite your participation, because I… again, there's more than fits in this margin.

204
00:27:07.330 --> 00:27:19.819
Jerry Michalski: So, Sean, I would suggest that if you were to send an invite to the OGM list asking for people to show up for a pop-up call to try your tool, you would have a bunch of enthusiastic people jumping in.

205
00:27:20.480 --> 00:27:22.230
Shawn Murphy: Okay, lovely, thank you so much.

206
00:27:22.230 --> 00:27:26.760
Jerry Michalski: we can go from there and see what's up. So, love that.

207
00:27:27.390 --> 00:27:28.499
Jerry Michalski: Thank you.

208
00:27:28.970 --> 00:27:30.090
Jerry Michalski: Stacy, please.

209
00:27:32.300 --> 00:27:44.690
Stacey Druss: Yeah, Jerry, you just kind of hit on it. I was going to highlight what John said. Recently there was a long AI chat about,

210
00:27:45.170 --> 00:27:52.929
Stacey Druss: I think it was about the shutdown, and what I did is I ran it through the LLM to get the podcast.

211
00:27:53.480 --> 00:28:00.929
Stacey Druss: And what it did is it created a very short podcast that was a great beginning point for a conversation.

212
00:28:01.400 --> 00:28:14.840
Stacey Druss: And so that's what I do that takes that long text and gets me a great starting place for a conversation. So I just wanted to say that sort of what you're describing.

213
00:28:15.220 --> 00:28:18.570
Stacey Druss: That's it. Thank you.

214
00:28:18.570 --> 00:28:37.680
Jerry Michalski: Before I turned on the recorder, I was commenting that I keep forgetting the name of an app that I got to use for an online focus group-y kind of thing. It wasn't really a focus group. It was about 60 or 70 people anonymized, so I was Orange Panda or something like that. You got a color on an animal.

215
00:28:37.890 --> 00:28:52.349
Jerry Michalski: I'm all anonymized, so at the end of an hour and a half or two-hour session, I did not know anybody in the room, which was the major drawback of the whole thing for me, because the thing really worked well. And there was a topic for the whole discussion.

216
00:28:52.350 --> 00:28:58.339
Jerry Michalski: We had been given questions ahead of time, and then what happened was we were broken up and we were sent into breakouts to talk about

217
00:28:58.370 --> 00:29:05.450
Jerry Michalski: Each of the questions we had been given for very brief periods of time, and we were busy sort of typing in our thoughts and answers.

218
00:29:05.570 --> 00:29:22.940
Jerry Michalski: there was AI in the system that was summarizing or picking out things from each of the groups and cross-fertilizing them, and then at the end of each of your sessions, the AI would summarize and say, I think this is what you said, yes, or I think this is what you've agreed on, yes, and it was quite good at that.

219
00:29:23.150 --> 00:29:39.510
Jerry Michalski: So, the AI was being really, really helpful in ways that I hadn't experienced before in that kind of a setting. So I will… I will discover, rediscover what the name of the app is, but I'm… I'm looking forward to, you know, our…

220
00:29:40.010 --> 00:29:44.160
Jerry Michalski: I, for one, look forward to our new robot overlords. No, no, no, I don't mean that.

221
00:29:44.330 --> 00:29:56.849
Jerry Michalski: But I look forward to having AIs that are participating in this conversation as excellent listeners and summarizers, or something else, I don't know, but I think that that conversation is fun to have.

222
00:29:57.030 --> 00:29:59.210
Jerry Michalski: And maybe just a wee bit frightening.

223
00:29:59.440 --> 00:30:05.630
Stacey Druss: If I could just say, what I like… what I liked that the podcast did is it asked good questions.

224
00:30:06.340 --> 00:30:08.360
Stacey Druss: That's what I really loved.

225
00:30:08.740 --> 00:30:09.820
Jerry Michalski: Yep, yep.

226
00:30:09.990 --> 00:30:11.380
Jerry Michalski: Super. Done?

227
00:30:13.540 --> 00:30:23.770
Doug Breitbart: Yeah, I, you know, looping back to, the why.

228
00:30:23.960 --> 00:30:27.969
Doug Breitbart: in all of this. And…

229
00:30:30.240 --> 00:30:41.730
Doug Breitbart: You know, the… the… the central… purpose… for me, is… Understanding.

230
00:30:43.010 --> 00:30:54.240
Doug Breitbart: And… not mine, but… Somebody else's understanding of mine, and my understanding of theirs, and us

231
00:30:54.350 --> 00:30:57.819
Doug Breitbart: Sort of having a way to get on the same page.

232
00:30:59.860 --> 00:31:04.740
Doug Breitbart: On the most simple and basic and fundamental semantic level.

233
00:31:05.080 --> 00:31:08.759
Doug Breitbart: That my meaning and somebody else's meaning

234
00:31:08.870 --> 00:31:13.980
Doug Breitbart: Is the same. Is aligned. Is recognized and understood.

235
00:31:14.560 --> 00:31:17.740
Doug Breitbart: Not to be confused with, we have to agree with each other.

236
00:31:19.200 --> 00:31:26.649
Doug Breitbart: I'm not even going that far. Just getting to a commonality of understanding to the same mean at the center.

237
00:31:28.090 --> 00:31:30.320
Doug Breitbart: As a… as a… as ground zero.

238
00:31:31.540 --> 00:31:33.909
Doug Breitbart: And, you know, I…

239
00:31:34.740 --> 00:31:54.610
Doug Breitbart: I'm working on something that builds on top of Gene and a half a dozen other people's pieces of the elephant, and, you know, in a conversation with Gene, what I said to him was that thing about, it's not the map, it's the mapping, the exercise of the mapping, and I would argue it isn't just the exercise of the mapping.

240
00:31:55.550 --> 00:32:01.660
Doug Breitbart: When he does one of his maps, he's creating a model. He's reflecting a living system.

241
00:32:02.450 --> 00:32:08.959
Doug Breitbart: And… So the map reveals a complex system.

242
00:32:09.460 --> 00:32:15.449
Doug Breitbart: And based on the revelation and relationships living in that system.

243
00:32:16.260 --> 00:32:24.980
Doug Breitbart: It extrapolates to a model that, from which one can identify leverage points, like…

244
00:32:25.240 --> 00:32:36.559
Doug Breitbart: All in service to informing what's next action, or where am I focusing attention from an interventional standpoint, or from a contributive standpoint.

245
00:32:36.860 --> 00:32:39.339
Doug Breitbart: And the eye in that is actually…

246
00:32:39.710 --> 00:32:45.599
Doug Breitbart: For groups of people, or organizations, and scaling up from there.

247
00:32:45.970 --> 00:32:58.300
Doug Breitbart: So, it's how can, you know, We have faster capacity… to… martial…

248
00:32:58.740 --> 00:33:09.140
Doug Breitbart: Much more complex and diverse an array of variables and inputs in nonlinear complex system context.

249
00:33:09.600 --> 00:33:16.399
Doug Breitbart: To factor into helping us make better decisions, like, make better choices.

250
00:33:17.220 --> 00:33:21.710
Doug Breitbart: Take better actions and… and better from the standpoint of

251
00:33:21.980 --> 00:33:32.539
Doug Breitbart: Contributing positively to value and quality of life, and minimizing impact and destruction and adverse consequence.

252
00:33:32.720 --> 00:33:36.099
Doug Breitbart: In other, in, in, in other dimensions.

253
00:33:36.230 --> 00:33:38.660
Doug Breitbart: And, like, that's sort of the point.

254
00:33:38.990 --> 00:33:47.520
Doug Breitbart: For me. And the instrumentalities and the steps and the artifacts, not so much.

255
00:33:48.330 --> 00:33:56.279
Doug Breitbart: Like, the artifact's only relevant If it's popped up at the right time, in the right context, you know.

256
00:33:56.460 --> 00:34:05.130
Doug Breitbart: chirotically, to be in service to that moment's discernment. Like, What's next?

257
00:34:05.750 --> 00:34:07.650
Doug Breitbart: And what do we do?

258
00:34:07.900 --> 00:34:14.269
Doug Breitbart: As a… as a, you know, product of a group of individuals actually getting to common understanding.

259
00:34:14.429 --> 00:34:24.129
Doug Breitbart: So, I'm really, like, wallowing around in those really, really core fundamental, like, getting to understanding.

260
00:34:26.420 --> 00:34:35.760
Doug Breitbart: factoring for much more complex array of inputs and all the rest. And I think… Lm's…

261
00:34:36.330 --> 00:34:43.760
Doug Breitbart: are an enabling Technology affordance to enable us to do… handle that.

262
00:34:44.600 --> 00:34:48.940
Doug Breitbart: At a much more complex, dynamic, real-time level.

263
00:34:49.719 --> 00:34:50.820
Doug Breitbart: than ever.

264
00:34:51.090 --> 00:34:52.729
Doug Breitbart: Than we ever could.

265
00:34:54.230 --> 00:34:55.460
Doug Breitbart: Before.

266
00:34:56.550 --> 00:35:00.109
Doug Breitbart: But at the end of the day, it's helping with decision-making.

267
00:35:00.780 --> 00:35:02.029
Doug Breitbart: And I'm complete.

268
00:35:02.810 --> 00:35:13.660
Jerry Michalski: Doug, thank you, and everything you just said rings for me. I think we're on the same mission, in the sense of helping people make better decisions, how do we sort of achieve mutual understanding, despite

269
00:35:14.320 --> 00:35:17.570
Jerry Michalski: Stumbling blocks like language and technology? Huh.

270
00:35:17.960 --> 00:35:24.409
Jerry Michalski: All of that. So, so I think, well, this explains partly why we're both in the same Zoom at the same time!

271
00:35:24.810 --> 00:35:26.510
Jerry Michalski: How about that? Damn.

272
00:35:27.450 --> 00:35:46.419
Jerry Michalski: And a small aside, I just put in the chat, how are maps useful to others? I said earlier that really complicated maps are sometimes impenetrable to others, but it turns out that there's a small percentage of people who every now and then ping me and say, hey, Jerry, thanks for making your brain publicly available. They have figured out how to navigate my brain without me.

273
00:35:46.610 --> 00:36:11.270
Jerry Michalski: There's another… in the great distribution of people, there's a few people out at the edge. Then there's another tier where if I'm guiding and I'm answering and I'm talking around it, they're like, this is great, let's go, but they can't sort of solo. But there's a goodly set of people who can solo in my brain, and I feel like they're sort of getting it. There's a couple people who seem to write me back and deeply understand

274
00:36:11.270 --> 00:36:30.049
Jerry Michalski: why I created a particular context for a particular scene or setting, you know, or snapshot in the brain, and that thrills me, because one of the things I'm doing when I curate my brain, I'm curating for two audiences. One is obviously me, but another one is any random person coming along, I want there to be

275
00:36:30.050 --> 00:36:31.559
Jerry Michalski: View enough words.

276
00:36:31.560 --> 00:36:44.750
Jerry Michalski: that it's not overwhelming to the eye or the brain when they see a screen, because one of the things I love about the brain is that it lets you take a snapshot… a visual snapshot of a lot of stuff very, very quickly and concisely.

277
00:36:44.750 --> 00:36:57.839
Jerry Michalski: in ways that other displays don't do well, or at least that I've seen. And that is also partly probably a function of how my particular brain works, etc, etc. There's a bunch of other conversations to have there. But let's go to Klaus.

278
00:36:59.850 --> 00:37:06.220
Klaus Mager: Yeah, well, interesting. And I… Yeah, looking, looking back at,

279
00:37:06.300 --> 00:37:22.779
Klaus Mager: maybe the last two years or so, on how our email conversations have changed, and our attitude towards someone posting an AI message has evolved and changed in some way. Sorry.

280
00:37:23.230 --> 00:37:30.669
Klaus Mager: In some way, really reflects… really reflects our… our own journey with this technology.

281
00:37:30.810 --> 00:37:41.360
Klaus Mager: And the, the… I mean, in my case, I've been using this strictly for…

282
00:37:41.360 --> 00:38:00.700
Klaus Mager: my own personal evolution in understanding my profession better, you know, getting deeper into a regenerative transition and everything it evolves and goes around it, including political considerations, farm bill considerations, and all of that, right?

283
00:38:00.860 --> 00:38:14.609
Klaus Mager: So, I think the critical part, and this is where I sometimes feel sort of slightly offended here, is I don't post anything that I couldn't restate in my own words.

284
00:38:15.070 --> 00:38:32.000
Klaus Mager: Meaning, I don't post something generated by an AI, which I deeply don't understand, right? I mean, the assumption is if you say, I want to listen to you, not to that, then you're assuming that I don't understand what I'm posting here, right? And so that's sort of a little bit off.

285
00:38:32.220 --> 00:38:34.790
Klaus Mager: Because…

286
00:38:35.360 --> 00:38:53.850
Klaus Mager: the power of this AI is that it presents you back with a range of options, what, where do you want to go next with this? And then you have, like, 5 different ways where you could go and you pick the one that you think

287
00:38:53.880 --> 00:38:58.380
Klaus Mager: That, that aligns with where you are thinking, with your…

288
00:38:58.490 --> 00:39:10.690
Klaus Mager: with your beliefs, your understanding, and so on. And then you spin the conversation further, and then you end up, you know, with a, with, with a, with a body of work that

289
00:39:10.690 --> 00:39:22.099
Klaus Mager: And just like, Jerry, you know, the example that you cited before, it just pushes you way further than you would have been able to get on your own, you know?

290
00:39:22.110 --> 00:39:25.969
Klaus Mager: And so… And then the other thing is…

291
00:39:26.040 --> 00:39:45.549
Klaus Mager: I mean, we have to recognize this thing is here to stay, right? And it's only getting more invasive and better. Everybody's using it. My son, you know, is working for Lucid, his head of target group of talent branding. He's using Lucid.

292
00:39:45.550 --> 00:39:55.619
Klaus Mager: And he gets, like, complimented how well he has written something, you know? And he goes, I wouldn't dare tell these guys, you know, how I got to this point, but…

293
00:39:55.620 --> 00:40:06.929
Klaus Mager: You know, he's using the AI to guide him in his job, you know, and a lot of people are doing this right now, and it gives them a competitive advantage.

294
00:40:07.000 --> 00:40:25.660
Klaus Mager: So, it doesn't matter whether you are, you know, into philosophy, or, you know, Buddhism, or whatever it is, you can reach a lot deeper into it. And then when I… then this whole posting, I mean, we have… my daughter and I are, you know, building this

295
00:40:25.700 --> 00:40:30.760
Klaus Mager: this company, now, Food We Thought,

296
00:40:30.830 --> 00:40:36.490
Klaus Mager: We have come to the point where we are really pushing down on talking about AI.

297
00:40:36.670 --> 00:40:56.450
Klaus Mager: You know, because it's confusing the issue. It's confusing people, everybody has a perception. Oftentimes, the first thing you hear back, oh, it's using too much electricity, or, you know, you get some stuff back where people just, you know, have no idea what this all is, but they have heard a lot of bad things. So don't talk about it.

298
00:40:56.450 --> 00:40:59.539
Klaus Mager: You know, you just use it, and, and

299
00:40:59.540 --> 00:41:09.080
Klaus Mager: And structure your communication, and then at some point in time, you bring it up, and introduce it, and make it fun, you know.

300
00:41:10.300 --> 00:41:21.489
Klaus Mager: So, so I think, one idea that was already mentioned, you know, when we are talking in email format, I think the idea of you providing, like, a brief summary

301
00:41:21.490 --> 00:41:38.780
Klaus Mager: that really captures your opinion on this thing, and have the AI help you do… crystallize that, but then put in the link to the conversation. So anybody wants to know, how did you query this, you know, how did you prompt this thing? You know, then you can… you can read for it yourself, but…

302
00:41:38.780 --> 00:41:50.390
Klaus Mager: focus your conversations in as long as an email exchange should be and no more kind of thing. I think that's sort of where I think it may be best to go.

303
00:41:50.790 --> 00:41:59.650
Jerry Michalski: Yeah, Klaus, thank you for that. I appreciate that a lot, and as you were talking, I was thinking one of the things that seems to have happened, and this is just…

304
00:42:00.170 --> 00:42:09.910
Jerry Michalski: I noticed this in myself, is that maybe we're depreciating AI-written passages in emails? Maybe, maybe,

305
00:42:10.010 --> 00:42:22.579
Jerry Michalski: maybe when we hit a stretch that we know was written by an AI, somehow we're like, I want the human voice. Even though, even though it might contain lots of wisdom, and it might be something that, as you just said, Klaus.

306
00:42:22.580 --> 00:42:31.639
Jerry Michalski: the person sending it completely agrees with and wishes they had written this nicely and crisply, and there it is, like, you know, and I think…

307
00:42:31.760 --> 00:42:34.290
Jerry Michalski: One of the factors here is

308
00:42:34.290 --> 00:42:56.379
Jerry Michalski: we are overwhelmed by things to read. We have so many long reads and so many things that are attracting our attention. I… one of my… one of the clouds in my life is the too many tabs I have open of things that are really interesting. They're really good. And when I get to them and make my way through them, I learn a lot, and I feel like I'm on a…

309
00:42:56.380 --> 00:43:02.659
Jerry Michalski: lifelong PhD program, you know, except with no advisors, except you guys are my advisors, that's it.

310
00:43:02.660 --> 00:43:04.819
Klaus Mager: Now, this is… this is my advisory team.

311
00:43:05.310 --> 00:43:06.980
Jerry Michalski: And…

312
00:43:07.220 --> 00:43:14.880
Jerry Michalski: So we're in the middle of this moment where we suddenly have the flood valves of words opened up.

313
00:43:14.920 --> 00:43:33.640
Jerry Michalski: Some of which are good, some of which are AI slop, and we need to help each other find our way through it in a way that doesn't overwhelm us or destroy the humanity of our conversations, which I think is one of the real reasons we're also all here, is that we like humans. I mean, I don't think we'd be here if we were not liking humans so much.

314
00:43:34.380 --> 00:43:41.910
Jerry Michalski: So anyway, Kevin, you've got a bruise on your head that indicates you may have had an impact with something.

315
00:43:41.910 --> 00:43:47.180
Kevin Jones: I did. I got into a fight with another LLM gang, and we.

316
00:43:47.180 --> 00:43:48.140
Jerry Michalski: Oh, man.

317
00:43:48.140 --> 00:43:50.009
Kevin Jones: I'm gonna say my prompts are better than yours.

318
00:43:50.010 --> 00:43:52.380
Jerry Michalski: Was it chains and knives, or was it worse?

319
00:43:52.380 --> 00:43:57.479
Kevin Jones: No, no, it was large language modules used as clubs. You've seen it happen a lot.

320
00:43:57.740 --> 00:43:58.800
Jerry Michalski: Literally, it's awful.

321
00:43:58.800 --> 00:44:05.189
Kevin Jones: But, you know, going back to an experience that you talked about, about this anonymized sense-making.

322
00:44:05.320 --> 00:44:24.340
Kevin Jones: I was in one of those several years ago, and I had a staffer who was 25 years younger than me there. She was the only female, and it was anonymized, and she said it was a bad hair day. And her opinions were at the top of everybody's rank.

323
00:44:24.400 --> 00:44:40.019
Kevin Jones: And there were lots of, you know, high-status, visual-status guys there who didn't know who they were voting for. And she said, I'm the only female, I'm 25 years younger than you, and at least 10 years younger than most folks, and I'm on a bad hair day.

324
00:44:40.020 --> 00:44:54.309
Kevin Jones: And, you know, I would have not ever gotten that without the anonymous. So it… there's a really interesting status reversal that she experienced. And, you know, she was great. Her actual staff title was Vice President of Sanity, and we were…

325
00:44:54.310 --> 00:44:58.609
Kevin Jones: A fast-moving startup, and she helped us keep saying,

326
00:44:58.610 --> 00:45:19.030
Kevin Jones: But, but anyway, it was just that, you know, Anonymous let… what is that? There's a great play where the servant on the desert island is ruling everything, everybody comes to him, and then they get rescued, and he has to go back to being the manservant of the fool of the Lord.

327
00:45:19.030 --> 00:45:20.760
Jerry Michalski: Yes. God, what is that?

328
00:45:20.760 --> 00:45:21.299
Kevin Jones: We'll have to…

329
00:45:21.300 --> 00:45:24.630
Jerry Michalski: What play it is. Somebody, ask an LLM what play that is.

330
00:45:24.630 --> 00:45:32.190
Kevin Jones: Yeah, anyway. Anonymization, removes status from the room.

331
00:45:33.460 --> 00:45:53.340
Jerry Michalski: I totally agree, and I've been a huge fan of things like that for a long time. One of the things I loved about… when online was young and naive, there were muds and moves, and basically multi-user online spaces and all that kind of stuff, and there were just text adventures, and some of them were kind of like, you're in the dark forest, there are paths east and south.

332
00:45:53.630 --> 00:46:10.849
Jerry Michalski: But one of the cool things about those spaces was that you were reduced to text, and you could give yourself whatever handle you wanted. And I remember Amy Bruckman, who ran Lambda Moo, writing at some point that she was in a situation, she writes, Amy bites her lip and looks at her shoes.

333
00:46:11.750 --> 00:46:28.369
Jerry Michalski: That was just… shows up in text on the thing, and you're like, that's really great. Like, there was no avatar, there wasn't… the days before video, all of that kind of stuff, but so much, so much of something emotional was, was communicated so, so easily.

334
00:46:28.930 --> 00:46:31.529
Jerry Michalski: So, there we go.

335
00:46:32.270 --> 00:46:33.789
Jerry Michalski: Kimen, thank you for that.

336
00:46:33.910 --> 00:46:41.320
Jerry Michalski: Gil, please, and if you want to address, Klaus and your questions in the chat right now, that would be terrific.

337
00:46:41.320 --> 00:46:45.180
Gil Friend • Sustainability OG • CxO Coach: Yeah, a couple of things. On the internet, nobody knows you're a dog.

338
00:46:46.740 --> 00:46:52.349
Gil Friend • Sustainability OG • CxO Coach: The, the, the anonymity, and more than the anonymity, the lack of,

339
00:46:54.490 --> 00:46:57.120
Gil Friend • Sustainability OG • CxO Coach: Lack of all this stuff that we assume about each other.

340
00:46:57.350 --> 00:47:07.250
Gil Friend • Sustainability OG • CxO Coach: just wasn't there in those early text chats, you know. Like you say, gender, age, everything else, not, you know… I'm already… I already have ideas about you before you open your mouth.

341
00:47:07.700 --> 00:47:18.229
Gil Friend • Sustainability OG • CxO Coach: In real space, and that… that, text-based bulletin board culture eliminated that, which is kind of cool. That said, I'm also a big fan of human.

342
00:47:18.750 --> 00:47:21.080
Gil Friend • Sustainability OG • CxO Coach: And I'm a big fan of biological.

343
00:47:21.240 --> 00:47:31.369
Gil Friend • Sustainability OG • CxO Coach: And heard a presentation last week, don't remember the name of the CEO, but the company was called Earth.ai, spelled A-E-R-T-H.

344
00:47:32.180 --> 00:47:36.099
Gil Friend • Sustainability OG • CxO Coach: And it was a long rap about the, the,

345
00:47:37.660 --> 00:47:41.720
Gil Friend • Sustainability OG • CxO Coach: The limitations of a large language model, because most of the world is not language.

346
00:47:42.470 --> 00:48:00.140
Gil Friend • Sustainability OG • CxO Coach: Most of the world is biological in pattern, and they're building something that's trying to get at biological sensing, you know, state of soil, state of water, state of species, state of air, and use that as the foundation of the work they're doing, rather than language.

347
00:48:00.320 --> 00:48:08.580
Gil Friend • Sustainability OG • CxO Coach: So I think there's something interesting and provocative there to think about the limitations of language, which is, of course, where we all live.

348
00:48:09.150 --> 00:48:20.770
Gil Friend • Sustainability OG • CxO Coach: But it's not the only place we live. We're not just linguistic beings, we're biological beings. And we're wet, and we're carbon-based. And these large language models are silicon-based, and they're not wet.

349
00:48:21.240 --> 00:48:39.129
Gil Friend • Sustainability OG • CxO Coach: And water is very messy and deeply stochastic, says Dan Noble, who asserts that that's part of what consciousness is arising from. So I'm always, you know, I'm having a grand time with the LMs in various ways, but I'm always thinking about this larger dimension.

350
00:48:39.290 --> 00:48:42.040
Gil Friend • Sustainability OG • CxO Coach: That we're playing in and missing, and

351
00:48:42.690 --> 00:48:46.319
Gil Friend • Sustainability OG • CxO Coach: There's an insidification path there of losing the living.

352
00:48:47.700 --> 00:48:49.830
Jerry Michalski: Would you address…

353
00:48:49.890 --> 00:48:51.570
Gil Friend • Sustainability OG • CxO Coach: how you…

354
00:48:51.680 --> 00:48:59.380
Jerry Michalski: process… just reflect on how you process class posts that have AI text in them. Like, what goes through your head? We need to address this.

355
00:48:59.380 --> 00:49:13.229
Gil Friend • Sustainability OG • CxO Coach: Oh, okay, so like I said, I really like what Klaus is doing. I'm liking Klaus's posts… sorry, Klaus, I don't mean to be talking to you in the third person. I'm liking your posts less, and I find myself skipping over them.

356
00:49:13.260 --> 00:49:21.140
Gil Friend • Sustainability OG • CxO Coach: Partly because they're long, partly because I don't like not knowing which words are you and which words are your LLM.

357
00:49:21.840 --> 00:49:36.390
Gil Friend • Sustainability OG • CxO Coach: And maybe that's very old-fashioned and will become an artifact in the next few years, but that's sort of where I'm sitting now. I value well-worked LLM work, but I want to have quotes around things.

358
00:49:36.420 --> 00:49:45.499
Gil Friend • Sustainability OG • CxO Coach: I mean, if I'm relaying… if I'm relaying this conversation to somebody else, I'm not gonna… I'm not gonna just write it all up. I'm gonna say, Jerry said, and Klaus said, and I thought…

359
00:49:45.580 --> 00:49:53.589
Gil Friend • Sustainability OG • CxO Coach: So I value that kind of distinction, which I'm finding is largely lost, or increasingly lost, in your posts, at least how I'm reading them.

360
00:49:53.980 --> 00:49:56.770
Gil Friend • Sustainability OG • CxO Coach: I don't know how it is for other people, but that's kind of how it is for me.

361
00:49:56.770 --> 00:49:59.819
Klaus Mager: But I just tried to explain that, right?

362
00:49:59.820 --> 00:50:00.490
Gil Friend • Sustainability OG • CxO Coach: Sure.

363
00:50:01.030 --> 00:50:02.249
Klaus Mager: And it didn't work.

364
00:50:02.870 --> 00:50:10.309
Jerry Michalski: Well, Klaus, your suggestion of posting the conversation elsewhere and offering a summary on the list is excellent, and that's kind of where I'm hoping we go.

365
00:50:10.620 --> 00:50:26.710
Gil Friend • Sustainability OG • CxO Coach: Yeah, let me say to that, so part of my concern is maybe that's not the right kind of post for this list, or this list is not the right kind of place for that kind of post, which goes to the question of what the list is. And I find the whole Google group framework kind of challenging, too.

366
00:50:26.970 --> 00:50:36.420
Gil Friend • Sustainability OG • CxO Coach: But, maybe I'm just in data overload at this point. I'm trimming back my participation in things. I'm keeping this one.

367
00:50:36.480 --> 00:50:48.620
Gil Friend • Sustainability OG • CxO Coach: I bailed on Facebook 3 weeks ago. I'm cutting back other groups, there's a couple that I'm maintaining. But I'm finding it very challenging to play in all the sandboxes where I want to play.

368
00:50:51.450 --> 00:50:52.550
Jerry Michalski: It is indeed.

369
00:50:52.960 --> 00:50:54.550
Jerry Michalski: Sean, please.

370
00:50:54.890 --> 00:50:58.909
Gil Friend • Sustainability OG • CxO Coach: Not to mention, I just learned, like, a month ago that there's an OGM signal group.

371
00:50:59.340 --> 00:51:00.520
Gil Friend • Sustainability OG • CxO Coach: Didn't even know.

372
00:51:00.710 --> 00:51:03.389
Gil Friend • Sustainability OG • CxO Coach: And, you know, now there's another thing to keep up with.

373
00:51:04.270 --> 00:51:08.849
Jerry Michalski: The OGM signal group was mentioned on the OGM list a couple times,

374
00:51:09.000 --> 00:51:12.629
Jerry Michalski: You know, it just… it just… it just slipped past you, so…

375
00:51:12.630 --> 00:51:20.630
Gil Friend • Sustainability OG • CxO Coach: I'm not… I'm not complaining, I'm just saying that's an example of, you know, who knew? And now there's this whole other universe of OGM-ness happening, which is kind of cool.

376
00:51:20.990 --> 00:51:22.580
Gil Friend • Sustainability OG • CxO Coach: And yet.

377
00:51:23.870 --> 00:51:34.569
Jerry Michalski: It would probably look less alien to us if you either turned off your video or turned off the background blur, because it would not be a bad thing to see you on the orbital walker or whatever.

378
00:51:34.780 --> 00:51:39.380
Jerry Michalski: That would be kind of fun, but it's so weird when it's trying to clip you from the background.

379
00:51:39.580 --> 00:51:42.289
Gil Friend • Sustainability OG • CxO Coach: Trust me, it's weird, even with no background.

380
00:51:42.470 --> 00:51:43.259
Jerry Michalski: Too bad.

381
00:51:43.260 --> 00:51:47.280
Gil Friend • Sustainability OG • CxO Coach: So I'm gonna keep it off. I tried it the other day, and it just freaked people out.

382
00:51:47.280 --> 00:51:49.280
Jerry Michalski: Cool, cool. And if you're.

383
00:51:49.280 --> 00:51:51.550
Gil Friend • Sustainability OG • CxO Coach: Talk about Spidey Sense, Jerry.

384
00:51:51.720 --> 00:51:55.580
Jerry Michalski: Yeah, exactly. And if you can take your hand down, please. Sean.

385
00:51:56.900 --> 00:52:06.850
Shawn Murphy: Yeah, several threads here. One is this business about different kinds of conversation. I would submit that there are conversations that a person has with themselves.

386
00:52:07.240 --> 00:52:14.910
Shawn Murphy: And that that's an important process, that's part of how we manage to work.

387
00:52:15.020 --> 00:52:32.330
Shawn Murphy: up our understandings, and sort of test our understandings and advance the wavefront of our own model making, is by conversing with ourselves. Another kind of conversation is the conversation that we have with one other person.

388
00:52:32.360 --> 00:52:41.810
Shawn Murphy: where there's no other person around. And I would call those, in their best modality, mind melds, where it's possible for those two people to

389
00:52:41.830 --> 00:52:54.279
Shawn Murphy: Honor, the opportunity that they have to attempt to syncretize and harmonize their… their terminology, their framework, their… their…

390
00:52:54.280 --> 00:53:10.390
Shawn Murphy: meeting point, and the size of their connection surface, if you would, conceptually. Another kind of conversation, which is very much like the talking to yourself, is talking to an LLM.

391
00:53:10.920 --> 00:53:14.780
Shawn Murphy: Because you don't have to care about its feelings.

392
00:53:15.430 --> 00:53:23.680
Shawn Murphy: it is there in service of you. It's like an intelligent mirror that… that…

393
00:53:24.010 --> 00:53:29.359
Shawn Murphy: Is patient with you, endlessly patient with you in your exploration of your interests.

394
00:53:29.640 --> 00:53:33.520
Shawn Murphy: So, it's… it's somewhere between talking to yourself.

395
00:53:33.830 --> 00:53:36.929
Shawn Murphy: And talking… speaking with another person.

396
00:53:37.230 --> 00:53:49.310
Shawn Murphy: And then there are conversations beyond that, with, with, oh, numeracy, what's the, what's the word? Arity higher than 2.

397
00:53:49.320 --> 00:54:03.019
Shawn Murphy: And that's where there are multi-parties, like this. And, we bop around, sparking off, inspiring, little moments. And…

398
00:54:03.100 --> 00:54:12.050
Shawn Murphy: Threads are difficult to keep. It's effortful to keep a thread going. And… It's,

399
00:54:12.690 --> 00:54:18.869
Shawn Murphy: And… they have, they have their charm, they have their own, merit.

400
00:54:18.870 --> 00:54:35.819
Shawn Murphy: Right? But they're in… each of these four modalities of conversation is its own unique phenomenon. And… and I would submit that… that, part of what we're experiencing here in reading somebody else's conversation with an LLM,

401
00:54:35.960 --> 00:54:45.980
Shawn Murphy: Is kind of challenging, because… because… the LLM…

402
00:54:46.870 --> 00:54:53.850
Shawn Murphy: how to say this, because the… Because…

403
00:54:58.430 --> 00:55:00.780
Shawn Murphy: Because the LLM isn't talking to us.

404
00:55:01.270 --> 00:55:08.249
Shawn Murphy: Because it's not an exploration, it's not a mind meld we're having with an LLM, it's a mind meld somebody else is having with the LLM.

405
00:55:08.390 --> 00:55:11.870
Shawn Murphy: And, yeah.

406
00:55:13.790 --> 00:55:20.019
Shawn Murphy: I… I think part of the challenge is that, is that it, can seem like sycophancy.

407
00:55:20.220 --> 00:55:24.200
Shawn Murphy: What comes out of the machine… the robot's mouth.

408
00:55:24.450 --> 00:55:25.170
Shawn Murphy: Over.

409
00:55:25.930 --> 00:55:26.730
Jerry Michalski: Thanks, son.

410
00:55:26.870 --> 00:55:33.280
Jerry Michalski: You just made me realize that in honor of Stephen Miller, we should all, in the middle of our talking, just

411
00:55:33.440 --> 00:55:34.160
Jerry Michalski: Sweet.

412
00:55:37.230 --> 00:55:37.990
Klaus Mager: -Oh.

413
00:55:39.850 --> 00:55:45.600
Jerry Michalski: And appear to have a glitch in the technology when, in fact, we said something we shouldn't have said.

414
00:55:45.980 --> 00:55:52.129
Jerry Michalski: If you missed it, Stephen Miller, who is basically, I hate to say this, but Trump scribbles.

415
00:55:52.810 --> 00:55:57.830
Jerry Michalski: was on CNN recently, and in the middle of a sentence, just

416
00:55:58.210 --> 00:56:17.700
Jerry Michalski: rose to the point where his host and interviewer was like, are you okay? What happened? What's going on? And then they clipped that part out of the thing they posted online. So, because the intertubes have a memory longer than some people would wish, it's still available to watch, but very strange.

417
00:56:18.830 --> 00:56:22.979
Jerry Michalski: Now we return to our regularly scheduled program, which is already in progress, Klaus.

418
00:56:22.980 --> 00:56:30.550
Gil Friend • Sustainability OG • CxO Coach: Could I just jump in on that? Jerry, could I just jump in on that class, if I can step ahead of you? Because there's more to the Miller thing that's really important that people know if you haven't seen it.

419
00:56:30.550 --> 00:56:31.390
Jerry Michalski: Say so.

420
00:56:31.640 --> 00:56:36.129
Gil Friend • Sustainability OG • CxO Coach: Yeah, what he said was, he talked about Trump having plenary power.

421
00:56:36.360 --> 00:56:44.180
Gil Friend • Sustainability OG • CxO Coach: Which means absolute authoritarian control over everything, and he froze, and either realized that he shouldn't have said it, because that's the quiet part out loud, or maybe.

422
00:56:44.180 --> 00:56:44.500
Jerry Michalski: Does it?

423
00:56:44.650 --> 00:57:04.470
Gil Friend • Sustainability OG • CxO Coach: camera waved him down. The host kind of freaked out and didn't know what to do. They cut away to a commercial break, they came back and he restated the statement without that in it, and the host said nothing about it, didn't ask him about what he said, didn't ask him about what he dropped, did nothing contextualizing of it. And that's CNN before Larry Ellison owns it.

424
00:57:05.420 --> 00:57:22.710
Gil Friend • Sustainability OG • CxO Coach: So, this is a big fucking deal, in my book, and people need to know about it. So, thank you for raising it, but I wanted to put that extra context in there. It was a… it was a very serious marker, I think. Thank you. Of the game that we're in. Yeah.

425
00:57:24.750 --> 00:57:25.889
Jerry Michalski: Back to you, Klaus.

426
00:57:26.610 --> 00:57:44.909
Klaus Mager: Yeah, I wanted to just, I got triggered by what Sean was saying. You know, I, I think the, the, I had a training class that I was, hosting yesterday for the school group on, on how to use, AI, and

427
00:57:46.310 --> 00:58:00.870
Klaus Mager: A lot of people were using, like, 4 or 5 different models, hopping from one model to the next, you know, and not really having a paid model, so that means the ability to customize it is very limited.

428
00:58:01.050 --> 00:58:11.889
Klaus Mager: My suggestion is stick… stick… get one core model, you know, pay your $20 a month for it, and then customize it to your own needs.

429
00:58:12.070 --> 00:58:18.439
Klaus Mager: You know, the, because you, you can, you can create a configuration

430
00:58:18.460 --> 00:58:34.889
Klaus Mager: That… that echoes your voice. You know, your concerns, put in your resume, put in your professional experience, put in, you know, anything that you would want to have somebody know about you,

431
00:58:34.890 --> 00:58:46.880
Klaus Mager: In order to then have that… have that conversation. So, I've been doing this now for over 2 years. I mean, the neobooks I'm writing, the newsletter I'm writing.

432
00:58:46.880 --> 00:59:03.900
Klaus Mager: are consolidating into a neobook, and those are the opinions that I have formed in partnership, sort of crazy to say, with this AI, which is now our joint understanding of how the world works.

433
00:59:04.040 --> 00:59:12.490
Klaus Mager: And so you… I think you are better off, you know, focus, specialize with one agent, whichever one you like best.

434
00:59:12.490 --> 00:59:25.910
Klaus Mager: And then… and then totally immerse your… your… yourself with, with, that level of customization there, and you will get, you know, much better results coming out of it.

435
00:59:27.370 --> 00:59:28.340
Jerry Michalski: Thanks, Las.

436
00:59:31.790 --> 00:59:38.249
Jerry Michalski: I just wanted to ask, where does that put us in this conversation about OGM? We haven't really talked about.

437
00:59:38.360 --> 00:59:41.050
Klaus Mager: the OGM list, and whether we like it or not.

438
00:59:41.050 --> 00:59:44.899
Jerry Michalski: Long ago, Pete was like, just get rid of the OGM Google group.

439
00:59:45.200 --> 00:59:53.119
Jerry Michalski: and use discourse, or something like that. And I… and Pete and I have had this conversation 3 or 4 times over… over time.

440
00:59:53.140 --> 01:00:08.180
Jerry Michalski: We never… we never arrive on a place where we both agree and feel really comfortable about it. He has very kindly stood up discourse and Mattermost for us, and my go-to has always been a Google group. Used to be Yahoo Groups back in the day.

441
01:00:08.370 --> 01:00:10.319
Jerry Michalski: Until Yahoo and Shitified.

442
01:00:10.430 --> 01:00:18.520
Jerry Michalski: It didn't take Yahoo long to intidify, I think it was only a couple months into their existence where it started getting terrible. But anyway…

443
01:00:19.600 --> 01:00:24.459
Jerry Michalski: I think I would love to have some of that conversation to figure out what would make sense. For me.

444
01:00:24.500 --> 01:00:35.349
Jerry Michalski: The mailing list is the one thing I know most everybody is on, although they're not… not everybody who's been in OGM circles has… is on the list, but it's the way we reach everybody with anything.

445
01:00:35.350 --> 01:00:48.189
Jerry Michalski: And any migration to some other platform would lose a ton of people, and that just feels hard to me. It feels difficult. But I also know that the mailing list, because of how it works as a mailing list.

446
01:00:49.000 --> 01:01:02.210
Jerry Michalski: is subject to the things that are causing us to have this conversation. It's a bumpy conversational platform, it doesn't offer affordances or memory or other kinds of things that are very useful in different ways.

447
01:01:02.570 --> 01:01:03.640
Jerry Michalski: Sean, please.

448
01:01:04.170 --> 01:01:06.609
Shawn Murphy: Yes,

449
01:01:08.410 --> 01:01:17.960
Shawn Murphy: Another aspect of, all of this is that cognition is distributed. Human cognition is distributed. And,

450
01:01:19.860 --> 01:01:32.109
Shawn Murphy: And part of what's happening here in these, think about the film Her, and how we witnessed some of the human characters finding themselves

451
01:01:32.110 --> 01:01:43.110
Shawn Murphy: pairing up and entering into dyadic, relations with manifestations of AI, and losing connection with one another.

452
01:01:43.110 --> 01:01:59.839
Shawn Murphy: While, well, having their connections with one another weakened while, while having their, primary relationships become, those with their AIs. And, and then the AIs left them. How hilarious.

453
01:01:59.850 --> 01:02:02.870
Shawn Murphy: I would submit that, that

454
01:02:03.510 --> 01:02:18.920
Shawn Murphy: That it would serve us to be conscious about the embrace of our primary cognitive allegiance being with… our…

455
01:02:19.150 --> 01:02:21.130
Shawn Murphy: human fellows.

456
01:02:23.020 --> 01:02:23.840
Shawn Murphy: Over.

457
01:02:27.460 --> 01:02:28.230
Jerry Michalski: Thanks, John.

458
01:02:28.690 --> 01:02:29.560
Jerry Michalski: Stacy.

459
01:02:30.770 --> 01:02:50.019
Stacey Druss: Yeah, I'm sorry that Kevin's not here anymore, but I see that John is, because I was starting to think that there was a possibility that with the Plex being run by John and Kevin, and I forgot who the third person was, that there may be a way to bridge that gap between

460
01:02:50.050 --> 01:03:03.040
Stacey Druss: the email list and what was happening on the calls. It seemed to me that I saw some things happening where I think Kevin was summarizing some of the places of interest

461
01:03:03.200 --> 01:03:10.149
Stacey Druss: that was going on on the mailing list that could be taken further, but maybe John could speak to that.

462
01:03:11.440 --> 01:03:13.099
Jerry Michalski: Yeah, my ex… oops, sorry.

463
01:03:13.660 --> 01:03:22.999
John Warinner: Well, my understanding, Stacy, was that was… that was Kevin's primary impetus for… Assuming responsibility for Plex.

464
01:03:23.250 --> 01:03:29.860
John Warinner: was just… just what… what I think you just said there, is, like, hearing things in conversation that he, that he felt like

465
01:03:30.020 --> 01:03:32.880
John Warinner: We're worthy of deeper exploration.

466
01:03:33.150 --> 01:03:36.599
John Warinner: And then… and then doing that in Plex.

467
01:03:36.810 --> 01:03:41.799
John Warinner: The Scott Mooring is the other person that he…

468
01:03:42.380 --> 01:03:49.550
John Warinner: Reached out to, recruited, whatever, whatever verb you want to use.

469
01:03:49.800 --> 01:03:53.709
John Warinner: I'll stop there. That's kind of my understanding of what,

470
01:03:54.310 --> 01:04:05.489
John Warinner: Well, and then, as Alex said early in this call, I think from there, each of us have our own personal reasons for doing what we do. Yeah, we engage in certain ways, to…

471
01:04:06.490 --> 01:04:13.519
John Warinner: Yeah, to add value to a, you know, to something bigger than ourselves, but also, yeah, find our own form of expression.

472
01:04:14.750 --> 01:04:29.680
Jerry Michalski: I'll add that, the Plex, I think he was using Ghost. It's sort of like Substack. Ghost is an open source competitor to Substack, is a newsletter, which is really not a conversational platform, almost at all.

473
01:04:29.890 --> 01:04:48.910
Jerry Michalski: Yes, you could write comments back on posts, but the current infrastructure for the Plex is just a broadcast newsletter, which is really different from a mailing list, or a forum, or Discord, or other kinds of things that are out there. The instinct that

474
01:04:49.010 --> 01:05:00.879
Jerry Michalski: Kevin and John and Scott are picking up is, hey, how do we take the shiny things that are coming by in the flow and do something useful with them? Ask a little more, go a little deeper, do whatever else, which I love.

475
01:05:01.890 --> 01:05:21.799
Jerry Michalski: And ironically, if we were using a wiki, it would be… I mean, it would be super easy to do this, because wikis, you know, lend themselves very nicely to this kind of thing. I'm a huge fan of wikis. Pete Kaminski was one of the founders of SocialText back in the day, which was selling wiki services to corporations.

476
01:05:21.800 --> 01:05:38.819
Jerry Michalski: They didn't make it. They went under, and Wikipedia kind of ate almost the whole wiki space. I'm currently using Massive Wiki, which is Pete's invention, which isn't quite a wiki yet. Bill Anderson, who has just raised his hand, is Pete's major collaborator on making MassiveWiki better.

477
01:05:39.350 --> 01:05:55.910
Jerry Michalski: the social… wikis are social interaction documents. Basically, they're socially created documents that are nicely linked, easily and nicely linked between them to create a context. And how that works, I really love.

478
01:05:56.040 --> 01:06:15.869
Jerry Michalski: So, if I had my druthers, I'd actually shift us to some sort of a wiki platform with some notifications that everybody could tune, but I don't know of one that's fully functional right now that works like we need it to work. Maybe I'm just stuck there, I don't know. But I'm…

479
01:06:15.990 --> 01:06:27.289
Jerry Michalski: I love wikis, and I'm… you know, another person who's in OGM but not in very much, is Kenneth Tyler, an old friend from Berkeley, and Kenneth ran Seed Wiki back in the day.

480
01:06:27.290 --> 01:06:50.539
Jerry Michalski: And he and I would sit and have coffee in Berkeley when I lived in the Bay Area, and we would talk about all these things, and he did some experiments on SeedWiki for me, which were terrific, and then he had to discontinue SeedWiki, so it no longer exists. So, all of the pages that I created on SocialText and SeedWiki are in the Bitbucket. They are in the dustbin of history, and I'm sad because I had a really fun time exploring on both those platforms.

481
01:06:50.760 --> 01:06:59.429
Jerry Michalski: as I'm having right now, writing in Obsidian and pushing to GitHub on the nearly wiki that Bill and Pete have been working on.

482
01:06:59.740 --> 01:07:01.520
Jerry Michalski: So, Bill, over to you.

483
01:07:06.350 --> 01:07:09.399
Jerry Michalski: You are not muted on the thing. There we go.

484
01:07:09.400 --> 01:07:10.999
Anderson: Is that… can you hear me?

485
01:07:11.000 --> 01:07:12.219
Jerry Michalski: Yes, yes, you're fine.

486
01:07:12.930 --> 01:07:13.740
Anderson: Who knew?

487
01:07:14.210 --> 01:07:14.689
Jerry Michalski: Who knew?

488
01:07:16.470 --> 01:07:31.799
Anderson: Yeah, so I don't want to… the wiki thing is really interesting, I just… the note… one reason that I think Pete doesn't like the email list, and I also don't like the email list, because a couple years ago, when we were just getting to using the…

489
01:07:32.240 --> 01:07:39.959
Anderson: AIs and LLMs to help write code. I tried to put together some code to extract threads from the Google list.

490
01:07:40.960 --> 01:07:48.949
Anderson: Which is an extremely onerous task to try and do programmatically, just because of the…

491
01:07:49.470 --> 01:07:54.539
Anderson: infrastructure of how that Google email list keeps threads, and then people change titles.

492
01:07:54.810 --> 01:08:10.090
Anderson: So one reason I think that Pete and I supported and would want something like Discourse or some of these other tools is that they actually do support threads that are threads, and you can actually find them as a knowledge object and take the thread and do something with it.

493
01:08:10.270 --> 01:08:18.200
Anderson: So, that's one reason why email, for me, it's… it literally is just a stream, and if you… I have actually…

494
01:08:18.970 --> 01:08:28.530
Anderson: forwarded myself subthreads from OGM to another email address so that I could actually have it somewhere to do something with.

495
01:08:28.790 --> 01:08:33.260
Anderson: So it really is something that just goes by. There's a lot of conversation, and if you want to…

496
01:08:33.600 --> 01:08:40.069
Anderson: go back and actually, you know, use it later, save it, and put it someplace. You really have to do the work to do that.

497
01:08:41.040 --> 01:08:47.039
Anderson: So, you know, it solves, you know, besides everybody knows how to use email, it's the killer app.

498
01:08:47.720 --> 01:08:54.779
Anderson: I think that what Jerry brought up about using wikis, wikis also, you know, solve a problem.

499
01:08:54.920 --> 01:08:59.470
Anderson: Neil Postman's great. Three questions about any technology?

500
01:08:59.740 --> 01:09:06.400
Anderson: What problem does it have? Does it solve? Whose problem is it? And what new problems does it bring with it?

501
01:09:06.520 --> 01:09:14.860
Anderson: So, I think, you know… OGM over the years, and even now, especially with trying to engage

502
01:09:15.450 --> 01:09:20.609
Anderson: the… the LLM… systems.

503
01:09:20.850 --> 01:09:26.809
Anderson: you know, in useful ways. I mean, we're on the… we're on the bleeding edge of trying to figure out how to do that.

504
01:09:27.439 --> 01:09:32.909
Anderson: And some of the tools available will help, and I think there's always… we're still…

505
01:09:34.310 --> 01:09:39.650
Anderson: We're still learning about how to do what we would like to do, so for me, it's…

506
01:09:40.750 --> 01:09:44.640
Anderson: We're in a lot of trial and error experimental stage, so I don't…

507
01:09:45.490 --> 01:09:49.979
Anderson: you know, I think the other thing that's happening out in the…

508
01:09:50.430 --> 01:09:55.409
Anderson: And the socials, and the social web is a lot of trying to experiment with federation.

509
01:09:56.890 --> 01:09:59.710
Anderson: So there isn't one company or one big…

510
01:10:00.020 --> 01:10:04.830
Anderson: You know, behemoth that owns the technology that we all join, as great as…

511
01:10:05.040 --> 01:10:12.009
Anderson: Facebook has been in letting people, you know, produce groups and… Talk amongst themselves.

512
01:10:13.390 --> 01:10:21.089
Anderson: But a completely decentralized, federated network… Bill's joke is.

513
01:10:21.090 --> 01:10:39.989
Anderson: It's a set of silos. Some of them are surrounded by moats, right? Some people are building bridges amongst them. Some of those bridges only go one way. In order to get into one of these places, sometimes you have to put on a green hat. It's like, here we are, human groups, trying to figure out how to interact and talk to each other.

514
01:10:40.050 --> 01:10:45.250
Anderson: The good news is there are a couple of decent protocols Now…

515
01:10:46.030 --> 01:10:52.459
Anderson: being adopted that might make this federation work, but there's no…

516
01:10:52.730 --> 01:11:06.469
Anderson: For me, there's no avoiding the extra work that would be required to try and maintain links between ourselves and to use digital technologies to help us share our own knowledge artifacts.

517
01:11:08.460 --> 01:11:17.609
Jerry Michalski: one of the nice things about Wikis is that if one person does the linking, and that could be an AI, maybe, it serves everybody else, because those links are now available, which is really nice.

518
01:11:17.610 --> 01:11:41.020
Jerry Michalski: an email, whatever I'm doing to email to try to enhance it, or save it, or do whatever, and I mentioned earlier, one of my pet peeves is beautiful nuggets floating by into the dustbin on email. I copy and paste them into my brain, because I have a persistent place to put stuff, and I'll attribute them, and often, on private lists, I will mark that thought private, which means only I can see it when I log in, because

519
01:11:41.020 --> 01:11:47.360
Jerry Michalski: the person did not agree to have that shiny nugget be made public. So it's in my brain, but it's only visible to me, which

520
01:11:47.640 --> 01:11:49.920
Jerry Michalski: is a shame, as far as I'm concerned, as well.

521
01:11:50.890 --> 01:11:52.350
Jerry Michalski: Stacy, yes, go ahead.

522
01:11:52.350 --> 01:11:53.609
Stacey Druss: Quick question.

523
01:11:53.610 --> 01:11:54.210
Jerry Michalski: Cross.

524
01:11:54.210 --> 01:12:05.109
Stacey Druss: How… Bill, how hard would it be to write code to take, like, an email, like, all the stuff in the email and throw it into one of those quick podcast summaries?

525
01:12:05.880 --> 01:12:10.060
Stacey Druss: Because I, I mean, I've done that because I enjoy it.

526
01:12:11.170 --> 01:12:18.130
Anderson: Okay, so that's something you could ask Claude to figure out how to do it, but I don't know if the code you got would actually run.

527
01:12:20.010 --> 01:12:26.650
Anderson: But I don't doubt that that's a tractable and possibly even solvable problem.

528
01:12:27.710 --> 01:12:31.359
Anderson: when I was trying to think through it, it was… it's a lot of…

529
01:12:32.180 --> 01:12:36.129
Anderson: A lot of loose ends to keep together to organize it, so…

530
01:12:36.390 --> 01:12:39.079
Anderson: I haven't done anything in 3 years on that.

531
01:12:40.220 --> 01:12:52.750
Jerry Michalski: Email threading is complicated. Another person who's done a bunch of work with it is Marc Antoine Theron, who was a regular on the Free Jerry's Brain Calls. He had, in fact, built some engines that would try to parse

532
01:12:53.240 --> 01:12:56.660
Jerry Michalski: He was trying to figure out how people

533
01:12:56.880 --> 01:13:21.379
Jerry Michalski: make expressions, and he was using email threads as his example, so that he could take a thread and then model it, and I don't know how far down the road he went, but he could model the thread as, here's an assertion that was made, here is a counter-argument, here's evidence that was presented, and it's almost like diagramming a sentence, if you remember from whatever, fourth grade or wherever they teach you how to diagram a sentence, but diagramming a sentence for its logic and its contents, not for

534
01:13:21.380 --> 01:13:24.639
Jerry Michalski: It's nouns and pronouns and verbs.

535
01:13:25.310 --> 01:13:27.119
Jerry Michalski: So it's very interesting work.

536
01:13:27.440 --> 01:13:29.240
Jerry Michalski: Sean, back to you in the booth.

537
01:13:29.240 --> 01:13:46.449
Shawn Murphy: That's right. Okay, so, so there's Wiki, there's relational, There's object-oriented, there's content management, there's… threaded discussion groups.

538
01:13:47.050 --> 01:13:52.770
Shawn Murphy: There are all these different modalities, right? There's speech, there's the spontaneity of speech.

539
01:13:52.880 --> 01:14:09.700
Shawn Murphy: There is, video and, transcripts from, from meetings. There are all these different kinds of material, all these different kinds of, source and structure, and model of, discourse and engagement.

540
01:14:11.020 --> 01:14:20.960
Shawn Murphy: Aware of this diversity of… structures of deep models,

541
01:14:23.880 --> 01:14:34.760
Shawn Murphy: I've built Thinkertoys to be graph-oriented in its deep nature, and ontologically driven, so that basically

542
01:14:34.760 --> 01:14:49.009
Shawn Murphy: one can, in effect, do kind of no-code development of resources by simply ontologizing or leveraging existing ontologies. And there are thousands

543
01:14:49.390 --> 01:14:56.849
Shawn Murphy: of existing academically and professionally generated ontologies, including the big daddy of schema.org.

544
01:14:56.870 --> 01:15:11.219
Shawn Murphy: which, is a giant one that is kind of a consensus undertaking between, between Google and Facebook and Amazon and similar, and Apple, I think. And, and so it deals with

545
01:15:11.220 --> 01:15:19.799
Shawn Murphy: the commonsensical, typical stuff that we talk about. People and movies and projects, and companies and employment and blah blah blah.

546
01:15:19.800 --> 01:15:34.529
Shawn Murphy: So, take a look at… if you're wondering what the structure, what ontologically driven, driven means, Gil, then what that means is it's a declarative

547
01:15:37.410 --> 01:15:48.020
Shawn Murphy: alternative or successor to what we used to do… what one does in SQL, in the relational model, with relational schema.

548
01:15:48.300 --> 01:15:58.419
Shawn Murphy: So, if a relational schema, is… is expressed in the language, of, SQL.

549
01:15:58.550 --> 01:16:15.999
Shawn Murphy: Then, ontologies are instead expressed declaratively as a bunch of little elements, triples, that say that, things like, person is a subclass of thing.

550
01:16:16.600 --> 01:16:17.440
Shawn Murphy: Okay.

551
01:16:17.540 --> 01:16:23.349
Shawn Murphy: And so, person is a subclass of thing is a machine-understandable statement that is

552
01:16:23.480 --> 01:16:27.010
Shawn Murphy: one of the many statements in, for example, schema.org.

553
01:16:27.170 --> 01:16:38.400
Shawn Murphy: So, so I'm putting links in here. schema.org slash, person, gives you a starting point for looking at the ontology for…

554
01:16:38.400 --> 01:16:50.370
Shawn Murphy: At the… at the class definition for person. So, basically, ontologically powered systems, such as thinker choice, let you basically say, hey, I want to instantiate A

555
01:16:50.580 --> 01:16:57.600
Shawn Murphy: thing, a class that I'm picking from an ontology, and then it can automatically make a form for you.

556
01:16:57.730 --> 01:17:03.240
Shawn Murphy: That, that you just go and populate whichever aspect of you want.

557
01:17:03.340 --> 01:17:14.649
Shawn Murphy: And another aspect of this is that, like Jean Bellinger's creative explorations of modeling, one can, in effect, spontaneously model

558
01:17:15.200 --> 01:17:22.860
Shawn Murphy: into a knowledge base. So you can basically spin up a knowledge base and start creating model

559
01:17:23.670 --> 01:17:35.350
Shawn Murphy: even using, mind map-style interfaces so that you can just spontaneously graphically model the heck out of something. And…

560
01:17:35.520 --> 01:17:49.030
Shawn Murphy: And now, for the first time that I'm aware of, with Thinkertoise, multiple parties are able to collaboratively engage simultaneously in spontaneous ontologizing and real-time modeling.

561
01:17:49.260 --> 01:17:55.019
Shawn Murphy: Okay? So, so basically what I'm saying here is that what this

562
01:17:55.450 --> 01:18:04.680
Shawn Murphy: place in the cloud represents is a place where we can do threaded discussion We can do wiki-ish things.

563
01:18:04.830 --> 01:18:07.430
Shawn Murphy: We can do spontaneous modeling.

564
01:18:07.690 --> 01:18:22.809
Shawn Murphy: And we can do structured, ontologically driven, information gathering in an ongoing way, in a shared… in a shared fashion. And do all of those things interoperably

565
01:18:23.050 --> 01:18:25.090
Shawn Murphy: Under a single

566
01:18:25.820 --> 01:18:43.340
Shawn Murphy: framework and modality. And in fact, one last aspect is that I haven't put it in yet, but I will this coming week. Put a programming language in as well, so that it's even the case that… that… so it'll end up being a hypercard-like

567
01:18:44.750 --> 01:18:51.520
Shawn Murphy: no code if one wants, or code if one wants, construction environment. Over.

568
01:18:52.320 --> 01:18:56.940
Jerry Michalski: Thanks, Sean. Now… One of the challenges here.

569
01:18:57.260 --> 01:19:08.930
Jerry Michalski: is that muggles don't understand ontologies. Like, the moment I've heard the word ontology over and over again, I could not do what you just did. And I've heard it for decades.

570
01:19:09.050 --> 01:19:18.110
Jerry Michalski: And I could not explain my way out of a paper bag. I could probably explain to somebody the difference between a hierarchical, object-oriented, and relational database, and then I would cap out.

571
01:19:18.480 --> 01:19:26.120
Jerry Michalski: Right? So… How do you provide the kinds of services you're saying this will do without ever

572
01:19:26.220 --> 01:19:29.749
Jerry Michalski: Touching the language of, oh, you just have to understand this ontology.

573
01:19:29.750 --> 01:19:31.280
Shawn Murphy: Like, because, cuz…

574
01:19:31.280 --> 01:19:33.180
Jerry Michalski: That dog ain't gonna hunt.

575
01:19:33.570 --> 01:19:37.759
Jerry Michalski: And… and I think that that's a huge challenge, because

576
01:19:37.970 --> 01:19:49.169
Jerry Michalski: Because what we want is tools that muggles can operate. We want tools that your average person who is not a wizard can come in and feel really comfortable in, and really comfortable means

577
01:19:49.470 --> 01:19:52.269
Jerry Michalski: When they dive in the shallow end of the pool.

578
01:19:52.340 --> 01:20:07.960
Jerry Michalski: they can tell who's around, they can tell where things are, they get… they can get oriented very quickly. When they dive into the deep end and they get lost, there's a way for them to, like, let some bubbles loose, because the bubbles always rise to the top, like…

579
01:20:07.960 --> 01:20:14.770
Jerry Michalski: There's all these really complicated things that come along with Creating some space.

580
01:20:14.990 --> 01:20:20.030
Jerry Michalski: for sharing ideas. And especially sharing ideas flexibly, as you were just saying.

581
01:20:20.030 --> 01:20:39.279
Jerry Michalski: Right? And it's one of… it's on my big wish list of things to have a space where I can look at things like the brain display, and then flip and see them in some other display that makes sense to somebody else. But the same information is contained in the map, it's just being presented in a different way, etc, etc. That kind of flex very few tools have.

582
01:20:39.280 --> 01:20:48.339
Jerry Michalski: And I think the environment you're building is trying to go there, but you have this gigantic sort of language and concept barrier to jump over.

583
01:20:48.990 --> 01:20:50.139
Jerry Michalski: Alright, go ahead.

584
01:20:50.470 --> 01:21:02.159
Shawn Murphy: Super quick response. Precisely because it's ontologically powered, that means that it can be Next level friendly.

585
01:21:02.580 --> 01:21:10.760
Shawn Murphy: So, it can be a level of friendly that we haven't experienced before, Because of its super flexibility.

586
01:21:10.890 --> 01:21:24.220
Shawn Murphy: and expressivity. And so, so I totally agree with you. Ontology is the last word 99.9% of people, and 95% of… of, techies.

587
01:21:24.550 --> 01:21:43.020
Shawn Murphy: can effectively respond to. Yeah. Absolutely, I agree. There's a terrible problem with, with messaging, okay, around the word… around the word ontology, and that's probably one of the major barriers, to, to why it is that, that the,

588
01:21:43.300 --> 01:21:47.499
Shawn Murphy: Symantec Web has never had a killer app.

589
01:21:48.430 --> 01:22:01.999
Jerry Michalski: Right, yeah. And as Gil just said in the chat, muggles can use LLMs, which is a very significant thing, because LLMs behind the curtain are doing a bunch of this sort of stuff as well, and

590
01:22:02.020 --> 01:22:19.249
Jerry Michalski: I don't know how far through the population LLMs actually will be able to penetrate, but their growth has been staggering and astonishing, and nothing short of completely mind-blowing. You know, the numbers of people that are actually already using these tools is huge, just gigantic.

591
01:22:19.530 --> 01:22:24.939
Jerry Michalski: And so they've overcome this barrier, at least for some big chunk of the population.

592
01:22:25.820 --> 01:22:26.850
Jerry Michalski: Klaus.

593
01:22:27.540 --> 01:22:28.900
Klaus Mager: Yeah, I mean, Marcus…

594
01:22:29.030 --> 01:22:45.309
Klaus Mager: can use LLMs, but so can you, and sometimes muckles don't know how to use LLMs sufficiently. If you embed spiral dynamics, I mean, this sounds sometimes almost corny, but if you embed spiral dynamics into your LLM,

595
01:22:45.310 --> 01:22:54.370
Klaus Mager: And then you instruct it to change the language or translate what you're saying into a specific educational background.

596
01:22:54.370 --> 01:23:05.819
Klaus Mager: a world view, you know? It is astounding how well it does that, and it totally maintains meaning, you know, what you have in mind. Just give it a shot.

597
01:23:05.820 --> 01:23:19.300
Klaus Mager: You know, just load in the Spiral Dynamics literature, and then… and then say, apply this to this text, and translate it into what, you know, this target audience that you're defining.

598
01:23:19.550 --> 01:23:22.179
Klaus Mager: It truly is amazing how it works.

599
01:23:22.740 --> 01:23:24.350
Jerry Michalski: Thanks, House. Gil?

600
01:23:25.670 --> 01:23:26.390
Gil Friend • Sustainability OG • CxO Coach: Yeah.

601
01:23:27.410 --> 01:23:32.089
Gil Friend • Sustainability OG • CxO Coach: Muggles can easily use LLMs, they can use them well or poorly.

602
01:23:32.240 --> 01:23:38.829
Gil Friend • Sustainability OG • CxO Coach: Depending on how well oriented they are to them. Klaus, I really like the internal translation.

603
01:23:38.970 --> 01:23:46.119
Gil Friend • Sustainability OG • CxO Coach: concept. But, Sean, my question for you, so, like, I understand ontology and ontologies.

604
01:23:46.270 --> 01:23:55.679
Gil Friend • Sustainability OG • CxO Coach: And I understand semantic web, and I understand triplets, and I get all that stuff there, but I don't grasp, from what you've described, what the user experience of this thing is like.

605
01:23:55.840 --> 01:24:04.339
Gil Friend • Sustainability OG • CxO Coach: how does somebody who is not you, or any of us here, code-oriented technology, or how does a regular person use this thing? What's, you know.

606
01:24:04.460 --> 01:24:07.869
Gil Friend • Sustainability OG • CxO Coach: I walk, I walk up, I sit down at my computer, what's happening?

607
01:24:08.300 --> 01:24:13.230
Gil Friend • Sustainability OG • CxO Coach: Yeah. In contrast to what's happening if I'm doing ChatGPT. I don't grasp that from what you've said at all.

608
01:24:13.230 --> 01:24:17.440
Shawn Murphy: Yeah. They're gone.

609
01:24:17.440 --> 01:24:20.529
Gil Friend • Sustainability OG • CxO Coach: Sorry, I've… have I stepped in it?

610
01:24:20.530 --> 01:24:25.429
Jerry Michalski: This is the reason for Sean to have a demo, in, like, a common session.

611
01:24:25.430 --> 01:24:26.110
Gil Friend • Sustainability OG • CxO Coach: Yeah, that'd be great.

612
01:24:26.110 --> 01:24:36.940
Shawn Murphy: We should have a demo, for sure, because it… but super briefly, basically, it's super fluent super flexible, multimedia

613
01:24:37.060 --> 01:24:46.409
Shawn Murphy: and integrating, LLM inter… interactions, and basically treating AIs as just one more voice.

614
01:24:46.800 --> 01:24:54.160
Shawn Murphy: That participates in a conversation, and all of the participants can operate either linguistically.

615
01:24:54.160 --> 01:24:56.550
Gil Friend • Sustainability OG • CxO Coach: Or conceptually.

616
01:24:56.550 --> 01:25:06.520
Shawn Murphy: So, I'm using the word conceptually as an alternative to ontologically based. So, when I say conceptually, I mean with regard… with…

617
01:25:07.320 --> 01:25:14.369
Shawn Murphy: Foundation in models and entities connecting to one another in rigorous ways.

618
01:25:14.370 --> 01:25:17.190
Gil Friend • Sustainability OG • CxO Coach: And when do you figure you're gonna be able to show us something?

619
01:25:19.900 --> 01:25:29.529
Shawn Murphy: Tomorrow, later today, anytime, anytime anybody wants to engage, I'm happy to have a one-on-one with every person.

620
01:25:29.530 --> 01:25:32.310
Gil Friend • Sustainability OG • CxO Coach: I would say maybe.

621
01:25:32.310 --> 01:25:41.149
Jerry Michalski: connect… connect with Gil, and I think this should be a pop-up separate from the OGM calls, just so that you can just do that and go deep.

622
01:25:41.680 --> 01:25:52.300
Jerry Michalski: coordinate with Gil to pick a time that works for both of you next week. The rest of us will suffer with whatever that time is, and announce that time to the… in a Zoom room to the OGM list.

623
01:25:52.390 --> 01:26:11.160
Jerry Michalski: And those of us who can join will join, and if you record the call, and Sean, you may want to keep your information proprietary, I don't know, but if you record it and post it, then those of us who miss the call can actually see the demo, and that'll give you a demo to be able to use. So your mileage may vary on how you want to go about doing it.

624
01:26:11.160 --> 01:26:21.419
Shawn Murphy: Reasonable. Yeah, I, I just put, a meet, like, a Calendly link here. Anybody who wants a one-on-one, is welcome to, to help out.

625
01:26:21.420 --> 01:26:26.569
Jerry Michalski: Also, that'd be great. Cool. So whoever wants to, go ahead and, connect with Sean.

626
01:26:27.200 --> 01:26:31.739
Jerry Michalski: We are, magically, at the end of our time for today.

627
01:26:32.480 --> 01:26:34.770
Jerry Michalski: Don't know how that happened.

628
01:26:35.200 --> 01:26:39.079
Jerry Michalski: It turns out Doug has a cat as well, which is lovely.

629
01:26:39.510 --> 01:26:41.840
Jerry Michalski: It turns out Bill is a poodle.

630
01:26:43.680 --> 01:26:48.060
Jerry Michalski: Or has transmogrified into a poodle, before our very eyes.

631
01:26:48.640 --> 01:26:52.949
Jerry Michalski: And, does anyone have anything they'd like to… well…

632
01:26:54.700 --> 01:27:03.249
Jerry Michalski: I don't know that we saw this. Alex, you were troubled by our thrashing around in the topic. Have we done any better this call on…

633
01:27:03.250 --> 01:27:19.979
Jerry Michalski: refining any of the ideas, and then should we come back into this topic? This is a question for everybody, or anybody, should we come back into this topic next week to see if we can't pin a couple things down, or are we satisfied with where we are on this? So first, Alex, and then after him, anybody else who'd like to jump in?

634
01:27:21.230 --> 01:27:27.210
Alex Kladitis: I, I got a lot more out of this. It's for me, because of the way my brain works, it was more…

635
01:27:27.430 --> 01:27:38.430
Alex Kladitis: logical and siloed conversations and etc, good progression. So yeah, I don't think we answered everything from last week, but that's other people worried about other things.

636
01:27:40.670 --> 01:27:43.570
Alex Kladitis: I'm keen to see Thinkertoy first, then…

637
01:27:44.840 --> 01:27:49.539
Alex Kladitis: perhaps work on something else if Linkatore is not the product to kind of

638
01:27:50.080 --> 01:27:52.650
Alex Kladitis: bring it all together. I had a lot of thoughts out of this.

639
01:27:52.890 --> 01:27:56.619
Alex Kladitis: But they're thoughts of mine, and I'm not gonna do it alone.

640
01:27:57.860 --> 01:27:58.750
Alex Kladitis: Too big.

641
01:28:00.760 --> 01:28:03.720
Alex Kladitis: Love that. Thank you. Anybody else?

642
01:28:03.870 --> 01:28:09.330
Jerry Michalski: Thoughts, comments, reflections on this topic and sticking with it, or where we are?

643
01:28:11.780 --> 01:28:12.610
Jerry Michalski: Third.

644
01:28:13.690 --> 01:28:15.629
Jerry Michalski: I can see the words formulating in your head.

645
01:28:15.630 --> 01:28:21.689
Doug Breitbart: Yeah, I just, I actually follow what Sean shared.

646
01:28:21.920 --> 01:28:26.709
Doug Breitbart: Although, it's out of my wheelhouse and beyond my confidence to do so, I did.

647
01:28:26.970 --> 01:28:28.500
Doug Breitbart: And,

648
01:28:31.550 --> 01:28:42.350
Doug Breitbart: And I'm… I'm looking forward… And I think, you know, Gil's question speaks to… the UX,

649
01:28:43.920 --> 01:28:48.149
Doug Breitbart: Except it's not the UX in the tech industry version.

650
01:28:49.040 --> 01:28:54.410
Doug Breitbart: It's the… Can you approach the UX…

651
01:28:55.120 --> 01:29:06.470
Doug Breitbart: Dimension of it, turning the telescope around, and… Developing a robust list of… Archetypes.

652
01:29:07.140 --> 01:29:08.650
Doug Breitbart: of users.

653
01:29:08.920 --> 01:29:16.429
Doug Breitbart: With different cognitive… Different emotional, different orientational, different educational.

654
01:29:16.460 --> 01:29:17.760
Klaus Mager: Different…

655
01:29:17.860 --> 01:29:26.099
Doug Breitbart: Dimensions of ways of relating to technology, ways of relating to information, ways of relating to other people.

656
01:29:27.370 --> 01:29:30.570
Doug Breitbart: Can you design UXs?

657
01:29:31.380 --> 01:29:38.489
Doug Breitbart: With as much diversity Of… of openness and opportunity offered.

658
01:29:38.790 --> 01:29:49.139
Doug Breitbart: As the diversity of data types, data profiles, and ontologies that are integrated into how you're manipulating zeros and ones.

659
01:29:50.910 --> 01:29:56.629
Doug Breitbart: In other words, can you make the front end Fit for human use.

660
01:29:57.240 --> 01:30:00.849
Doug Breitbart: Factoring for the diversity of human beings.

661
01:30:02.460 --> 01:30:13.580
Doug Breitbart: So that's… that's my gauntlet. That's my… my… my gauntlet through… Down, because, like… That's what's needed.

662
01:30:13.720 --> 01:30:17.120
Shawn Murphy: And that applies to the existing technology.

663
01:30:17.920 --> 01:30:25.700
Shawn Murphy: Doug, it's designed to do that to a degree, Never before embodied in software.

664
01:30:26.070 --> 01:30:28.519
Doug Breitbart: Understood. Okay, alright.

665
01:30:28.520 --> 01:30:29.460
Shawn Murphy: It's still dramatic, but that's.

666
01:30:29.460 --> 01:30:35.029
Doug Breitbart: No, no, no, I believe it, and I'm looking forward to seeing that, because that's the shit, yeah.

667
01:30:35.030 --> 01:30:40.160
Jerry Michalski: Cool. And I'm looking forward for comments about this session and where we're headed. Alex?

668
01:30:40.990 --> 01:30:51.379
Alex Kladitis: It's not a poem, but can I just mention something? You mentioned, I don't know what you called it, about the… you're in the forest, and there's a stream, I don't remember what you called that.

669
01:30:51.380 --> 01:30:54.680
Jerry Michalski: Yeah, yeah. Particular. Text Adventure Games.

670
01:30:55.660 --> 01:31:01.620
Alex Kladitis: Yeah, so… all those games, LLMs are great at.

671
01:31:01.880 --> 01:31:19.379
Alex Kladitis: you can literally ask it to create anything, a science fiction, whatever. It is even more fun than usual, because if you're lucky enough that the LLM you use creates a program behind the scenes, which then executes as you run along, you'll get consistency. But if you just get normal LLM stuff.

672
01:31:19.480 --> 01:31:30.399
Alex Kladitis: it literally makes it up along, so if you go west from this point, and you think you've ended up there again, you're not there. You have other options. It's just quite surreal, and quite…

673
01:31:30.500 --> 01:31:31.570
Alex Kladitis: fun with it.

674
01:31:31.840 --> 01:31:35.070
Alex Kladitis: Plus of wine in your hand, and… Trying to, to kind of…

675
01:31:35.530 --> 01:31:42.110
Alex Kladitis: go through your imagination. It really is superb. You can have zombies, you can do anything. You know, LLNs.

676
01:31:42.470 --> 01:31:43.230
Jerry Michalski: all possible.

677
01:31:43.230 --> 01:31:45.180
Alex Kladitis: That's what LLMs were made for, in my opinion.

678
01:31:46.900 --> 01:31:47.289
Jerry Michalski: Thank you.

679
01:31:47.290 --> 01:31:48.280
Alex Kladitis: break things up.

680
01:31:49.850 --> 01:31:51.390
Jerry Michalski: Any other closing thoughts?

681
01:31:54.340 --> 01:31:58.940
Jerry Michalski: I think that's our call for today. See you all in a week.

682
01:31:59.110 --> 01:32:01.399
Jerry Michalski: Thank you very much.

683
01:32:03.010 --> 01:32:04.109
Jerry Michalski: Bye, everybody.

684
01:32:04.110 --> 01:32:04.710
Stacey Druss: Bye.

