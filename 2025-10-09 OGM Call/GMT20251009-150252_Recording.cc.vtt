WEBVTT

00:00:00.000 --> 00:00:00.000
Where did, uh, record to the cloud.

00:00:00.000 --> 00:00:02.000
Good.

00:00:02.000 --> 00:00:07.000
Welcome to the Open Global Mind weekly call on Thursday, October 9th, 2023.

00:00:07.000 --> 00:00:15.000
5. Um, last week, in lieu of a check-in, which is what we normally do on the first month… first Thursdays of every month,

00:00:15.000 --> 00:00:22.000
We started talking about OGM and our process and our protocols, and how we go about doing stuff.

00:00:22.000 --> 00:00:25.000
A conversation which went really nicely.

00:00:25.000 --> 00:00:28.000
and took us into some really interesting questions that are

00:00:28.000 --> 00:00:32.000
Beyond the immediate needs of how do we do what we do, but rather

00:00:32.000 --> 00:00:40.000
How do online communities actually communicate, and where is this whole damn thing going? And how does AI factor into the mix in a productive way?

00:00:40.000 --> 00:00:43.000
So I'd like to continue that quest today.

00:00:43.000 --> 00:00:45.000
I'd like to see where we can… where we go.

00:00:45.000 --> 00:00:53.000
A couple people, uh, Thorbjorn men and John Warner, sent very thoughtful, uh, replies to my invite.

00:00:53.000 --> 00:00:56.000
to this topic, to the OGM list,

00:00:56.000 --> 00:00:59.000
Which I was just kind of reviewing.

00:00:59.000 --> 00:01:01.000
And, um…

00:01:01.000 --> 00:01:11.000
And I'm wondering if there's any… first, if anybody has any afterthoughts from last week, or just on the topic in general. I just want to open the floor and see what's

00:01:11.000 --> 00:01:13.000
what's on… what's on our minds about this?

00:01:13.000 --> 00:01:16.000
Yeah, and here's John.

00:01:16.000 --> 00:01:19.000
Stacy, please.

00:01:19.000 --> 00:01:24.000
Well, I just had one afterthought. And Jesse actually covered it really well.

00:01:24.000 --> 00:01:26.000
But, um… When… when…

00:01:26.000 --> 00:01:31.000
Strategy runs with this.

00:01:31.000 --> 00:01:32.000
I… that was John, and I just… he's in his car on his phone, so I just muted his line.

00:01:32.000 --> 00:01:37.000
Is there a… is there an echo? Okay.

00:01:37.000 --> 00:01:38.000
No, no worries, John. It's not anybody invading the, uh…

00:01:38.000 --> 00:01:40.000
As long as it's not…

00:01:40.000 --> 00:01:43.000
the call.

00:01:43.000 --> 00:01:51.000
Um, the idea of Facebook had come up. You know, as a negative, and I just wanted to point out that.

00:01:51.000 --> 00:01:58.000
My understanding is that when Facebook was developed, it was developed for them.

00:01:58.000 --> 00:02:07.000
And the idea of developing a platform. For yourself, I think, is an important piece to that.

00:02:07.000 --> 00:02:14.000
That aligns with what Jesse was explaining about the way women create.

00:02:14.000 --> 00:02:20.000
So I just wanted to make that parallel. Like, I know there's so many negatives to Facebook.

00:02:20.000 --> 00:02:28.000
But I think the negatives came in when the idea of monetizing something came in, so I just wanted to bring it back to.

00:02:28.000 --> 00:02:30.000
Creating for community.

00:02:30.000 --> 00:02:34.000
That is a lovely thing to bring back into the conversation. I really appreciate that.

00:02:34.000 --> 00:02:42.000
And I want… and I… my memory of the Facebook birth story is that Zuckerberg created an app to basically find, excuse the language,

00:02:42.000 --> 00:02:45.000
hot babes on the Harvard campus.

00:02:45.000 --> 00:02:46.000
That was the original purpose of Face Smash, I think is what it was called originally.

00:02:46.000 --> 00:02:51.000
Yeah.

00:02:51.000 --> 00:03:00.000
Because when you get to campuses like that, you often, at least you used to, you would get a Facebook, we also called it the Pig Book.

00:03:00.000 --> 00:03:06.000
Because I went to Penn for a while, and it has photos and names of all the people in your class.

00:03:06.000 --> 00:03:11.000
And, uh, you know, you use it to figure out who the heck is that that, you know, seeing down the hall or whatever. Anyway…

00:03:11.000 --> 00:03:16.000
Uh, so yeah, so Stacy, the birth story of Facebook is the opposite.

00:03:16.000 --> 00:03:21.000
of the energy that you and Jesse were sort of bringing into the room, which I would love.

00:03:21.000 --> 00:03:24.000
to explore much more.

00:03:24.000 --> 00:03:28.000
Well, but the point I want to make is, yes, there is that negative there.

00:03:28.000 --> 00:03:29.000
Yeah.

00:03:29.000 --> 00:03:37.000
But there's always two pieces to everything, so while I want to throw out the negative, I don't want to lose the positive piece to it.

00:03:37.000 --> 00:03:45.000
Which, which, if I'm gonna reinterpret, correct me if I'm wrong, which is they built something that was useful for them, that it was useful

00:03:45.000 --> 00:03:46.000
Yes.

00:03:46.000 --> 00:03:52.000
for a group of people trying to get something done, which I totally agree with. The best software… the best software pre-enshinification

00:03:52.000 --> 00:03:54.000
Exactly.

00:03:54.000 --> 00:04:00.000
is software that helps people get something done they really, really want to do.

00:04:00.000 --> 00:04:04.000
And there's… and there's an open question right now about, and I don't think this…

00:04:04.000 --> 00:04:07.000
I don't think this will apply to the big software communication platforms.

00:04:07.000 --> 00:04:11.000
There's an open question right now about will we be looking at apps in the App Store anymore?

00:04:11.000 --> 00:04:14.000
Or will we just talk to an, you know, AI?

00:04:14.000 --> 00:04:22.000
So our AI agent, which then generates ad hoc software that we need to get something done.

00:04:22.000 --> 00:04:30.000
Uh, this kind of obviates persistent software like a community, because if you can't whip up new software every time you need to talk to your buddies,

00:04:30.000 --> 00:04:38.000
Because you need to have been talking to them over time. There's no… I think that the… we'll just whip it up in a moment's notice model doesn't work at all for

00:04:38.000 --> 00:04:48.000
Anything with some history or something like that. But there is this open question that AI is going to get rid of the need to have specialty, small-purpose software apps, because

00:04:48.000 --> 00:04:50.000
It'll just go, oh, I can do that.

00:04:50.000 --> 00:04:52.000
And here's a new app.

00:04:52.000 --> 00:04:55.000
Um, kill.

00:04:55.000 --> 00:05:03.000
Oh, I can do that, but without the quality control.

00:05:03.000 --> 00:05:04.000
For sure.

00:05:04.000 --> 00:05:08.000
that you might find with the professional software shop. I mean, I've been playing a lot with AI, and it takes a lot of iterations.

00:05:08.000 --> 00:05:10.000
To get it to where I want it to get to.

00:05:10.000 --> 00:05:15.000
Um, but back to the platforms for them, platforms for us, etc.

00:05:15.000 --> 00:05:22.000
Um, what I'm wondering is, is insidification tightly correlated with monetization?

00:05:22.000 --> 00:05:25.000
I… so, um…

00:05:25.000 --> 00:05:26.000
I think…

00:05:26.000 --> 00:05:29.000
And the related question is, how do you build something

00:05:29.000 --> 00:05:34.000
Um, that has the appropriate reach for what's trying to be done.

00:05:34.000 --> 00:05:37.000
Uh, and can be sustained economically.

00:05:37.000 --> 00:05:41.000
In the course of doing that, without falling into the acidification machine.

00:05:41.000 --> 00:05:44.000
Yes, and I think that's, uh, one of the… one of the big questions here is,

00:05:44.000 --> 00:05:51.000
Is it possible to have viable, um, sustainable, ongoing concerns software

00:05:51.000 --> 00:05:54.000
that doesn't need to go down the intridification rat hole.

00:05:54.000 --> 00:05:57.000
And related to that,

00:05:57.000 --> 00:05:59.000
Does it if so factor depend on volunteer

00:05:59.000 --> 00:06:02.000
labor to sustain.

00:06:02.000 --> 00:06:09.000
Which means that the volunteers have to have sources of income from somewhere else, so they're, you know, they may be tied into the insidification machine for most of their lives.

00:06:09.000 --> 00:06:12.000
And do volunteer good work, which is lovely.

00:06:12.000 --> 00:06:20.000
And it's how we've gotten a lot of the open source, you know, brilliance of the last 50 years, but it's not a sustainable strategy.

00:06:20.000 --> 00:06:23.000
Um… yes.

00:06:23.000 --> 00:06:25.000
And, um…

00:06:25.000 --> 00:06:32.000
Oh, lost my phone. Uh, other thoughts? I'm sort of checking in for those of you who just joined the call. Kevin Jose.

00:06:32.000 --> 00:06:43.000
Um, I'm asking, are there any afterthoughts from last week's call, or about this topic in general? So we're just, uh, kind of setting the stage for our conversation.

00:06:43.000 --> 00:06:47.000
I wasn't here last week, so I'm not sure that I have any…

00:06:47.000 --> 00:06:49.000
Any thoughts yet?

00:06:49.000 --> 00:06:52.000
No worries, thanks.

00:06:52.000 --> 00:07:02.000
I think John asked… A really interesting set of questions in the email that I'd like to…

00:07:02.000 --> 00:07:08.000
I agree. John, thank you for… you and Thorbjorn sent really thoughtful notes to the retreat list.

00:07:08.000 --> 00:07:11.000
And…

00:07:11.000 --> 00:07:20.000
I'm going to actually copy-paste your note into the chat, which I think will blow the chat

00:07:20.000 --> 00:07:21.000
Let's see if it blows the chat text limit.

00:07:21.000 --> 00:07:29.000
Yes, I have to cut it in half, hold on.

00:07:29.000 --> 00:07:36.000
There we go. That, and that, and of course, it didn't do any of the carriage returns. Sorry about that.

00:07:36.000 --> 00:07:40.000
Um, but John, if you're, um…

00:07:40.000 --> 00:07:45.000
near your… near your device, would you like to, um…

00:07:45.000 --> 00:07:48.000
Talk about what you posted.

00:07:48.000 --> 00:07:51.000
And he… oh, good.

00:07:51.000 --> 00:08:03.000
Yeah, sorry. Um, I, uh… I don't know quite what to say that wouldn't just be repeating what I wrote, but I understand not everybody is.

00:08:03.000 --> 00:08:05.000
That's on the call has seen the… the email…

00:08:05.000 --> 00:08:10.000
I just paste… I just pasted into the chat, although it doesn't look as nice as in your email, but it is available to everybody on this call right now.

00:08:10.000 --> 00:08:25.000
Yeah. I mean, I mean, the main… the main theme of what I was getting at is, uh, for me, it was a bit of an epiphany, and I'm sure that others, uh, you know, this occurred to others, you know, long ago, but…

00:08:25.000 --> 00:08:36.000
Um, in more than one setting, not just OGM, I'm having this experience where I personally am doing a lot with ChatGPT, you know, just as my own internal.

00:08:36.000 --> 00:08:50.000
You know, feedback device, cogitation device, whatever. Um, but I'm seeing more and more, like, in the OGM list and elsewhere, where people are having, you know, like, these…

00:08:50.000 --> 00:08:58.000
Um, revealing, you know, experiences by… by one-on-one explorations with… with AI.

00:08:58.000 --> 00:09:10.000
And understandably, like, we're wanting to share that with others, but we're… so many of us are doing it, like, we don't really have time, you know, to fully digest.

00:09:10.000 --> 00:09:18.000
You know, like, I'm having my own conversations. I don't know that I have… whether it's a matter of interest, time, whatever, bandwidth.

00:09:18.000 --> 00:09:30.000
You know, you just kind of like… it's all we can do to get, you know, get through our own busy days and busy schedules, so it's like… when someone shares… hey, here's the, you know.

00:09:30.000 --> 00:09:37.000
2,000, 4,000 word, you know, exchange that I had, which was probably even an excerpt, right?

00:09:37.000 --> 00:09:51.000
Because it's like the… the words pile up pretty quick, you know, with an exchange back and forth, and a… you know, dialogue through a one-hour hike or something, like what I'm doing occasionally with ChatGPT, and…

00:09:51.000 --> 00:10:14.000
Anyway, so the main point was, how do we share it? And… the other day was just kind of the first time that this light bulb went on in my head, that it was like.

00:10:14.000 --> 00:10:15.000
Sure.

00:10:15.000 --> 00:10:19.000
Well, I guess you gotta back up and ask, like, why are we all having one-on-ones with ChatGPT? Like, why aren't we having conversations like this, where… not just ChatGPT, but you guys get what I mean. Um, like we're the LLM is just a participant in the conversation.

00:10:19.000 --> 00:10:26.000
So that kind of prompted me to ask ChatGPT, like, well, how far off is that? Because I hadn't even really thought about it, you know?

00:10:26.000 --> 00:10:35.000
And the response I got was, oh yeah, you know, like, that's in the pipeline, and it's, you know, maybe a couple years away, because there's a lot to it.

00:10:35.000 --> 00:10:51.000
But it just really got me thinking about, that's gonna be a different… chapter of this deal than what… than what we're currently in, and maybe, in a way, it tempers some of the, you know, the internal frustrations where we're… we're bombarding each other with these…

00:10:51.000 --> 00:11:06.000
You know, loquacious, you know, exchanges, because we really want to share it, right? Like, because we really feel like we are having some… you know, like, intellectual insights and breakthroughs, and so there's a strong desire to share it, but…

00:11:06.000 --> 00:11:12.000
The bandwidth, personal, individual bandwidth just isn't there for each of us to share all this stuff. So, anyway.

00:11:12.000 --> 00:11:25.000
Um, John, thank you for that. That helps and amplifies what you said online. And I'm going to make an analogy that may or may not work for you, but… because I use the brain and do mind mapping, but I also talk to people

00:11:25.000 --> 00:11:31.000
who do Kumu, system maps, and all this and all that, and my experience of other people's maps

00:11:31.000 --> 00:11:33.000
And this may ring a bell for people.

00:11:33.000 --> 00:11:46.000
My experience of other people's maps is that maps that they think are really profound and explain a lot are often impenetrable to me or need to be unpacked really slowly, which takes time. The times that it works best,

00:11:46.000 --> 00:11:56.000
Uh, like with Gene Bellinger, are when Gene starts with a little piece of the story, and then he adds on a loop, and then he adds on a piece over here, and then he basically builds it out.

00:11:56.000 --> 00:12:01.000
which takes the effort on both of our sides. He has to… he has to go back

00:12:01.000 --> 00:12:07.000
Once he's elaborated a large map, he has to go back and figure out what is… how do I then unpack this for people?

00:12:07.000 --> 00:12:20.000
And I think what's happening, um, in LLMs is that these LLMs are very capable of taking us down really fruitful and interesting paths. A couple years ago, early in this Gen AI era,

00:12:20.000 --> 00:12:30.000
Um, I went to pi.ai. It said, what's on your mind? I answered that, and I proceeded in 90 minutes to have the best interview I've ever had with anybody.

00:12:30.000 --> 00:12:39.000
Um, and I think I've mentioned this in an OGM call before, but what it kept doing was, it would say, oh, it sounds like you said this, and it would raise the conversation. Do you mean this?

00:12:39.000 --> 00:12:42.000
And I'd be like, yeah, yeah, yeah, that. And then it would do it again.

00:12:42.000 --> 00:12:50.000
It was not just doing ELISA back to me, it wasn't a stochastic parrot, it was somehow deeply understanding what I was up to, and I was like, oh my god, oh my god!

00:12:50.000 --> 00:12:58.000
And I, you know, I haven't actually gone back to that thread. I think it sort of shook me a little bit, because I'm trying to figure out how to incorporate

00:12:58.000 --> 00:13:04.000
this new set of AIs into my life more, which is a whole different conversation.

00:13:04.000 --> 00:13:06.000
But I say all that because

00:13:06.000 --> 00:13:12.000
Many of us are having these exciting conversations, and then we share them in to our media,

00:13:12.000 --> 00:13:21.000
Uh, which tends to be, at this point, the OGM list, and then it's a lot of words, it's a couple thousand words, and it's in an out form that's

00:13:21.000 --> 00:13:26.000
relatively easy to digest, because bullets and bold help, and the separations help.

00:13:26.000 --> 00:13:31.000
These devices are smart enough to format nicely, but they're sort of not as exciting

00:13:31.000 --> 00:13:35.000
Um, as they are to the person who had the conversation,

00:13:35.000 --> 00:13:52.000
But worse, they impose a long read on whoever is supposed to be, sort of, you know, doing that. And maybe… I'm going to propose a little solution along the way here. Maybe one way to do that is, a lot of these LLMs give you a URL for the conversation,

00:13:52.000 --> 00:14:06.000
And instead of pasting the conversation into the email and sharing the whole thing, just to paste the link to the conversation, and anybody who's up for the long read can go do that, and other people can then skip. But I don't know. I think part of our conversation here is,

00:14:06.000 --> 00:14:13.000
What are the protocols that will make us less uncomfortable or unhappy with how we're going about

00:14:13.000 --> 00:14:25.000
having these conversations, but I like the other part of the conversation, John, which you also opened up, which is, at some point, these agents will be part of our conversation. We will want them to be engaged with this group as a… as…

00:14:25.000 --> 00:14:29.000
Gil's Fathom Notetaker is right now, which I sort of resent.

00:14:29.000 --> 00:14:36.000
Because… because when Gil shows up, he shows up with, like, 3 different note-takers now. I'm like, no, no, don't want them

00:14:36.000 --> 00:14:46.000
But if… but if one of these were as good an interviewer or processor of group dialogue as my experience a couple years ago with Pi.ai,

00:14:46.000 --> 00:14:52.000
I'm in. That could be really fruitful for where we're heading. And I'll go to John first, and then I'll go back to the queue.

00:14:52.000 --> 00:14:58.000
Yeah, so just a quick response to that, Jerry. It's funny that you mentioned Gene Bellinger, because he was the one.

00:14:58.000 --> 00:15:11.000
That told me, this is several years ago. When I was getting into system… thinking, systems mapping, Kumu, all that stuff, and he said, John, just don't make the mistake, he said.

00:15:11.000 --> 00:15:17.000
The value of this is the… is the gerund, right? Like, it's the mapping, it's not the map.

00:15:17.000 --> 00:15:26.000
And, like, it just exact… he forewarned me of what you said, and I had had that experience myself. You do all this work, you bring people the map.

00:15:26.000 --> 00:15:34.000
They can't make any sense of your map. Um, but it's… it's… but if you can lead a group through a mapping exercise.

00:15:34.000 --> 00:15:41.000
They get value from the mapping, not the map. And I would think that the same.

00:15:41.000 --> 00:15:49.000
Totally what you said, the same… operative, as it work when you're… when you're mapping or thinking, or…

00:15:49.000 --> 00:15:55.000
Conversing with an LLM, I think the greatest value is in the conversation.

00:15:55.000 --> 00:15:58.000
Not in the product of the conversation. I'll pass on to others.

00:15:58.000 --> 00:16:01.000
Thank you. Thank you, thank you, thank you.

00:16:01.000 --> 00:16:04.000
Um, Gil, is your hand up from before, or…?

00:16:04.000 --> 00:16:05.000
Are you back in the queue?

00:16:05.000 --> 00:16:07.000
No, it's from before, sorry.

00:16:07.000 --> 00:16:12.000
Okay, good. Um, Alex?

00:16:12.000 --> 00:16:14.000
Uh, you are muted, however.

00:16:14.000 --> 00:16:17.000
We will only hear your wisdom if you unmute.

00:16:17.000 --> 00:16:19.000
Thank you.

00:16:19.000 --> 00:16:26.000
Sorry about that. Um, thanks, John. And, uh. I've got something to say about that, but let me go back first.

00:16:26.000 --> 00:16:34.000
To what you said about what we discussed last week, and um… what we started off this week, and we've gone into one of the topics.

00:16:34.000 --> 00:16:43.000
And it's as follows. Last week, I had the impression that we… it's like… having multiple balloons up in the air at the same time.

00:16:43.000 --> 00:16:50.000
And you kind of keep… you know, backing away whatever comes down, the balloons with the topics.

00:16:50.000 --> 00:16:56.000
And I got the feeling that. We didn't really know what we wanted.

00:16:56.000 --> 00:17:10.000
To discuss, or we knew what we wanted to discuss, but there's so many aspects to it that… We couldn't cohere to anything, come together to anything. So, that's what I was going to say before John spoke, and now I'll go John spoke. So, one of the things is, what are we here for?

00:17:10.000 --> 00:17:14.000
I'm… I'm here. Maybe we should all say what we're here for.

00:17:14.000 --> 00:17:22.000
I'm here because I hear. New perspectives, or existing perspectives from different people with a different viewpoint.

00:17:22.000 --> 00:17:29.000
And it makes me think. Okay, so… so that's why I'm here. But it strikes me that other people are here for other reasons.

00:17:29.000 --> 00:17:34.000
So if we're going to have a tool. What is the tool that will address.

00:17:34.000 --> 00:17:42.000
And I'll just go back to… tied up to John's, uh… what John said, and what you said, Gerry.

00:17:42.000 --> 00:17:49.000
If we take the conversation that someone pastes. A lot of text onto, uh.

00:17:49.000 --> 00:17:55.000
From the start. If I go into email to look at OGM.

00:17:55.000 --> 00:18:00.000
Or if I go into email, I'm looking for snappy, short things to act on.

00:18:00.000 --> 00:18:07.000
If someone says, read this 20-minute thing. That's my view, okay? That doesn't mean everyone…

00:18:07.000 --> 00:18:12.000
But if someone says, read this 20-minute thing, I've got to say, right.

00:18:12.000 --> 00:18:19.000
Okay, I've got all these things to do for my day, let me park it for later, let me do… there's all sorts of… it's a different mechanic.

00:18:19.000 --> 00:18:24.000
So, the other thing about it, if the LLM is participating.

00:18:24.000 --> 00:18:27.000
I doubt if we'd let it go on for 20 minutes.

00:18:27.000 --> 00:18:34.000
On a topic, because it would cut down what it says. One of the value of what we're doing is we're limited to 2-3 minutes.

00:18:34.000 --> 00:18:39.000
It's a self-limitation, I know. But, um, it could be endorsed.

00:18:39.000 --> 00:18:45.000
But when we talk about LLMs, we're… It's a different type of thinking and talking.

00:18:45.000 --> 00:18:53.000
So… come back to this. What is… what do people want out of this group, or this conversation? Not necessarily the group, the conversation?

00:18:53.000 --> 00:19:00.000
And I think that's important. I'll tell you what I want, but…

00:19:00.000 --> 00:19:04.000
Thanks, Alex. Um, that's really useful, and you're…

00:19:04.000 --> 00:19:09.000
I did this when you were talking about you want emails to be short and crisp because

00:19:09.000 --> 00:19:15.000
I run into way too many long reads that are in emails, not because that's a great place for them,

00:19:15.000 --> 00:19:19.000
But because there's somebody who just posted something super interesting, and I know I need to read it.

00:19:19.000 --> 00:19:22.000
I'm remembering that my…

00:19:22.000 --> 00:19:23.000
Yeah.

00:19:23.000 --> 00:19:28.000
But it makes you put… Sorry, but it makes me feel… different, because suddenly it's a different.

00:19:28.000 --> 00:19:33.000
Sorry, the email is not the problem. It's what you have allocated yourself time for.

00:19:33.000 --> 00:19:34.000
Yes.

00:19:34.000 --> 00:19:40.000
So one of Duo GMs. I have to say, I'm not doing email anymore. I am…

00:19:40.000 --> 00:19:44.000
In deep study mode, or… and I'm allocating an hour of my time.

00:19:44.000 --> 00:19:45.000
And that's… That helps.

00:19:45.000 --> 00:19:50.000
which is our self-control, which is our ability to… our ability to manage our time and our attention and all that kind of stuff as well.

00:19:50.000 --> 00:19:55.000
Um, one, one of, um, this is a side dish, but it's related. One of my pet peeves

00:19:55.000 --> 00:19:59.000
for private mailing lists over time and private forums.

00:19:59.000 --> 00:20:04.000
has been that I've seen people post brilliant things that are not in public,

00:20:04.000 --> 00:20:12.000
So I can't point to them and, you know, retweet them, repost them, do whatever. I feel sad that retweet is now, like, a dead verb.

00:20:12.000 --> 00:20:14.000
But…

00:20:14.000 --> 00:20:20.000
a policy I tried to talk about years ago was post-outside-in.

00:20:20.000 --> 00:20:31.000
Meaning, if you have an interesting idea, go write a post about it on whatever your favorite, you know, if it's LiveJournal or Blogger, whatever, go write a post about it, then give us the link here.

00:20:31.000 --> 00:20:38.000
And then we can sort of have the conversation in the more public view, in a different forum, perhaps. That would, you know, I thought that was interesting, because

00:20:38.000 --> 00:20:46.000
I think there's a lot of really good stuff just trapped in private, you know, private online spaces that are now in the Bitbucket. They… nobody goes back and sees them.

00:20:46.000 --> 00:20:49.000
communities move, whatever else happens.

00:20:49.000 --> 00:20:53.000
Um, I'm also reminded that my first experience online was on the well.

00:20:53.000 --> 00:20:58.000
My first sort of good set of

00:20:58.000 --> 00:21:05.000
It's been the well was out of Sausalito, it was a really interesting thing that ran into the bulletin board system called PicoSpan,

00:21:05.000 --> 00:21:08.000
And one of the problems with forums like The Well

00:21:08.000 --> 00:21:13.000
And we had a… well, Pete set up a forum for us here on Discourse. One of my problems with those formats is,

00:21:13.000 --> 00:21:19.000
I can't read through a long list of posts and remember that there was a really good one here, and a really good one here.

00:21:19.000 --> 00:21:23.000
with a whole stretch of crap in between, and then it…

00:21:23.000 --> 00:21:31.000
I knew that there was really good stuff floating by, but I could never keep myself up to date in the conversation.

00:21:31.000 --> 00:21:39.000
on those kinds of media, and that just may be a permanent problem. But I think that it's one of the many facets of what we're trying to solve for here, Alex, and you're right to point out that we've got 12 different questions going?

00:21:39.000 --> 00:21:49.000
I think that… I think… I think the right tool isn't there, and, you know, we should get the right tool, but what we've got now is…

00:21:49.000 --> 00:21:58.000
However, I would like to ask you, Jerry. Why are you in this group? I know you're organizing and owning it, but…

00:21:58.000 --> 00:21:59.000
First of all, um,

00:21:59.000 --> 00:22:00.000
What's in it for you?

00:22:00.000 --> 00:22:06.000
I learn a lot of stuff I would never figure out, um, because everybody shows up here and shares in,

00:22:06.000 --> 00:22:08.000
stuff they're finding, stuff they're thinking, and I leave

00:22:08.000 --> 00:22:13.000
Many, not all, but many of our conversations going, oh, I hadn't… I hadn't put that plus that.

00:22:13.000 --> 00:22:16.000
Together in my head, and it's really interesting and useful.

00:22:16.000 --> 00:22:19.000
Uh, and I'm busy curating that into my brain, so for me,

00:22:19.000 --> 00:22:27.000
I get a long-term benefit because the curation over time gets better. Things… knowledge accumulates or accrues,

00:22:27.000 --> 00:22:30.000
And that really works well for me. Also,

00:22:30.000 --> 00:22:36.000
I love who shows up, and I just love the social interactions, I love getting to know more about you.

00:22:36.000 --> 00:22:41.000
I love… I love that we show up and we're not like, oh, what do you do?

00:22:41.000 --> 00:22:46.000
Right? We're like, oh, we know what we do, we've been sitting here talking for, like, 4 or 5 years.

00:22:46.000 --> 00:22:57.000
Um, we're busy onto a whole set of things, and we've got history and all of that, so I love that. And I love also maybe a third thing real quick, and then I'll go to Sean.

00:22:57.000 --> 00:23:05.000
I love that bringing people together in places like this allows people to get to know each other, and then they go off and have conversations and do projects and do things.

00:23:05.000 --> 00:23:15.000
And that is a huge payoff for me. I'm like, if 3 people met someplace that I helped convene, and they did something they loved, I'm… like, that's…

00:23:15.000 --> 00:23:17.000
that's gold for me.

00:23:17.000 --> 00:23:22.000
Um, so that's top of mind, that's kind of why I'm here.

00:23:22.000 --> 00:23:28.000
there's probably a couple other reasons that I'm here that have been frustrated. Like, I would really like to build a global mind,

00:23:28.000 --> 00:23:35.000
I really… the big fungus that I've talked about a little bit on the calls here is my… my funny sort of analogy, uh,

00:23:35.000 --> 00:23:40.000
for what that could look like. We really haven't done that. We haven't done much of that at all.

00:23:40.000 --> 00:23:47.000
Um, and I'm… and I'm wondering, uh, where those things might go as well as part of this process. And I'll also add,

00:23:47.000 --> 00:23:54.000
It's very possible that LLMs are really nice companions for long, threaded discussions, because they might be excellent summarizers,

00:23:54.000 --> 00:24:02.000
or highlighters of what actually happened in these super too-long-to-read threads. The TLDR agent

00:24:02.000 --> 00:24:11.000
might be a real boon here as we walk into deeper conversations, and I think deeper conversations are very important to have.

00:24:11.000 --> 00:24:16.000
Sean, thank you for your patience.

00:24:16.000 --> 00:24:27.000
Well, I'm, uh, typing feverishly, because what I'm deploying is the very latest, latest version of, uh, Thinkertoys, and on my own development machine,

00:24:27.000 --> 00:24:31.000
It's working, and I'm having a hard time getting it deployed.

00:24:31.000 --> 00:24:35.000
If I could only deploy it, then I would be inviting you

00:24:35.000 --> 00:24:40.000
Uh, individually by email to come and start

00:24:40.000 --> 00:24:42.000
Knowledge…

00:24:42.000 --> 00:24:45.000
Collaborating at the knowledge level,

00:24:45.000 --> 00:24:47.000
in real time.

00:24:47.000 --> 00:24:49.000
And so, I… I…

00:24:49.000 --> 00:24:55.000
I… sorry, I keep saying this, but I think I'm building the tool that we're talking about.

00:24:55.000 --> 00:25:01.000
And I think this morning, I'm deploying it, if only the blessed thing I've got!

00:25:01.000 --> 00:25:08.000
I think I've got one little configuration line that's problematic. But, um, uh, what it is, is a…

00:25:08.000 --> 00:25:20.000
I was working it with a different crew last Saturday, and what we were doing was we were working a cycle where we were doing, um, editing a prompt together,

00:25:20.000 --> 00:25:24.000
for an LLM, then we're running the prompt through the LLM,

00:25:24.000 --> 00:25:28.000
And then it was generating

00:25:28.000 --> 00:25:36.000
That prompt was asking for output in Turtle, which is a knowledge format, a graph knowledge representation technology.

00:25:36.000 --> 00:25:40.000
And we were pumping that into

00:25:40.000 --> 00:25:44.000
into Thinkerto Toys, and then immediately graphing

00:25:44.000 --> 00:25:48.000
The, um, uh, the output of that.

00:25:48.000 --> 00:25:51.000
And the graphing was just one

00:25:51.000 --> 00:25:56.000
of multiple visualization technologies, some tabular, some graphical, some

00:25:56.000 --> 00:26:04.000
wiggly graph things, and we were going through the cycle of basically exploring a topic

00:26:04.000 --> 00:26:09.000
in almost real time, because it wasn't all fully wired together last weekend, but it…

00:26:09.000 --> 00:26:12.000
by Saturday, it will be. In, um…

00:26:12.000 --> 00:26:17.000
in near real time, where it was a group of us,

00:26:17.000 --> 00:26:19.000
Collaborating with AI,

00:26:19.000 --> 00:26:23.000
And then the AI was populating…

00:26:23.000 --> 00:26:26.000
one of many shared…

00:26:26.000 --> 00:26:29.000
knowledge, collaboration,

00:26:29.000 --> 00:26:33.000
Workspaces that Thinkerto Toys provides so that

00:26:33.000 --> 00:26:38.000
In effect, we were collaborating together. It was…

00:26:38.000 --> 00:26:40.000
produce an output that we were able to

00:26:40.000 --> 00:26:45.000
Springboard off of and continue to work on together.

00:26:45.000 --> 00:26:50.000
Okay, so there's too much to say about what it scope is.

00:26:50.000 --> 00:26:53.000
But I… I think…

00:26:53.000 --> 00:26:55.000
I'm making the tool.

00:26:55.000 --> 00:26:58.000
that we're talking about, and I invite

00:26:58.000 --> 00:27:02.000
Invite your participation, because I… I…

00:27:02.000 --> 00:27:06.000
again, there's more than fits in this margin.

00:27:06.000 --> 00:27:14.000
So, Sean, I would suggest that if you were to send an invite to the OGM list asking for people to show up for a pop-up

00:27:14.000 --> 00:27:20.000
call to try your tool, you would have a bunch of enthusiastic people jumping in.

00:27:20.000 --> 00:27:21.000
Okay, lovely, thank you so much.

00:27:21.000 --> 00:27:24.000
And then we can go from there and see what's up, so…

00:27:24.000 --> 00:27:25.000
Okay.

00:27:25.000 --> 00:27:28.000
I love that. Um, thank you.

00:27:28.000 --> 00:27:32.000
Stacy, please.

00:27:32.000 --> 00:27:40.000
Um, yeah, Jerry, um, you just kind of hit on it. I was going to highlight what John said.

00:27:40.000 --> 00:27:47.000
Um, recently there was a long AI chat about, um. I think it was about the shutdown.

00:27:47.000 --> 00:27:53.000
And what I did is I ran it through the LLM to get the podcast.

00:27:53.000 --> 00:28:01.000
And what it did is it created a very short podcast that was a great beginning point for a conversation.

00:28:01.000 --> 00:28:06.000
And so that's what I do, that… takes that long text.

00:28:06.000 --> 00:28:13.000
And gets me a great starting place for a conversation. So I just wanted to say that.

00:28:13.000 --> 00:28:16.000
Sort of what you're describing. Um, that's it.

00:28:16.000 --> 00:28:29.000
Thank you. Um, before I turn on the recorder, I was commenting that I keep forgetting the name of an app that I got to use for an online focus group-y kind of thing. It wasn't really a focus group.

00:28:29.000 --> 00:28:33.000
It was about 60 or 70 people anonymized,

00:28:33.000 --> 00:28:37.000
So I was Orange Panda or something like that. You got a color on an animal.

00:28:37.000 --> 00:28:43.000
Um, all anonymized, so at the end of an hour and a half or two-hour session, I did not know anybody in the room.

00:28:43.000 --> 00:28:51.000
which was the major drawback of the whole thing for me, because the thing really worked well. Then there was a topic for the whole discussion,

00:28:51.000 --> 00:28:56.000
We had been given questions ahead of time, and then what happened was we were broken up in… we were sent into breakouts,

00:28:56.000 --> 00:28:59.000
To talk about each of the questions we had been given,

00:28:59.000 --> 00:29:04.000
For very brief periods of time, and we were busy sort of typing in our thoughts and answers.

00:29:04.000 --> 00:29:07.000
There was AI in the system.

00:29:07.000 --> 00:29:12.000
that was summarizing or picking out things from each of the groups and cross-fertilizing them.

00:29:12.000 --> 00:29:20.000
And then at the end of each of your sessions, the AI would summarize and say, I think this is what you said, yes, or I think this is what you've agreed on, yes.

00:29:20.000 --> 00:29:22.000
And it was quite good at that.

00:29:22.000 --> 00:29:28.000
So, the AI was being really, really helpful in ways that I hadn't experienced before.

00:29:28.000 --> 00:29:34.000
in that kind of a setting. Um, so I will… I will discover, rediscover what the name of the app is,

00:29:34.000 --> 00:29:39.000
But I'm… I'm looking forward to, you know, our, uh…

00:29:39.000 --> 00:29:43.000
I, for one, look forward to our new Robot Overlords. No, no, no, I don't mean that.

00:29:43.000 --> 00:29:54.000
But I look forward to having AIs that are participating in this conversation as excellent listeners and summarizers, or something else, I don't know, but I think that

00:29:54.000 --> 00:29:56.000
that conversation is fun to have.

00:29:56.000 --> 00:29:58.000
Uh, and maybe just a wee bit frightening.

00:29:58.000 --> 00:29:59.000
Yes.

00:29:59.000 --> 00:30:07.000
If I could just say, what I like… what I like that the podcast did is it asked good questions.

00:30:07.000 --> 00:30:08.000
Yep, yep. Super. Done?

00:30:08.000 --> 00:30:13.000
That's what I really liked.

00:30:13.000 --> 00:30:22.000
Yeah, I, um… you know, looping back to, um…

00:30:22.000 --> 00:30:30.000
The why in all of this. Um, and…

00:30:30.000 --> 00:30:36.000
You know, the central… Purpose…

00:30:36.000 --> 00:30:43.000
For me, is… understanding.

00:30:43.000 --> 00:30:51.000
And… not mine, but… Um, somebody else's understanding of mine.

00:30:51.000 --> 00:31:00.000
And my understanding of theirs, and us. Sort of having a way to get on the same page.

00:31:00.000 --> 00:31:05.000
On the most simple and basic and fundamental. Semantic level.

00:31:05.000 --> 00:31:11.000
That my meaning and somebody else's meaning. Is the same. Is aligned.

00:31:11.000 --> 00:31:19.000
Is recognized and understood. Not to be confused with, we have to agree with each other.

00:31:19.000 --> 00:31:28.000
I'm not even going that far. Just getting to a commonality of understanding to the same mean at the center.

00:31:28.000 --> 00:31:35.000
As a… as ground zero. And, you know, I…

00:31:35.000 --> 00:31:41.000
I'm working on something that builds on top of Gene and a half a dozen other people's pieces of the elephant.

00:31:41.000 --> 00:31:45.000
And, you know, in a conversation with Jean, what I said to him was.

00:31:45.000 --> 00:31:50.000
That thing about it's not the map, it's the mapping, the exercise of the mapping.

00:31:50.000 --> 00:31:55.000
And I would argue it isn't just the exercise of the mapping.

00:31:55.000 --> 00:32:02.000
When he does one of his maps, he's creating a model. He's reflecting a living system.

00:32:02.000 --> 00:32:09.000
And, um… So, the map reveals a complex system.

00:32:09.000 --> 00:32:16.000
And based on the revelation and relationships living in that system.

00:32:16.000 --> 00:32:24.000
It extrapolates to a model. That, uh, from which one can identify leverage points.

00:32:24.000 --> 00:32:34.000
Like, all in service to informing what's next action. Or where am I focusing attention from an interventional standpoint?

00:32:34.000 --> 00:32:40.000
Or for… from a contributive standpoint. And the I in that is actually.

00:32:40.000 --> 00:32:46.000
For groups of people. Or organizations and scaling up from there.

00:32:46.000 --> 00:32:54.000
So, it's how can… you know, we have faster.

00:32:54.000 --> 00:32:59.000
Capacity… to marshal.

00:32:59.000 --> 00:33:07.000
Much more complex and… diverse an array of variables and inputs in nonlinear.

00:33:07.000 --> 00:33:12.000
Complex system context. To factor into.

00:33:12.000 --> 00:33:22.000
Helping us make better decisions, like make better choices. Take better actions and… and better from the standpoint of.

00:33:22.000 --> 00:33:27.000
Contributing positively. To value and quality of life.

00:33:27.000 --> 00:33:36.000
And minimizing impact and destruction and adverse consequence. In other… in… in other dimensions.

00:33:36.000 --> 00:33:46.000
And, like, that's sort of the point. For me. And the instrumentalities and the steps and the artifacts…

00:33:46.000 --> 00:33:51.000
Not so much. Like, the artifact's only relevant.

00:33:51.000 --> 00:33:56.000
If it's popped up at the right time in the right context, you know.

00:33:56.000 --> 00:34:06.000
Chirotically, to be in service to that moment's discernment, like. What's next?

00:34:06.000 --> 00:34:13.000
And what do we do? As a… as a, you know, product of a group of individuals actually getting to common.

00:34:13.000 --> 00:34:22.000
So, I'm really, like, wallowing around in the… in those really, really core fundamental, like.

00:34:22.000 --> 00:34:32.000
Getting to understanding… factoring for much more complex array of inputs and all the rest.

00:34:32.000 --> 00:34:38.000
And I think, um, LM's… are an enabling.

00:34:38.000 --> 00:34:44.000
Technology affordance to enable us to do. Handled that.

00:34:44.000 --> 00:34:54.000
Um, at a much more complex, dynamic, real-time level. Than ever… than we ever could.

00:34:54.000 --> 00:35:01.000
Uh, before. Um, but at the end of the day, it's helping with decision-making.

00:35:01.000 --> 00:35:02.000
And I'm complete.

00:35:02.000 --> 00:35:07.000
Doug, thank you, and everything you just said rings for me. I think we're on the same mission, in the sense of…

00:35:07.000 --> 00:35:10.000
Helping people make better decisions, how do we, sort of,

00:35:10.000 --> 00:35:13.000
achieve mutual understanding, despite

00:35:13.000 --> 00:35:16.000
stumbling blocks like language and technology.

00:35:16.000 --> 00:35:20.000
All of that. So, I think well.

00:35:20.000 --> 00:35:24.000
This explains partly why we're both in the same Zoom at the same time!

00:35:24.000 --> 00:35:27.000
How about that? Damn.

00:35:27.000 --> 00:35:32.000
And a small aside, I just put in the chat, how are maps useful to others?

00:35:32.000 --> 00:35:40.000
I said earlier that that really complicated maps are sometimes impenetrable to others, but it turns out that there's a small percentage of people who, every now and then ping me and say,

00:35:40.000 --> 00:35:45.000
Thanks for making your brain publicly available. They have figured out how to navigate my brain without me.

00:35:45.000 --> 00:35:58.000
There's another… in the great distribution of people, there's a few people out at the edge, then there's another tier where if I'm guiding and I'm answering and I'm talking around it, they're like, this is great, let's go.

00:35:58.000 --> 00:36:07.000
But they can't sort of solo. But there's a goodly set of people who can solo in my brain, and I feel like they're sort of getting it. There's a couple people

00:36:07.000 --> 00:36:16.000
who seem to write me back and deeply understand why I created a particular context for a particular scene or setting

00:36:16.000 --> 00:36:25.000
Uh, you know, or snapshot in the brain, and that thrills me, because one of the things I'm doing when I curate my brain, I'm curating for two audiences. One is obviously me,

00:36:25.000 --> 00:36:29.000
But another one is, any random person coming along, I want there to be

00:36:29.000 --> 00:36:38.000
few enough words that it's not overwhelming to the eye or the brain when they see a screen, because one of the things I love about the brain is that it lets you

00:36:38.000 --> 00:36:44.000
take a snapshot… a visual snapshot of a lot of stuff very, very quickly and concisely.

00:36:44.000 --> 00:36:48.000
in ways that other displays don't do well, or at least that I've seen.

00:36:48.000 --> 00:36:54.000
And that is also partly probably a function of how my particular brain works, et cetera, et cetera. There's a bunch of other conversations to have there.

00:36:54.000 --> 00:36:59.000
Um, but let's go to class.

00:36:59.000 --> 00:37:06.000
Yeah, well, interesting. I… Yeah, looking… looking back at, um…

00:37:06.000 --> 00:37:11.000
Maybe the last 2 years or so, on how our email.

00:37:11.000 --> 00:37:17.000
Conversations have changed, and our attitude towards someone posting an AI.

00:37:17.000 --> 00:37:23.000
A message has evolved and changed. Exactly, sorry.

00:37:23.000 --> 00:37:31.000
In some way, really reflects. Really reflects our… our own journey with this technology.

00:37:31.000 --> 00:37:41.000
Um, and the… the… I mean, in my case, I've been using this strictly for.

00:37:41.000 --> 00:37:48.000
My own personal evolution in understanding my profession better. They're not getting deeper into.

00:37:48.000 --> 00:37:57.000
A regenerative transition, and then everything, it evolves and goes around it, including political considerations.

00:37:57.000 --> 00:38:10.000
Farm Bill considerations and all of that, right? So, I think the critical part, and this is where I sometimes feel sort of slightly offended here, is.

00:38:10.000 --> 00:38:15.000
I don't post anything. That I couldn't restate in my own words.

00:38:15.000 --> 00:38:25.000
Meaning, I don't post something generated by an AI. Which I deeply don't understand, right? I mean, the assumption is if you say, I want to listen to you, not to that.

00:38:25.000 --> 00:38:29.000
Then you're assuming that I don't understand what I'm posting here, right?

00:38:29.000 --> 00:38:35.000
And so that's sort of a little bit off. Um, because…

00:38:35.000 --> 00:38:44.000
The power of this AI is. That it… it presents you back with a range of options.

00:38:44.000 --> 00:38:54.000
Um, what, uh, where do you want to go next with this? And then you have, like, five different ways where you could go, and you pick the one that you think.

00:38:54.000 --> 00:39:06.000
That aligns with where you are thinking, with your… your beliefs, your understanding, and so on, and then you spin the conversation further, and then you end up.

00:39:06.000 --> 00:39:22.000
You know, with a, uh, with a body of work that… Um, and just like, uh… Cherry, you know, the example that you cited before, it just pushes you way further than you would have been able to get on your own now.

00:39:22.000 --> 00:39:33.000
And so… And then the other thing is… I mean, we have to recognize this thing is here to stay, right? And it's only getting more invasive and better.

00:39:33.000 --> 00:39:41.000
Um, everybody's using it. My son… You know, it's working for Lucid as head of target group of, uh, well.

00:39:41.000 --> 00:39:51.000
Talent branding. He's using Lucid. And he gets, like, complimented how well he has written something, you know?

00:39:51.000 --> 00:40:00.000
He goes, I wouldn't dare tell these guys, you know, how I got to this point, but… Um, you know, he's using the AI to guide him in his job.

00:40:00.000 --> 00:40:07.000
You know, and a lot of people are doing this right now, and it gives them a competitive advantage.

00:40:07.000 --> 00:40:16.000
So, it doesn't matter whether you are, you know, into philosophy or, you know, Buddhism, or whatever it is.

00:40:16.000 --> 00:40:26.000
Um, you can reach a lot deeper into it. And then when I… then this whole posting, I mean, we have… my daughter and I are, you know, building this…

00:40:26.000 --> 00:40:37.000
This company, uh, now, Food with Thought. Um, we have come to the point where we are really pushing down on talking about AI.

00:40:37.000 --> 00:40:47.000
You know, because it's confusing the issue. It's confusing people, everybody has a perception, oftentimes, the first thing you hear back, always using too much electricity, or.

00:40:47.000 --> 00:40:55.000
You know, you get some stuff back where people just, you know, have no idea what this all is, but they have heard a lot of bad things.

00:40:55.000 --> 00:41:07.000
So don't talk about it. You know, you just use it, and uh… Uh, and structure your communication, and then at some point in time, you bring it up and introduce it, and.

00:41:07.000 --> 00:41:16.000
And make it fun, you know? Um, so… so I think, uh, one idea that was already mentioned, you know, when we are talking.

00:41:16.000 --> 00:41:21.000
In email format, I think the idea of you providing, like, a brief summary.

00:41:21.000 --> 00:41:28.000
That really captures your opinion on this thing, and have the AI help you do… crystallize that.

00:41:28.000 --> 00:41:35.000
But then put in the link to the conversation, so anybody wants to know, how did you query this, you know, how did you prompt this thing?

00:41:35.000 --> 00:41:44.000
You know, then you can… you can wait for it yourself, but… Focus your conversations in as long as an email exchange should be and no more.

00:41:44.000 --> 00:41:50.000
Kind of thing, I think that sort of… Now, where I think it may be best to go.

00:41:50.000 --> 00:41:52.000
Yeah, Klaus, thank you for that. I, I, um…

00:41:52.000 --> 00:41:59.000
I appreciate that a lot. And as you were talking, I was thinking, one of the things that seems to have happened, and this is just

00:41:59.000 --> 00:42:05.000
I noticed this in myself, is that maybe we're depreciating AI-written passages in emails,

00:42:05.000 --> 00:42:08.000
Maybe… maybe, um…

00:42:08.000 --> 00:42:15.000
Uh, maybe when we hit a stretch that we know was written by an AI, somehow we're like, ah, I want the human voice.

00:42:15.000 --> 00:42:21.000
Even though… even though it might contain lots of wisdom, and it might be something that, as you just said, Klaus,

00:42:21.000 --> 00:42:27.000
the person sending it completely agrees with and wishes they had written this, this nicely and crisply,

00:42:27.000 --> 00:42:31.000
And there it is, like, you know, and I think…

00:42:31.000 --> 00:42:33.000
One of the factors here is

00:42:33.000 --> 00:42:40.000
we are overwhelmed by things to read. We have so many long reads and so many things that are attracting our attention,

00:42:40.000 --> 00:42:45.000
I… one of my… one of the clouds in my life is the too-many-tabs

00:42:45.000 --> 00:42:55.000
I have open of things that are really interesting. They're really good. And when I get to them and make my way through them, I learn a lot, and I feel like I'm on a…

00:42:55.000 --> 00:42:58.000
Lifelong PhD program, um, you know, except with no advisors, except you guys are my advisors, that's it.

00:42:58.000 --> 00:43:02.000
Yeah.

00:43:02.000 --> 00:43:05.000
Uh, this is… this is my advisory team.

00:43:05.000 --> 00:43:14.000
And… so… so we're in the middle of this moment where we suddenly have the flood valves of words opened up,

00:43:14.000 --> 00:43:17.000
Uh, some of which are good, some of which are AI slop.

00:43:17.000 --> 00:43:24.000
And we need to help each other find our way through it in a way that doesn't overwhelm us or destroy the humanity of our conversations.

00:43:24.000 --> 00:43:29.000
Which I think is one of the real reasons we're also all here, is that we like humans.

00:43:29.000 --> 00:43:34.000
I mean, I don't think we'd be here if we were not liking humans so much.

00:43:34.000 --> 00:43:35.000
So anyway, uh, Kevin, you've got a…

00:43:35.000 --> 00:43:36.000
We found it.

00:43:36.000 --> 00:43:46.000
bruise on your head that indicates you may have had an impact with something.

00:43:46.000 --> 00:43:47.000
Oh, man.

00:43:47.000 --> 00:43:49.000
I did. I got into a fight with another LLM gang, and we were trying to say, my props are better than yours.

00:43:49.000 --> 00:43:52.000
Was it chains and knives, or was it worse?

00:43:52.000 --> 00:43:56.000
No, no, it was large language modules used as clubs. You've seen it happen a lot.

00:43:56.000 --> 00:43:57.000
Totally, it's awful.

00:43:57.000 --> 00:44:05.000
Um, but, you know, going back to an experience that you talked about, about this anonymized sense-making.

00:44:05.000 --> 00:44:16.000
I was in one of those several years ago, and uh… I had a staffer who was 25 years younger than me there. She was the only female.

00:44:16.000 --> 00:44:19.000
And it was anonymized, and she said it was a bad hair day.

00:44:19.000 --> 00:44:32.000
And her opinions were at the top of everybody's rank. And there were lots of, you know, high-status, visual-status guys there who didn't know who they were voting for.

00:44:32.000 --> 00:44:38.000
And she said, I'm the only female, I'm 10… I'm 25 years younger than you, and at least 10 years younger than most folks.

00:44:38.000 --> 00:44:54.000
And I'm on a bared hair day. And, you know, I would have not ever gotten that without the anonymous, so it… There's a really interesting status reversal that she experienced, and, you know, she was great. Her actual staff title was Vice President of Sanity. We were…

00:44:54.000 --> 00:45:03.000
A fast-moving startup, and she helped us keep sane. But anyway, it was just that, you know, Anonymous let…

00:45:03.000 --> 00:45:18.000
What is that? There's a great play, uh, where the servant on the desert island is ruling everything, everybody comes to him, and then.

00:45:18.000 --> 00:45:20.000
Yes, God, what is that?

00:45:20.000 --> 00:45:21.000
We'll have to ask… somebody ask an LLM what play that is.

00:45:21.000 --> 00:45:24.000
They got rescued, and he has to go back to being the manservant of the fool of the Lord. It's a… I forget what play it is, but it's…

00:45:24.000 --> 00:45:32.000
Yeah, anyway, anonymization removes status from the room.

00:45:32.000 --> 00:45:40.000
I totally agree, and I've been a huge fan of things like that for a long time. One of the things I loved about… when online was young,

00:45:40.000 --> 00:45:54.000
and naive. There were muds and moves, and basically, uh, multi-user online spaces and all that kind of stuff, and they were just text adventures, and some of them were kind of like, you're in the dark forest, there are paths east and south.

00:45:54.000 --> 00:45:58.000
Um, but one of the cool things about those spaces was that you were reduced to text.

00:45:58.000 --> 00:46:00.000
And you could give yourself whatever handle you wanted.

00:46:00.000 --> 00:46:03.000
And I remember Amy Bruckman, who ran LambdaMoo,

00:46:03.000 --> 00:46:11.000
writing at some point that she was in a situation, and she writes, Amy bites her lip and looks at her shoes.

00:46:11.000 --> 00:46:16.000
That was just shows up in text on the thing, and you're like, that's really great!

00:46:16.000 --> 00:46:23.000
But, like, there was no avatar, it wasn't… the days before video, all of that kind of stuff, but so much

00:46:23.000 --> 00:46:29.000
So much of something emotional was communicated so easily.

00:46:29.000 --> 00:46:31.000
So, there we go.

00:46:31.000 --> 00:46:40.000
Uh, Kevin, thank you for that. Uh, Gil, please, and if you want to address Klaus and your questions in the chat right now, that would be terrific.

00:46:40.000 --> 00:46:46.000
Yeah, a couple of things. Uh, you know, on the internet, nobody knows you're a dog.

00:46:46.000 --> 00:46:53.000
the anonymity, and more than the anonymity, the lack of, um…

00:46:53.000 --> 00:46:56.000
The lack of all this stuff that we assume about each other,

00:46:56.000 --> 00:47:02.000
just wasn't there in those early text chats, you know. Like you say, gender, age, everything else, not, you know…

00:47:02.000 --> 00:47:07.000
I'm already… I already have ideas about you before you open your mouse.

00:47:07.000 --> 00:47:09.000
In real space. And that…

00:47:09.000 --> 00:47:15.000
that, uh, text-based bulletin board culture eliminated that, which is kind of cool. That said, um…

00:47:15.000 --> 00:47:18.000
I'm also a big fan of human.

00:47:18.000 --> 00:47:20.000
Uh, and I'm a big fan of biological.

00:47:20.000 --> 00:47:31.000
And heard a presentation last week, um, don't remember the name of the CEO, but the company was called Earth.ai, spelled A-E-R-T-H.

00:47:31.000 --> 00:47:37.000
And it was a long rap about the, um…

00:47:37.000 --> 00:47:41.000
The limitations of a large language model, because most of the world is not language.

00:47:41.000 --> 00:47:54.000
Most of the world is biological and pattern. Um, and they're building something that's trying to get at biological sensing, you know, state of soil, state of water, state of species, state of air.

00:47:54.000 --> 00:47:59.000
And use that as the foundation of the work they're doing, rather than language.

00:47:59.000 --> 00:48:04.000
So I think there's something interesting and provocative there to think about the limitations of language.

00:48:04.000 --> 00:48:07.000
Uh, which is, of course, where we all live.

00:48:07.000 --> 00:48:13.000
Um, but it's not the only place we live. We're not just linguistic beings, we're biological beings, and we're wet.

00:48:13.000 --> 00:48:20.000
And we're carbon-based. And these large language models are silicon-based, and they're not wet.

00:48:20.000 --> 00:48:27.000
Uh, and water is very messy and deeply stochastic, says Dan Noble, who asserts that that's part of what consciousness

00:48:27.000 --> 00:48:33.000
is arising from. So I'm always, you know, I'm having a grand time with the LMs.

00:48:33.000 --> 00:48:38.000
In various ways. But I'm always thinking about this larger dimension.

00:48:38.000 --> 00:48:42.000
that we're playing and missing, and um…

00:48:42.000 --> 00:48:47.000
There's an acidification path there of losing the living.

00:48:47.000 --> 00:48:49.000
Um, would you address…

00:48:49.000 --> 00:48:55.000
how you process… just reflect on how you process class posts that have AI text in them. Well, like, what goes through your head?

00:48:55.000 --> 00:48:57.000
Oh.

00:48:57.000 --> 00:48:58.000
We need to address this.

00:48:58.000 --> 00:49:08.000
Oh, okay, um, so like I said, I really like what Klaus is doing. I'm liking Klaus's posts… I'm sorry, Klaus, I don't keep talking to you in the third person.

00:49:08.000 --> 00:49:12.000
I'm liking your post less, and I find myself skipping over them.

00:49:12.000 --> 00:49:21.000
Um, partly because they're long, partly because I don't like not knowing which words are you and which words are your LLM.

00:49:21.000 --> 00:49:26.000
Uh, and maybe that's very old-fashioned and will become an artifact, uh, in the next few years.

00:49:26.000 --> 00:49:35.000
But that's sort of where I'm sitting now. I value well… well-worked LLM work, but I want to have quotes around things.

00:49:35.000 --> 00:49:50.000
I mean, if I'm relaying… if I'm relaying this conversation to somebody else, I'm not gonna… I'm not gonna just write it all up. I'm gonna say, Jerry said, and Klaus said, and I thought. Uh, so I value that kind of distinction, which I'm finding is largely lost, or increasingly lost.

00:49:50.000 --> 00:49:53.000
In your posts, at least how I'm reading them.

00:49:53.000 --> 00:49:59.000
I don't know how it is for other people, but that's kind of how it is for me.

00:49:59.000 --> 00:50:00.000
Sure.

00:50:00.000 --> 00:50:02.000
But I just tried to explain that, right? And it didn't work.

00:50:02.000 --> 00:50:10.000
Well, Klaus, your suggestion of posting the conversation elsewhere and offering the summary on the list is excellent, and that's kind of where I'm hoping we go.

00:50:10.000 --> 00:50:11.000
Um…

00:50:11.000 --> 00:50:12.000
Yeah.

00:50:12.000 --> 00:50:18.000
Yeah, and actually, let me say to that, so part of my concern is maybe that's not the right kind of post for this list, or this list is not the right kind of place for that kind of post.

00:50:18.000 --> 00:50:20.000
Which goes to the question of what the list is.

00:50:20.000 --> 00:50:26.000
Um, and I find the whole Google group framework kind of challenging, too.

00:50:26.000 --> 00:50:34.000
Um, but, um, maybe I'm just in data overload at this point. I'm trimming back my participation in things.

00:50:34.000 --> 00:50:38.000
I'm keeping this one. I bailed on Facebook 3 weeks ago.

00:50:38.000 --> 00:50:41.000
I'm cutting back other groups. There's a couple that I'm maintaining.

00:50:41.000 --> 00:50:50.000
Um, but I'm finding it very challenging to play in all the sandboxes where I want to play.

00:50:50.000 --> 00:50:54.000
It is indeed. Um, Sean, please.

00:50:54.000 --> 00:50:58.000
Not to mention, I just learned, like, a month ago, that there's no GM signal group.

00:50:58.000 --> 00:51:03.000
Didn't even know. And, you know, now there's another thing to keep up with.

00:51:03.000 --> 00:51:09.000
The OGM signal group was mentioned on the OGM list a couple times, uh…

00:51:09.000 --> 00:51:10.000
Of course, of course.

00:51:10.000 --> 00:51:11.000
you know, it just… it just… it just slipped past you, so…

00:51:11.000 --> 00:51:20.000
I'm not… I'm not complaining, I'm just saying that's an example of, you know, who knew? And now there's this whole other universe of OGM-ness happening, which is kind of cool.

00:51:20.000 --> 00:51:22.000
Um, and yet.

00:51:22.000 --> 00:51:23.000
Anyway…

00:51:23.000 --> 00:51:34.000
It would probably look less alien to us if you either turned off your video or turned off the background blur, because it would not be a bad thing to see you on the orbital walker or whatever.

00:51:34.000 --> 00:51:38.000
That would be kind of fun, but it's so weird when it's trying to clip you from the background.

00:51:38.000 --> 00:51:41.000
Yeah, uh, trust me, it's weird, even with no background.

00:51:41.000 --> 00:51:42.000
Ah, too bad.

00:51:42.000 --> 00:51:46.000
So I'm gonna keep it off. I tried it the other day, and it just freaked people out.

00:51:46.000 --> 00:51:48.000
Cool, cool. Um, and if you can take your ear…

00:51:48.000 --> 00:51:51.000
Talk about spidey sense, Jerry.

00:51:51.000 --> 00:51:56.000
Yeah, exactly. Uh, and if you can take your hand down, please. Uh, Sean.

00:51:56.000 --> 00:52:02.000
Yeah, several threads here. One is this business about different kinds of conversation.

00:52:02.000 --> 00:52:06.000
Um, I would submit that there are conversations that a person has with themselves.

00:52:06.000 --> 00:52:13.000
And that that's an important process, that's part of how we manage to

00:52:13.000 --> 00:52:15.000
work up.

00:52:15.000 --> 00:52:18.000
our understandings and, sort of,

00:52:18.000 --> 00:52:22.000
test our understandings and advance the wavefront of our own

00:52:22.000 --> 00:52:24.000
model-making, is by

00:52:24.000 --> 00:52:31.000
conversing with ourselves. Um, another kind of conversation is the conversation that we have with one other person.

00:52:31.000 --> 00:52:34.000
Where there's no other person around.

00:52:34.000 --> 00:52:41.000
And I would call those, in their best modality, mind melds, where it's possible for those two people to

00:52:41.000 --> 00:52:48.000
honor, um, the opportunity that they have to attempt to syncretize and

00:52:48.000 --> 00:52:52.000
harmonize their… their terminology, their framework, their…

00:52:52.000 --> 00:52:59.000
their meeting point, and the size of their connection surface, if you would.

00:52:59.000 --> 00:53:07.000
Conceptually. Um, another kind of conversation, uh, which is very much like the talking to yourself,

00:53:07.000 --> 00:53:10.000
is talking to an LLM.

00:53:10.000 --> 00:53:14.000
Because you don't have to care about its feelings.

00:53:14.000 --> 00:53:20.000
It is there in service of you. It's like an intelligent mirror

00:53:20.000 --> 00:53:23.000
that… that…

00:53:23.000 --> 00:53:28.000
is patient with you, endlessly patient with you in your exploration of your interests.

00:53:28.000 --> 00:53:33.000
So, it's… it's somewhere between talking to yourself,

00:53:33.000 --> 00:53:36.000
And talking… speaking with another person.

00:53:36.000 --> 00:53:41.000
And then there are conversations beyond that, uh, with, uh, with, um…

00:53:41.000 --> 00:53:47.000
uh… oh, numeracy, what's the… what's the word? Um, arity.

00:53:47.000 --> 00:53:54.000
higher than 2, and that's where there are multi-parties, like this. And, um, we bop around,

00:53:54.000 --> 00:53:59.000
Um, uh, sparking off, uh, inspiring, um, uh,

00:53:59.000 --> 00:54:02.000
little moments, and…

00:54:02.000 --> 00:54:08.000
Uh, threads are difficult to keep. It's effortful to keep a thread going. Um…

00:54:08.000 --> 00:54:12.000
And it's, um…

00:54:12.000 --> 00:54:18.000
And they have… they have their charm, they have their own, um, merit.

00:54:18.000 --> 00:54:25.000
Right? But they're in… at each of these four modalities of conversation is its own unique phenomenon.

00:54:25.000 --> 00:54:29.000
And I would submit that, um…

00:54:29.000 --> 00:54:35.000
Part of what we're experiencing here in reading somebody else's conversation with an LLM,

00:54:35.000 --> 00:54:37.000
um, is…

00:54:37.000 --> 00:54:41.000
kind of challenging, because…

00:54:41.000 --> 00:54:43.000
because…

00:54:43.000 --> 00:54:46.000
The LLM…

00:54:46.000 --> 00:54:50.000
How to say this, because the…

00:54:50.000 --> 00:54:57.000
Because…

00:54:57.000 --> 00:55:00.000
Because the LLM isn't talking to us.

00:55:00.000 --> 00:55:07.000
Because it's not an exploration… it's not a mind meld we're having with an LLM. It's a mind meld somebody else is having with the LLM.

00:55:07.000 --> 00:55:10.000
And, um…

00:55:10.000 --> 00:55:13.000
Yeah.

00:55:13.000 --> 00:55:17.000
I think part of the challenge is that… is that it, uh…

00:55:17.000 --> 00:55:19.000
can seem like sycophancy.

00:55:19.000 --> 00:55:23.000
Um, what comes out of the machine… the robot's mouth.

00:55:23.000 --> 00:55:25.000
Over.

00:55:25.000 --> 00:55:28.000
Thanks, son. You just made me realize that

00:55:28.000 --> 00:55:39.000
In honor of Stephen Miller, we should all, in the middle of our talking, just

00:55:39.000 --> 00:55:41.000
and appear to have a…

00:55:41.000 --> 00:55:45.000
of glitch in the technology, when in fact, we said something we shouldn't have said.

00:55:45.000 --> 00:55:52.000
Um, if you missed it, uh, Stephen Miller, who is basically… I hate to say this, but Trump scribbles,

00:55:52.000 --> 00:55:57.000
was on CNN recently, and in the middle of a sentence, just

00:55:57.000 --> 00:56:03.000
froze, to the point where his host and interviewer was like, are you okay? What happened? What's going on?

00:56:03.000 --> 00:56:08.000
And then they clipped that part out of the thing they posted online, so…

00:56:08.000 --> 00:56:12.000
Uh, because the inter tubes have a memory longer than

00:56:12.000 --> 00:56:18.000
some people would wish, uh, it's still available to watch, but very strange.

00:56:18.000 --> 00:56:23.000
Now we return to our regularly scheduled program, which is already in progress, Klaus!

00:56:23.000 --> 00:56:24.000
Yeah.

00:56:24.000 --> 00:56:30.000
Could I just jump in? Jerry, could I just jump in on that classification step ahead of you? Because there's more to the Miller thing that's really important that people know if you haven't seen it.

00:56:30.000 --> 00:56:31.000
Say so.

00:56:31.000 --> 00:56:35.000
Yet what he said was, he talked about Trump having plenary power.

00:56:35.000 --> 00:56:44.000
Which means absolute authoritarian control over everything, and they froze, and either realized that he shouldn't have said it, because that's the quiet part out loud, or maybe somebody…

00:56:44.000 --> 00:57:00.000
Camera waved him down. The host kind of freaked out and didn't know what to do. They cut away to a commercial break, they came back, and he restated the statement without that in it, and the host said nothing about it, didn't ask him about what he said, didn't ask him about what he dropped, did nothing contextualizing of it.

00:57:00.000 --> 00:57:04.000
And that's CNN before Larry Ellison owns it.

00:57:04.000 --> 00:57:09.000
So, this is a big fucking deal, uh, in my book.

00:57:09.000 --> 00:57:18.000
And people need to know about it. So thank you for raising it, but I wanted to put that extra context in there. It was a very serious marker, I think.

00:57:18.000 --> 00:57:19.000
Thank you.

00:57:19.000 --> 00:57:20.000
of the game that we're in.

00:57:20.000 --> 00:57:21.000
Yeah, yeah.

00:57:21.000 --> 00:57:24.000
Klaus, don't interrupt.

00:57:24.000 --> 00:57:26.000
Back to you, Klaus.

00:57:26.000 --> 00:57:32.000
Yeah, I wanted to just, uh, got triggered by what Sean was saying.

00:57:32.000 --> 00:57:40.000
You know, I think the… I had a training class that I was hosting yesterday for the school group.

00:57:40.000 --> 00:57:52.000
On… on how to use, uh, AI. And uh… um… a lot of people were using, like, 4 or 5 different models, hoping from one model to the next, you know.

00:57:52.000 --> 00:58:01.000
And not really having a paid model, so… That means the ability to customize it is very limited.

00:58:01.000 --> 00:58:05.000
My suggestion is… Stick, stick, get one.

00:58:05.000 --> 00:58:10.000
Core model, you know, pay your $20 a month for it, and then customize it.

00:58:10.000 --> 00:58:18.000
To your own needs. You know, the… Um, because you… you can… you can create a configuration.

00:58:18.000 --> 00:58:28.000
That… that echoes your voice. You know, your concerns put in your resume, put in your… professional experience, put in.

00:58:28.000 --> 00:58:34.000
Now, anything that you would want. To have somebody know about you.

00:58:34.000 --> 00:58:44.000
Um, in order to then have that… have that conversation. So, I've been doing this now for over 2 years. I mean, the neobooks I'm writing.

00:58:44.000 --> 00:58:50.000
The newsletter I'm writing. Consolidating into a neobook.

00:58:50.000 --> 00:58:57.000
Um, and those are the opinions. That I have formed in partnership, sort of crazy to say, with this AI.

00:58:57.000 --> 00:59:04.000
Um, which is now our joint… understanding of, you know, how the world works.

00:59:04.000 --> 00:59:10.000
Um, and so you… I think you are better off, you know, focused, specialized with one agent.

00:59:10.000 --> 00:59:16.000
Whichever one you like best. And then… and then totally immerse your…

00:59:16.000 --> 00:59:26.000
Yourself with, with, uh… that level of customization there, and you will get, you know, much better results coming out of it.

00:59:26.000 --> 00:59:29.000
Thanks, Las.

00:59:29.000 --> 00:59:31.000
Um,

00:59:31.000 --> 00:59:37.000
We're… I just wanted to ask, where… where does that put us in this conversation about OGM? We haven't really talked about

00:59:37.000 --> 00:59:40.000
The OGM list, and whether we like it or not.

00:59:40.000 --> 00:59:45.000
Long ago, Pete was, like, just get rid of the OGM Google group.

00:59:45.000 --> 00:59:48.000
and use discourse, or something like that.

00:59:48.000 --> 00:59:52.000
And I… and Pete and I have had this conversation 3 or 4 times over time.

00:59:52.000 --> 00:59:57.000
We never… we never arrive on a place where we both agree and feel really comfortable about it.

00:59:57.000 --> 01:00:00.000
He has very kindly stood up

01:00:00.000 --> 01:00:07.000
Discourse and matter most for us. And my go-to has always been a Google group. It used to be Yahoo groups back in the day.

01:00:07.000 --> 01:00:09.000
Uh, until Yahoo and Shitified.

01:00:09.000 --> 01:00:16.000
It didn't take Yahoo long to intidify, I think it was only a couple months into their existence where it started getting terrible.

01:00:16.000 --> 01:00:19.000
But anyway, um…

01:00:19.000 --> 01:00:24.000
I think I would love to have some of that conversation to figure out what would make sense. For me,

01:00:24.000 --> 01:00:32.000
The mailing list is the one thing I know most everybody is on, although they're not… not everybody who's been in OGM circles has… is on the list.

01:00:32.000 --> 01:00:34.000
But it's the way we reach everybody with anything.

01:00:34.000 --> 01:00:40.000
And any migration to some other platform would lose a ton of people, and that just…

01:00:40.000 --> 01:00:48.000
feels hard to me. It feels difficult. But I also know that the mailing list, because of how it works as a mailing list,

01:00:48.000 --> 01:00:55.000
is subject to the things that are causing us to have this conversation. It's a bumpy conversational platform.

01:00:55.000 --> 01:01:02.000
It doesn't offer affordances, or memory, or other kinds of things that are very useful in different ways.

01:01:02.000 --> 01:01:03.000
Sean, please.

01:01:03.000 --> 01:01:07.000
Yes, um, uh…

01:01:07.000 --> 01:01:13.000
Another aspect of, um, all of this is that cognition is distributed.

01:01:13.000 --> 01:01:15.000
Human cognition is distributed.

01:01:15.000 --> 01:01:17.000
And, um…

01:01:17.000 --> 01:01:25.000
And part of what's happening here in these, um, uh, think about the film Her.

01:01:25.000 --> 01:01:31.000
And how we witnessed the human characters finding themselves

01:01:31.000 --> 01:01:39.000
pairing up and entering into dyadic relations with manifestations of AI.

01:01:39.000 --> 01:01:42.000
Uh, and, uh, losing connection with one another.

01:01:42.000 --> 01:01:47.000
Um, while, um, uh, well, having their connections with one another weakened.

01:01:47.000 --> 01:01:54.000
While, um, while having their, uh, primary relationships become, uh, um, those with their AIs.

01:01:54.000 --> 01:01:58.000
And, um, and then the AIs left them. The hilarious.

01:01:58.000 --> 01:02:02.000
Um, uh, I would submit that, uh, that

01:02:02.000 --> 01:02:05.000
that it would serve us

01:02:05.000 --> 01:02:10.000
to be conscious about the embrace

01:02:10.000 --> 01:02:16.000
of our primary cognitive allegiance being

01:02:16.000 --> 01:02:18.000
with our…

01:02:18.000 --> 01:02:22.000
human fellows.

01:02:22.000 --> 01:02:26.000
Over.

01:02:26.000 --> 01:02:31.000
Thanks, John. Stacey.

01:02:31.000 --> 01:02:35.000
I'm sorry that Kevin's not here anymore, but I see that John is.

01:02:35.000 --> 01:02:45.000
Because I was starting to think that there was a possibility that with the Plex being run by John and Kevin, and I forgot who the third person was.

01:02:45.000 --> 01:02:50.000
That there may be a way to bridge that gap between.

01:02:50.000 --> 01:02:55.000
The email list and what was happening on the calls. It seemed to me that.

01:02:55.000 --> 01:03:01.000
I saw some things happening where. I think Kevin was summarizing some of the.

01:03:01.000 --> 01:03:10.000
Places of interest. That was going on on the mailing list that could be taken further, but maybe John could speak to that.

01:03:10.000 --> 01:03:13.000
Yeah, and I… go ahead, John.

01:03:13.000 --> 01:03:14.000
Go ahead.

01:03:14.000 --> 01:03:19.000
Yeah, my under… oops, sorry. Well, my understanding, Stacy, was that was… that was Kevin's primary impetus for…

01:03:19.000 --> 01:03:30.000
Uh, assuming responsibility for Plex. Uh, was just… just what I think you just said there, is, like, hearing things in conversation that he… that he felt like.

01:03:30.000 --> 01:03:37.000
We're worthy of deeper exploration. Uh, and then… and then doing that in Plex.

01:03:37.000 --> 01:03:48.000
Um, the Scott Mooring is the other person that he… reached out to, recruited, whatever, whatever verb you want to use, um…

01:03:48.000 --> 01:04:02.000
Uh… I'll stop there. That's kind of my understanding of what… Um, well, and then, as Alex said early in this call, I think from there, each of us have our own personal reasons for doing what we do.

01:04:02.000 --> 01:04:14.000
Yeah, we engage in certain ways, uh, to… Yeah, to add value to a, you know, to something bigger than ourselves, but also, yeah, find our own form of expression.

01:04:14.000 --> 01:04:17.000
I'll add that, um…

01:04:17.000 --> 01:04:29.000
The Plex, I think he was using Ghost. It's sort of like Substack. Ghost is an open-source competitor to Substack, is a newsletter, which is really not a conversational platform, almost at all.

01:04:29.000 --> 01:04:37.000
Um, yes, you could write comments back on posts, but the current infrastructure for the Plex

01:04:37.000 --> 01:04:39.000
is just a broadcast newsletter.

01:04:39.000 --> 01:04:43.000
Which is really different from a mailing list or a forum.

01:04:43.000 --> 01:04:46.000
or Discord, or other kinds of things that are out there.

01:04:46.000 --> 01:04:48.000
The instinct that

01:04:48.000 --> 01:04:56.000
Kevin and John and Scott are picking up is, hey, how do we take the shiny things that are coming by in the flow and do something useful with them?

01:04:56.000 --> 01:05:01.000
Ask a little more, go a little deeper, do whatever else, which I love.

01:05:01.000 --> 01:05:07.000
And ironically, if we were using a wiki, it would be, I mean,

01:05:07.000 --> 01:05:13.000
It would be super easy to do this, because wikis, you know, lend themselves very nicely to this kind of thing,

01:05:13.000 --> 01:05:18.000
I'm a huge fan of wikis. Pete Kaminski was one of the founders of SocialText back in the day, which was

01:05:18.000 --> 01:05:21.000
Selling wiki services to corporations,

01:05:21.000 --> 01:05:26.000
They didn't make it. They went under, and Wikipedia kind of ate almost the whole wiki space.

01:05:26.000 --> 01:05:37.000
I'm currently using MassiveWiki, which is Pete's invention, which isn't quite a wiki yet. Bill Anderson, who has just raised his hand, is Pete's major collaborator on making a massive wiki better.

01:05:37.000 --> 01:05:45.000
Uh, and the social… wikis are social interaction documents. Basically, they're socially created

01:05:45.000 --> 01:05:53.000
documents that are nicely linked, uh, easily and nicely linked between them to create a context.

01:05:53.000 --> 01:05:55.000
And how that works, I really love.

01:05:55.000 --> 01:05:57.000
Um, so…

01:05:57.000 --> 01:06:05.000
if I had my druthers, I'd actually shift us to some sort of a wiki platform with some notifications that everybody could tune,

01:06:05.000 --> 01:06:11.000
But I don't know of one that's fully functional right now that works like we need it to work.

01:06:11.000 --> 01:06:13.000
And maybe I'm just…

01:06:13.000 --> 01:06:15.000
stuck there, I don't know. But I'm…

01:06:15.000 --> 01:06:21.000
I love wikis, and I'm… you know, another person who's in OGM but not in very much,

01:06:21.000 --> 01:06:26.000
is Kenneth Tyler, an old friend from Berkeley, and Kenneth ran SeedWiki back in the day.

01:06:26.000 --> 01:06:35.000
And he and I would sit and have coffee in Berkeley when I lived in the Bay Area, and we would talk about all these things, and he did some experiments on Seed Wiki for me, which were terrific.

01:06:35.000 --> 01:06:46.000
And he had to discontinue SeedWiki, so it no longer exists. So, all of the pages that I created on Social Text and Seed Wiki are in the bit bucket. They are in the dustbin of history, and I'm sad because

01:06:46.000 --> 01:06:50.000
I had a really fun time exploring on both those platforms.

01:06:50.000 --> 01:06:54.000
as I'm having right now, writing in Obsidian and pushing to GitHub,

01:06:54.000 --> 01:06:59.000
on the nearly wiki that Bill and Pete have been working on.

01:06:59.000 --> 01:07:05.000
So, Bill, over to you.

01:07:05.000 --> 01:07:10.000
You are not muted on the thing. There we go.

01:07:10.000 --> 01:07:11.000
Yes, yes, you're fine.

01:07:11.000 --> 01:07:13.000
Is that… can you hear me? Who knew?

01:07:13.000 --> 01:07:16.000
Who knew?

01:07:16.000 --> 01:07:28.000
Yeah, uh, so I don't want to… the wiki thing is really interesting. I just… the note… One reason that I think Pete doesn't like the email list, and I also don't like the email list, because…

01:07:28.000 --> 01:07:32.000
A couple of years ago, when we were just getting to using the.

01:07:32.000 --> 01:07:41.000
Ais and LLMs to help write code. I tried to put together some code to extract threads from the Google list.

01:07:41.000 --> 01:07:55.000
Which is extremely onerous task to try and do programmatically. Just because of the… infrastructure of how that Google email list keeps threads, and then people change titles.

01:07:55.000 --> 01:08:00.000
So one reason I think that. Pete and I supported and would want something like discourse.

01:08:00.000 --> 01:08:10.000
Or some of these other tools is that they actually do support threads that are threads, and you can actually find them as a knowledge object, and take a thread and do something with it.

01:08:10.000 --> 01:08:19.000
So, that's one reason why email, for me, it's… it literally is just a stream. I have actually.

01:08:19.000 --> 01:08:25.000
Forwarded myself subthreads from OGM to another email address so that I can actually be.

01:08:25.000 --> 01:08:33.000
Have it somewhere to do something with. So it really is something that just goes by. There's a lot of conversation, and if you want to.

01:08:33.000 --> 01:08:38.000
Go back and actually, you know, use it later, save it, and put it someplace?

01:08:38.000 --> 01:08:48.000
You really have to do the work to do that. So, you know, it solves, you know, besides, everybody knows how to use email, it's the killer app.

01:08:48.000 --> 01:08:55.000
Um, I think that what Jerry brought up about using wikis. Wikis also… you know, solve a problem.

01:08:55.000 --> 01:09:02.000
Neil Postman's great. Three questions about any technology? What problem does it have? Does it solve?

01:09:02.000 --> 01:09:06.000
Whose problem is it? And what new problems does it bring with it?

01:09:06.000 --> 01:09:15.000
So, I think, you know. Ogm over the years, and even now, especially with trying to engage.

01:09:15.000 --> 01:09:21.000
The LLM… systems.

01:09:21.000 --> 01:09:27.000
You know, in useful ways. I mean, we're on the… we're on the bleeding edge of trying to figure out how to do that.

01:09:27.000 --> 01:09:41.000
And some of the tools available will help, and I think there's always… we're still… We're still learning about how to do what we would like to do, so for me, it's…

01:09:41.000 --> 01:09:50.000
We're in a lot of trial and error experimental stage. So I don't… you know… I think the other thing that's happening out in the…

01:09:50.000 --> 01:09:55.000
In the socials, in the social web is a lot of trying to experiment with federation.

01:09:55.000 --> 01:09:57.000
Mm-hmm.

01:09:57.000 --> 01:10:05.000
So there isn't one company or one big… you know, behemoth that owns the technology that we all join, as great as…

01:10:05.000 --> 01:10:13.000
Facebook has been in letting people, you know. Produce groups, and… talk amongst themselves.

01:10:13.000 --> 01:10:25.000
But a completely decentralized, federated network. Bill's joke is, it's a set of silos, some of them are surrounded by moats.

01:10:25.000 --> 01:10:30.000
Right? Some people are building bridges amongst them. Some of those bridges only go one way.

01:10:30.000 --> 01:10:40.000
In order to get into one of these places, sometimes you have to put on a green hat. It's like, here we are, human groups, trying to figure out how to interact and talk to each other.

01:10:40.000 --> 01:10:44.000
The good news is there are a couple of decent protocols.

01:10:44.000 --> 01:10:49.000
Now… being adapt… adopted.

01:10:49.000 --> 01:10:58.000
That might make this Federation work, but there's no… For me, there's no avoiding the extra work that would be required to try and.

01:10:58.000 --> 01:11:04.000
Maintain links between ourselves, and to use. Digital technologies to help us share.

01:11:04.000 --> 01:11:07.000
Our own knowledge, artifacts.

01:11:07.000 --> 01:11:17.000
One of the nice things about wikis is that if one person does the linking, and that could be an AI maybe, it serves everybody else, because those links are now available, which is really nice.

01:11:17.000 --> 01:11:28.000
and email whatever I'm doing to email to try to enhance it or save it, or do whatever. And I mentioned earlier one of my pet peeves is beautiful nuggets floating by into the dustbin on email.

01:11:28.000 --> 01:11:34.000
I copy and paste them into my brain, because I have a persistent place to put stuff, and I'll attribute them, and often,

01:11:34.000 --> 01:11:40.000
On private lists, I will mark that thought private, which means only I can see it when I log in, because

01:11:40.000 --> 01:11:47.000
The person did not agree to have that shiny nugget be made public, so it's in my brain, but it's only visible to me, which

01:11:47.000 --> 01:11:50.000
is a shame, as far as I'm concerned, as well.

01:11:50.000 --> 01:11:52.000
Stacy, yes, go ahead.

01:11:52.000 --> 01:11:53.000
Yes.

01:11:53.000 --> 01:11:58.000
Good question. How… how… Bill, how hard would it be to write code to take.

01:11:58.000 --> 01:12:06.000
Like an email, like, all the stuff in the email, and throw it into one of those quick podcast summaries.

01:12:06.000 --> 01:12:11.000
Because I've, I mean, I've done that because… I enjoy it.

01:12:11.000 --> 01:12:20.000
Okay, so that's something you could ask Claude to figure out how to do it, but I don't know if the code you got would actually say run.

01:12:20.000 --> 01:12:25.000
But I don't doubt that that's a… that's a tractable and possibly even.

01:12:25.000 --> 01:12:32.000
Solvable problem. When I was trying to think through it, it was… it's a lot of…

01:12:32.000 --> 01:12:39.000
A lot of loose ends to keep together. Organize it. So, I haven't done anything in 3 years on that.

01:12:39.000 --> 01:12:40.000
Okay, good.

01:12:40.000 --> 01:12:45.000
Email threading is complicated. Another person who's done a bunch of work with it is Marc Antoine Parron.

01:12:45.000 --> 01:12:48.000
who was a regular on the Free Jerry's Brain Calls,

01:12:48.000 --> 01:12:52.000
Uh, he had, in fact, built some engines that would try to parse

01:12:52.000 --> 01:12:56.000
Um, he was trying to figure out how people

01:12:56.000 --> 01:13:05.000
make expressions, and he was using email threads as his example, so that he could take a thread and then model it, and I don't know how far down the road he went,

01:13:05.000 --> 01:13:15.000
But he could model the threat as, here's an assertion that was made, here is a counterargument, here's evidence that was presented, and it's almost like diagramming a sentence, if you remember from whatever

01:13:15.000 --> 01:13:21.000
fourth grade, or wherever they teach you how to diagram a sentence, but diagramming a sentence for its logic and its contents, not for its nouns and

01:13:21.000 --> 01:13:24.000
and pronouns and verbs.

01:13:24.000 --> 01:13:26.000
Um, so it's very interesting work.

01:13:26.000 --> 01:13:28.000
Uh, Sean, back to you in the booth.

01:13:28.000 --> 01:13:30.000
That's right.

01:13:30.000 --> 01:13:34.000
Okay, so, uh, so there's Wiki.

01:13:34.000 --> 01:13:37.000
there's relational.

01:13:37.000 --> 01:13:39.000
There's object-oriented,

01:13:39.000 --> 01:13:41.000
There's content management,

01:13:41.000 --> 01:13:43.000
there's…

01:13:43.000 --> 01:13:46.000
threaded discussion groups.

01:13:46.000 --> 01:13:52.000
Um, there are all these different modalities, right? There's speech, there's the spontaneity of speech.

01:13:52.000 --> 01:13:57.000
There is, uh, video and, uh, transcripts from, uh, from meetings.

01:13:57.000 --> 01:14:03.000
There are all these different kinds of material, all these different kinds of source and structure.

01:14:03.000 --> 01:14:08.000
Um, and model of, uh, discourse and engagement.

01:14:08.000 --> 01:14:10.000
Um…

01:14:10.000 --> 01:14:14.000
aware of this diversity of…

01:14:14.000 --> 01:14:19.000
structures of deep models

01:14:19.000 --> 01:14:23.000
Um,

01:14:23.000 --> 01:14:26.000
I've built thinker Toys to be

01:14:26.000 --> 01:14:29.000
graph-oriented in its deep nature,

01:14:29.000 --> 01:14:32.000
And ontologically driven.

01:14:32.000 --> 01:14:35.000
So that, basically, one can

01:14:35.000 --> 01:14:38.000
In effect, do kind of no-code

01:14:38.000 --> 01:14:41.000
development of resources,

01:14:41.000 --> 01:14:43.000
By simply ontologizing…

01:14:43.000 --> 01:14:46.000
or leveraging existing ontologies.

01:14:46.000 --> 01:14:48.000
And there are thousands

01:14:48.000 --> 01:14:56.000
of existing academically and professionally generated ontologies, including the big daddy of schema.org,

01:14:56.000 --> 01:15:06.000
Which, um, is a giant one that is kind of a consensus undertaking between, uh, between Google and Facebook and Amazon and similar.

01:15:06.000 --> 01:15:10.000
And Apple, I think. And, um, and so it deals with

01:15:10.000 --> 01:15:14.000
the commonsensical, typical stuff that we talk about.

01:15:14.000 --> 01:15:19.000
people and movies and projects and companies and employment and blah blah blah.

01:15:19.000 --> 01:15:25.000
Um, so, uh, take a look at… if you're wondering what the structure… what

01:15:25.000 --> 01:15:31.000
ontologically driven means, um, guilt, then what that means is

01:15:31.000 --> 01:15:34.000
It's a declarative

01:15:34.000 --> 01:15:36.000
Um…

01:15:36.000 --> 01:15:39.000
alternative or successor

01:15:39.000 --> 01:15:41.000
to what we used to do… what…

01:15:41.000 --> 01:15:47.000
what one does in SQL in the relational model with relational schema.

01:15:47.000 --> 01:15:51.000
So, if a relational schema, um, is…

01:15:51.000 --> 01:15:55.000
is expressed in the language

01:15:55.000 --> 01:15:57.000
of, uh, SQL.

01:15:57.000 --> 01:16:04.000
then, um, ontologies are instead expressed declaratively

01:16:04.000 --> 01:16:06.000
as a bunch of little

01:16:06.000 --> 01:16:14.000
Elements, triples, that say that, um, things like, uh, person is a subclass of

01:16:14.000 --> 01:16:16.000
thing. Okay.

01:16:16.000 --> 01:16:22.000
And so, person is a subclass of thing, is a machine understandable statement that is

01:16:22.000 --> 01:16:26.000
One of the many statements Ian, for example, schema.org.

01:16:26.000 --> 01:16:33.000
So, um, so I'm putting links in here. Um, uh, schema.org slash, uh, person.

01:16:33.000 --> 01:16:37.000
Uh, gives you a starting point for looking at the ontology for…

01:16:37.000 --> 01:16:44.000
at the… at the class definition for person. So, basically, ontologically powered systems

01:16:44.000 --> 01:16:46.000
Such as thinker Toys let you…

01:16:46.000 --> 01:16:49.000
basically say, hey, I want to instantiate A

01:16:49.000 --> 01:16:57.000
a class that I'm picking from an ontology, and then it can automatically make a form for you.

01:16:57.000 --> 01:17:02.000
Um, that, uh, that you just go and populate whichever aspect of you want.

01:17:02.000 --> 01:17:07.000
Um, and another aspect of this is that, like Gene Bellinger's

01:17:07.000 --> 01:17:09.000
creative explorations of modeling,

01:17:09.000 --> 01:17:14.000
One can, in effect, spontaneously model

01:17:14.000 --> 01:17:20.000
into a knowledge base. So you can basically spin up a knowledge base,

01:17:20.000 --> 01:17:22.000
And start creating model

01:17:22.000 --> 01:17:28.000
even using, um, uh, mind map-style interfaces

01:17:28.000 --> 01:17:32.000
So that you can just spontaneously graphically model the heck out of something.

01:17:32.000 --> 01:17:34.000
Um, and…

01:17:34.000 --> 01:17:38.000
And now, for the first time that I'm aware of, with Thinkertoys,

01:17:38.000 --> 01:17:44.000
multiple parties are able to collaborately engage simultaneously

01:17:44.000 --> 01:17:47.000
and spontaneous ontologizing, and…

01:17:47.000 --> 01:17:53.000
real-time modeling. Okay? So, um, so basically what I'm saying here is that

01:17:53.000 --> 01:18:00.000
what this place in the cloud represents is a place where we can do threaded discussion

01:18:00.000 --> 01:18:04.000
We can do wiki-ish things.

01:18:04.000 --> 01:18:07.000
we can do spontaneous modeling,

01:18:07.000 --> 01:18:11.000
And we can do structured, ontologically driven

01:18:11.000 --> 01:18:16.000
Um… information gathering in an ongoing way, in a shared…

01:18:16.000 --> 01:18:22.000
In a shared fashion. And do all of those things interoperably

01:18:22.000 --> 01:18:25.000
under a single

01:18:25.000 --> 01:18:32.000
framework and modality. And in fact, one last aspect is that I haven't put it in yet, but I will this coming week.

01:18:32.000 --> 01:18:36.000
Put a programming language in as well, um, so that

01:18:36.000 --> 01:18:38.000
It's even the case that

01:18:38.000 --> 01:18:44.000
that… so it'll end up being a HyperCard-like

01:18:44.000 --> 01:18:50.000
no code if one wants, or code if one wants, um, um, construction environment.

01:18:50.000 --> 01:18:51.000
Over.

01:18:51.000 --> 01:18:54.000
Um, thanks, Sean. Now,

01:18:54.000 --> 01:18:56.000
One of the challenges here,

01:18:56.000 --> 01:18:58.000
is that muggles

01:18:58.000 --> 01:19:01.000
don't understand ontologies.

01:19:01.000 --> 01:19:04.000
At the moment I've heard the word ontology over and over again,

01:19:04.000 --> 01:19:06.000
I could not do what you just did.

01:19:06.000 --> 01:19:08.000
And I've heard it for decades.

01:19:08.000 --> 01:19:09.000
Yeah.

01:19:09.000 --> 01:19:17.000
And I could not explain my way out of a paper bag. I could probably explain to somebody the difference between a hierarchical, object-oriented, and relational database, and then I would cap out.

01:19:17.000 --> 01:19:24.000
Right? So, how do you provide the kinds of services you're saying this will do without

01:19:24.000 --> 01:19:29.000
ever touching the language of, oh, you just have to understand this ontology.

01:19:29.000 --> 01:19:30.000
Yeah, yeah.

01:19:30.000 --> 01:19:33.000
Like, because that dog ain't gonna hunt.

01:19:33.000 --> 01:19:34.000
Yeah.

01:19:34.000 --> 01:19:37.000
And I think that that's a huge challenge, because

01:19:37.000 --> 01:19:44.000
Because what we want is tools that muggles can operate. We want tools that your average person who's not a wizard

01:19:44.000 --> 01:19:48.000
can come in and feel really comfortable in, and really comfortable means

01:19:48.000 --> 01:19:51.000
When they dive in the shallow end of the pool,

01:19:51.000 --> 01:19:58.000
Um, they can tell who's around, they can tell where things are, they get… they can get oriented very quickly.

01:19:58.000 --> 01:20:04.000
When they dive into the deep end, and they get lost, there's a way for them to, like, let some bubbles loose.

01:20:04.000 --> 01:20:07.000
Because the bubbles always rise to the top, like, like…

01:20:07.000 --> 01:20:12.000
there's all these really complicated things that come along with

01:20:12.000 --> 01:20:14.000
Creating some space,

01:20:14.000 --> 01:20:19.000
for sharing ideas, and especially sharing ideas flexibly, as you were just saying.

01:20:19.000 --> 01:20:32.000
Right? And it's one of… it's on my big wish list of things to have a space where I can look at things like the brain display, and then flip and see them in some other display that makes sense to somebody else. But the same information is contained

01:20:32.000 --> 01:20:38.000
in the map, it's just being presented in a different way, et cetera, et cetera. That kind of flex, very few tools have.

01:20:38.000 --> 01:20:44.000
And I think the environment you're building is trying to go there, but you have this gigantic, sort of,

01:20:44.000 --> 01:20:48.000
language and concept barrier to jump over.

01:20:48.000 --> 01:20:49.000
Alright, go ahead.

01:20:49.000 --> 01:20:51.000
Super quick response.

01:20:51.000 --> 01:20:57.000
Precisely because it's ontologically powered, that means that it

01:20:57.000 --> 01:20:59.000
can be…

01:20:59.000 --> 01:21:01.000
next-level friendly.

01:21:01.000 --> 01:21:07.000
So, it can be a level of friendly that we haven't experienced before.

01:21:07.000 --> 01:21:10.000
because of its super flexibility,

01:21:10.000 --> 01:21:12.000
And expressivity.

01:21:12.000 --> 01:21:14.000
And so… so I…

01:21:14.000 --> 01:21:21.000
I totally agree with you, ontology is the last word 99.9% of people, and 95% of

01:21:21.000 --> 01:21:23.000
of, uh, techies.

01:21:23.000 --> 01:21:27.000
um, can effectively respond to.

01:21:27.000 --> 01:21:32.000
Um, absolutely, I agree. There's a terrible problem with messaging, okay?

01:21:32.000 --> 01:21:40.000
around the word… around the word ontology, and that's probably one of the major barriers to why it is that, uh,

01:21:40.000 --> 01:21:42.000
that the, um…

01:21:42.000 --> 01:21:48.000
um… semantic web has never had a killer app.

01:21:48.000 --> 01:21:49.000
Yeah.

01:21:49.000 --> 01:21:56.000
Right. And as Gil just said in the chat, muggles can use LLMs, which is a very significant thing, because

01:21:56.000 --> 01:22:01.000
LOMs behind the curtain are doing a bunch of this sort of stuff as well, and

01:22:01.000 --> 01:22:07.000
I don't know how far through the population LLMs actually will be able to penetrate,

01:22:07.000 --> 01:22:13.000
But their growth has been staggering and astonishing, and nothing short of completely mind-blowing.

01:22:13.000 --> 01:22:19.000
you know, the numbers of people that are actually already using these tools is huge, just gigantic.

01:22:19.000 --> 01:22:21.000
And so they've overcome

01:22:21.000 --> 01:22:25.000
this barrier, at least for some big chunk of the population.

01:22:25.000 --> 01:22:27.000
Um, class.

01:22:27.000 --> 01:22:31.000
Yeah, I mean, Marcus can use LLMs, but so can you.

01:22:31.000 --> 01:22:37.000
And sometimes smokers don't know how to use LLMs sufficiently. If you embed.

01:22:37.000 --> 01:22:43.000
Inspire the dynamics. I mean, this sounds sometimes almost corny, but if you embed spiral dynamics.

01:22:43.000 --> 01:22:49.000
Into your LLM, and then you instruct it to change the language or translate.

01:22:49.000 --> 01:23:03.000
What you're saying into a specific educational background. Um, a, uh, world fuel, you know? It is astounding how well it does that, and it totally maintains meaning.

01:23:03.000 --> 01:23:13.000
What you have in mind. Just give it a shot. You know, it's just load in the spiral Dynamics literature, and then… and then say, apply this to this text.

01:23:13.000 --> 01:23:21.000
And translate it into what, you know, this target audience that you're defining.

01:23:21.000 --> 01:23:22.000
Thanks, Khaus. Gil?

01:23:22.000 --> 01:23:25.000
Twitter is amazing how it works.

01:23:25.000 --> 01:23:31.000
Yeah. Uh, muggles can easily use LLMs, they can use them well or poorly.

01:23:31.000 --> 01:23:38.000
Depending on how well-oriented they are to them. Klaus, I really like the internal translation.

01:23:38.000 --> 01:23:40.000
concept.

01:23:40.000 --> 01:23:45.000
But, Sean, my question for you, so, like, I understand ontology and ontologies,

01:23:45.000 --> 01:23:55.000
And I understand Semantic Web, and I understand triplets, and I get all that stuff there, but I don't grasp from what you've described what the user experience of this thing is like.

01:23:55.000 --> 01:24:02.000
How does somebody who is not you or any of us here code-oriented technology-oriented? How does a regular person use this thing?

01:24:02.000 --> 01:24:07.000
what's, you know… I walk up, I sit down at my computer, what's happening?

01:24:07.000 --> 01:24:08.000
Yeah.

01:24:08.000 --> 01:24:12.000
Uh, in contrast to what's happening if I'm doing ChatGPT. I don't… I don't grasp that from what you've said at all.

01:24:12.000 --> 01:24:16.000
Yeah. They're gone. Oh, y'all.

01:24:16.000 --> 01:24:17.000
Oh, sorry, I've… have I stepped in it?

01:24:17.000 --> 01:24:22.000
No, no, no, it's that…

01:24:22.000 --> 01:24:23.000
I should have a demo.

01:24:23.000 --> 01:24:24.000
You know, but this is the reason for Sean to have a demo in, like, to call a session. Yeah.

01:24:24.000 --> 01:24:25.000
Yeah, that would be great.

01:24:25.000 --> 01:24:27.000
I should have a demo, for sure, because it…

01:24:27.000 --> 01:24:32.000
But super briefly, basically, it's superfluent

01:24:32.000 --> 01:24:34.000
super flexible

01:24:34.000 --> 01:24:36.000
multimedia.

01:24:36.000 --> 01:24:40.000
and integrating, uh, LLM inter… interactions.

01:24:40.000 --> 01:24:41.000
Mm-hmm.

01:24:41.000 --> 01:24:45.000
And basically treating AIs as just one more voice,

01:24:45.000 --> 01:24:46.000
Mm-hmm.

01:24:46.000 --> 01:24:48.000
that participates in a conversation,

01:24:48.000 --> 01:24:53.000
And all of the participants can operate either linguistically,

01:24:53.000 --> 01:24:55.000
or conceptually.

01:24:55.000 --> 01:24:58.000
So, I'm using the word…

01:24:58.000 --> 01:25:02.000
conceptually as an alternative to ontologically based.

01:25:02.000 --> 01:25:06.000
So, when I say conceptually, I mean with regard… with…

01:25:06.000 --> 01:25:12.000
foundation and models and entities connecting to one another in

01:25:12.000 --> 01:25:13.000
rigorous ways.

01:25:13.000 --> 01:25:16.000
Okay. And when do you figure you're going to be able to show us something?

01:25:16.000 --> 01:25:19.000
Um…

01:25:19.000 --> 01:25:20.000
So, that'll be cool.

01:25:20.000 --> 01:25:24.000
Uh, tomorrow, later today?

01:25:24.000 --> 01:25:25.000
Okay.

01:25:25.000 --> 01:25:28.000
Anytime… anytime anybody wants to engage, I'm happy to have a one-on-one with every person.

01:25:28.000 --> 01:25:29.000
I would say maybe, um, connect…

01:25:29.000 --> 01:25:31.000
Or maybe… maybe next week in the group.

01:25:31.000 --> 01:25:38.000
Can I connect with Gil? And, uh, I think this should be a pop-up separate from the OGM calls, just so that

01:25:38.000 --> 01:25:39.000
Yeah.

01:25:39.000 --> 01:25:41.000
Um, so that you can just do that and go deep.

01:25:41.000 --> 01:25:44.000
coordinate with Gil to pick a time that works for both of you next week.

01:25:44.000 --> 01:25:50.000
The rest of us will suffer with whatever that time is, and announce that time to the… in a Zoom room to the…

01:25:50.000 --> 01:25:58.000
OGM list. And, uh, those of us who can join will join, and if you record the call, and Sean, you may want to keep your information proprietary, I don't know,

01:25:58.000 --> 01:26:03.000
But if you record it and post it, then those of us who missed the call can actually see the demo.

01:26:03.000 --> 01:26:10.000
And that'll give you a demo to be able to use. So I… your mileage may vary on how you want to go about doing that.

01:26:10.000 --> 01:26:20.000
reasonable. Yeah, I just put a meat, uh, like a Calendly link here on anybody who wants a one-on-one, um, is welcome to, uh, to help out.

01:26:20.000 --> 01:26:22.000
Also, that'd be great. Cool.

01:26:22.000 --> 01:26:26.000
So whoever wants to go ahead and, uh, connect with Sean.

01:26:26.000 --> 01:26:31.000
Um, we are, magically, at the end of our time for today.

01:26:31.000 --> 01:26:34.000
Um, don't know how that happened.

01:26:34.000 --> 01:26:39.000
Uh, it turns out Doug has a cat as well, which is lovely.

01:26:39.000 --> 01:26:42.000
It turns out Bill is a poodle.

01:26:42.000 --> 01:26:43.000
Hm?

01:26:43.000 --> 01:26:48.000
Um, or has transmogrified into a poodle, uh, before our very eyes.

01:26:48.000 --> 01:26:54.000
And, uh, does anyone have anything they'd like to… well…

01:26:54.000 --> 01:27:03.000
I don't know that we solved this. Alex, you were troubled by our thrashing around in the topic. Have we done any better this call on…

01:27:03.000 --> 01:27:09.000
refining any of the ideas, and then should we come back into this topic? This is a question for everybody.

01:27:09.000 --> 01:27:17.000
Or anybody, should we come back into this topic next week to see if we can't pin a couple things down, or are we satisfied with where we are on this? So, first, Alex, and then

01:27:17.000 --> 01:27:21.000
After him, anybody else who'd like to jump in?

01:27:21.000 --> 01:27:30.000
I got a lot more out of this. It's for me, because of the way my brain works, it was more… logical and siloed conversations, and.

01:27:30.000 --> 01:27:37.000
Et cetera, a good progression. So yeah, I don't think we answered everything from last week, but that's other people worried about.

01:27:37.000 --> 01:27:45.000
Other things. Um… I'm keen to see Thinkertoi first, and…

01:27:45.000 --> 01:27:48.000
Perhaps work on something else if Linkato is not the problem.

01:27:48.000 --> 01:27:53.000
The product to kind of… bring it all together. I had a lot of thoughts out of this.

01:27:53.000 --> 01:27:58.000
The thoughts of mine, and I'm not going to do it alone.

01:27:58.000 --> 01:27:59.000
Love that. Thank you. Anybody else?

01:27:59.000 --> 01:28:03.000
Too big.

01:28:03.000 --> 01:28:05.000
Thoughts, comments, reflections on…

01:28:05.000 --> 01:28:11.000
This topic and sticking with it, or where we are.

01:28:11.000 --> 01:28:13.000
Doug?

01:28:13.000 --> 01:28:17.000
I can see the words formulating in your head.

01:28:17.000 --> 01:28:27.000
Yeah, I just, um… I actually follow what Sean shared. Although, um, it's out of my wheelhouse and beyond my competence to do so, I did.

01:28:27.000 --> 01:28:36.000
And, um. And I'm… I'm looking forward…

01:28:36.000 --> 01:28:44.000
And I think, you know, Gil's question speaks to… the UX.

01:28:44.000 --> 01:28:51.000
Except it's not the UX in the tech industry version. It's the…

01:28:51.000 --> 01:28:59.000
Um, can you approach the UX. Dimension of it, turning the telescope around, and…

01:28:59.000 --> 01:29:05.000
Developing a robust. List of…

01:29:05.000 --> 01:29:09.000
Archetypes. Of users.

01:29:09.000 --> 01:29:16.000
With different cognitive… different emotional, different orientational, different educational.

01:29:16.000 --> 01:29:24.000
Different dimensions of ways of relating to technology, ways of relating to information.

01:29:24.000 --> 01:29:31.000
Ways of relating to other people. Can you design UXs?

01:29:31.000 --> 01:29:39.000
With as much diversity. Of… of openness and opportunity offered.

01:29:39.000 --> 01:29:47.000
As the diversity of data types, data profiles. And ontologies that are integrated into how you're.

01:29:47.000 --> 01:29:54.000
Manipulating zeros and ones. In other words, can you make the front end.

01:29:54.000 --> 01:30:02.000
Fit for human use. Factoring for the diversity of human beings.

01:30:02.000 --> 01:30:03.000
A tall, tall order.

01:30:03.000 --> 01:30:13.000
So that's… that's my gauntlet. That's my… my… my gauntlet through… Uh, down, because… Um, like, that's what's needed.

01:30:13.000 --> 01:30:17.000
It's…

01:30:17.000 --> 01:30:18.000
Doug, it's designed to do that to a degree,

01:30:18.000 --> 01:30:21.000
And that applies to the existing technology.

01:30:21.000 --> 01:30:25.000
Never before embodied in software.

01:30:25.000 --> 01:30:26.000
I'm sorry that that sounds so dramatic, but that's the reality.

01:30:26.000 --> 01:30:34.000
Understood. Okay, I'm looking… No, no, no, I believe it, and I'm looking forward to seeing that, because that's the shit.

01:30:34.000 --> 01:30:35.000
Cool. And I'm looking forward for comments about this session and where we're headed. Um, Alex?

01:30:35.000 --> 01:30:44.000
Yeah.

01:30:44.000 --> 01:30:50.000
Sure.

01:30:50.000 --> 01:30:51.000
Yeah, yeah. Text Adventure Games.

01:30:51.000 --> 01:30:55.000
It's not a poem, but can I just mention something? You mentioned… I don't know what you called it, about the… You're in the forest, and there's a stream, I don't remember what you called that particular…

01:30:55.000 --> 01:31:02.000
Yeah, so… all those games, LLMs are great at.

01:31:02.000 --> 01:31:06.000
You can literally ask it to create anything, a science fiction or whatever.

01:31:06.000 --> 01:31:11.000
It is even more fun than usual, because if you're lucky enough that the LLM you use.

01:31:11.000 --> 01:31:15.000
Creates a program behind the scenes. Which then executes as you run along.

01:31:15.000 --> 01:31:19.000
You'll get consistency. But if you just get normal LLM stuff.

01:31:19.000 --> 01:31:23.000
It literally makes it up along, so if you go west from this point.

01:31:23.000 --> 01:31:30.000
And you think you have ended up there again. You're not there. You have other options. It's quite surreal, and quite…

01:31:30.000 --> 01:31:41.000
Fun with a glass of wine in your hand, and… trying to kind of go through your imagination. It really is superb. You can have zombies, you can do anything.

01:31:41.000 --> 01:31:42.000
All possible.

01:31:42.000 --> 01:31:46.000
You know, in a lens. That's where LLMs were made for, in my opinion.

01:31:46.000 --> 01:31:49.000
Thank you, thank you.

01:31:49.000 --> 01:31:50.000
Any other closing thoughts?

01:31:50.000 --> 01:31:53.000
Make things up…

01:31:53.000 --> 01:31:56.000
I think that's our call for today.

01:31:56.000 --> 01:31:58.000
See you all in a week.

01:31:58.000 --> 01:32:02.000
Um, thank you very much.

01:32:02.000 --> 01:32:06.000
Hi, everybody.

